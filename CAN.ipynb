{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manujo-varghese/CANalyzat0r/blob/master/CAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYgr6h7LWlk2",
        "outputId": "f656e751-8094-4cfc-bc6a-592c667e4fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 26 log files: Fuzzing_valid_IDs.log, Steering_angle_attack.log, FCA_warning_attack.log, Steering_angle_replay.log, Gear_shifter_attack_1.log, Door_open_warning_attack.log, DoS_attack.log, Min_speedometer_attack_2.log, Fuzzing_valid_IDs_DoS.log, Max_speedometer_attack.log, Break_and_fog_light_attack.log, EMS_attack.log, Min_speedometer_attack_3.log, Drive_mode_changing_attack.log, Parking_break_attack.log, EMS_replay_attack.log, Gear_shifter_attack_2.log, Fuzzing_random_IDs.log, Wiper_warning_attack.log, Multiple_attacks_1.log, Reverse_speedometer_fuzzing_attack.log, Multiple_attacks_2.log, Min_speedometer_attack_1.log, Break_warning_attack.log, EMS_replay_long_attack.log, Power_steering_attack.log\n",
            "Using 2 processors for parallel analysis\n",
            "\n",
            "Analyzing Steering_angle_attack.log...\n",
            "Analyzing Fuzzing_valid_IDs.log...\n",
            "\n",
            "Loaded 257413 frames with 56 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:   4%|▎         | 2/56 [00:00<00:19,  2.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 369630 frames with 56 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [00:42<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 224 signals across 56 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [01:01<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 204 signals across 56 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Fuzzing_valid_IDs\n",
            "Completed analysis of Fuzzing_valid_IDs.log\n",
            "\n",
            "Analyzing FCA_warning_attack.log...\n",
            "Loaded 581637 frames with 56 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  14%|█▍        | 8/56 [00:12<01:08,  1.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Steering_angle_attack\n",
            "Completed analysis of Steering_angle_attack.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rIdentifying signals:  16%|█▌        | 9/56 [00:16<01:39,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Steering_angle_replay.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  30%|███       | 17/56 [00:24<00:39,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 475808 frames with 56 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [01:34<00:00,  1.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 198 signals across 56 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [01:17<00:00,  1.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 201 signals across 56 CAN IDs\n",
            "Results saved to autocan_results/Steering_angle_replay\n",
            "Completed analysis of Steering_angle_replay.log\n",
            "\n",
            "Analyzing Gear_shifter_attack_1.log...\n",
            "Results saved to autocan_results/FCA_warning_attack\n",
            "Completed analysis of FCA_warning_attack.log\n",
            "\n",
            "Analyzing Door_open_warning_attack.log...\n",
            "Loaded 438213 frames with 54 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:   4%|▎         | 2/54 [00:00<00:20,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 406609 frames with 54 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  17%|█▋        | 9/54 [00:14<01:01,  1.37s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals: 100%|██████████| 54/54 [01:11<00:00,  1.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 206 signals across 54 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 54/54 [01:08<00:00,  1.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 214 signals across 54 CAN IDs\n",
            "Results saved to autocan_results/Gear_shifter_attack_1\n",
            "Completed analysis of Gear_shifter_attack_1.log\n",
            "\n",
            "Analyzing DoS_attack.log...\n",
            "Results saved to autocan_results/Door_open_warning_attack\n",
            "Completed analysis of Door_open_warning_attack.log\n",
            "\n",
            "Analyzing Min_speedometer_attack_2.log...\n",
            "Loaded 341787 frames with 55 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:   9%|▉         | 5/55 [00:06<01:17,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 578618 frames with 54 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  13%|█▎        | 7/55 [00:08<00:59,  1.23s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals: 100%|██████████| 55/55 [00:57<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 197 signals across 55 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  94%|█████████▍| 51/54 [01:30<00:08,  2.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/DoS_attack\n",
            "Completed analysis of DoS_attack.log\n",
            "\n",
            "Analyzing Fuzzing_valid_IDs_DoS.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 54/54 [01:38<00:00,  1.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 201 signals across 54 CAN IDs\n",
            "Loaded 441332 frames with 55 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  11%|█         | 6/55 [00:10<01:47,  2.19s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  24%|██▎       | 13/55 [00:16<00:37,  1.12it/s]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  42%|████▏     | 23/55 [00:31<00:26,  1.19it/s]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  51%|█████     | 28/55 [00:38<00:24,  1.09it/s]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  55%|█████▍    | 30/55 [00:48<01:08,  2.75s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  78%|███████▊  | 43/55 [01:01<00:08,  1.35it/s]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  85%|████████▌ | 47/55 [01:09<00:16,  2.09s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Min_speedometer_attack_2\n",
            "Completed analysis of Min_speedometer_attack_2.log\n",
            "\n",
            "Analyzing Max_speedometer_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  93%|█████████▎| 51/55 [01:17<00:07,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 417094 frames with 56 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:   4%|▎         | 2/56 [00:00<00:17,  3.08it/s]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals: 100%|██████████| 55/55 [01:23<00:00,  1.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 209 signals across 55 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  96%|█████████▋| 54/56 [01:12<00:03,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Fuzzing_valid_IDs_DoS\n",
            "Completed analysis of Fuzzing_valid_IDs_DoS.log\n",
            "\n",
            "Analyzing Break_and_fog_light_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [01:13<00:00,  1.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 215 signals across 56 CAN IDs\n",
            "Loaded 508148 frames with 56 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  36%|███▌      | 20/56 [00:32<01:35,  2.64s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  61%|██████    | 34/56 [00:50<00:25,  1.15s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Max_speedometer_attack\n",
            "Completed analysis of Max_speedometer_attack.log\n",
            "\n",
            "Analyzing EMS_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  70%|██████▉   | 39/56 [00:58<00:20,  1.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 325678 frames with 56 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [01:26<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 206 signals across 56 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  98%|█████████▊| 55/56 [00:56<00:01,  1.34s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals: 100%|██████████| 56/56 [00:56<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 213 signals across 56 CAN IDs\n",
            "Results saved to autocan_results/Break_and_fog_light_attack\n",
            "Completed analysis of Break_and_fog_light_attack.log\n",
            "\n",
            "Analyzing Min_speedometer_attack_3.log...\n",
            "Loaded 279517 frames with 54 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:   7%|▋         | 4/54 [00:03<00:45,  1.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/EMS_attack\n",
            "Completed analysis of EMS_attack.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rIdentifying signals:   9%|▉         | 5/54 [00:03<00:42,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Drive_mode_changing_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  20%|██        | 11/54 [00:09<00:27,  1.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 238877 frames with 56 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 54/54 [00:46<00:00,  1.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 213 signals across 54 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [00:39<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 212 signals across 56 CAN IDs\n",
            "Results saved to autocan_results/Drive_mode_changing_attack\n",
            "Completed analysis of Drive_mode_changing_attack.log\n",
            "\n",
            "Analyzing Parking_break_attack.log...\n",
            "Loaded 146664 frames with 56 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:   7%|▋         | 4/56 [00:02<00:38,  1.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Min_speedometer_attack_3\n",
            "Completed analysis of Min_speedometer_attack_3.log\n",
            "\n",
            "Analyzing EMS_replay_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  27%|██▋       | 15/56 [00:08<00:23,  1.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 324147 frames with 56 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  61%|██████    | 34/56 [00:20<00:17,  1.27it/s]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals: 100%|██████████| 56/56 [00:28<00:00,  1.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 213 signals across 56 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  71%|███████▏  | 40/56 [00:43<00:15,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Parking_break_attack\n",
            "Completed analysis of Parking_break_attack.log\n",
            "\n",
            "Analyzing Gear_shifter_attack_2.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  88%|████████▊ | 49/56 [00:49<00:08,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 406438 frames with 54 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [00:58<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 210 signals across 56 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  70%|███████   | 38/54 [00:51<00:15,  1.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/EMS_replay_attack\n",
            "Completed analysis of EMS_replay_attack.log\n",
            "\n",
            "Analyzing Fuzzing_random_IDs.log..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rIdentifying signals:  72%|███████▏  | 39/54 [00:53<00:17,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  91%|█████████ | 49/54 [01:04<00:07,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 729007 frames with 646 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  98%|█████████▊| 53/54 [01:14<00:01,  2.00s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals: 100%|██████████| 54/54 [01:14<00:00,  1.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 217 signals across 54 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  68%|██████▊   | 437/646 [01:13<00:10, 19.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Gear_shifter_attack_2\n",
            "Completed analysis of Gear_shifter_attack_2.log\n",
            "\n",
            "Analyzing Wiper_warning_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  69%|██████▉   | 445/646 [01:18<00:36,  5.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 233555 frames with 56 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  38%|███▊      | 21/56 [00:17<00:45,  1.30s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals: 100%|██████████| 56/56 [00:37<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 219 signals across 56 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 646/646 [02:10<00:00,  4.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 637 signals across 480 CAN IDs\n",
            "Results saved to autocan_results/Wiper_warning_attack\n",
            "Completed analysis of Wiper_warning_attack.log\n",
            "\n",
            "Analyzing Multiple_attacks_1.log...\n",
            "Loaded 1672671 frames with 54 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  24%|██▍       | 13/54 [01:04<02:28,  3.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Fuzzing_random_IDs\n",
            "Completed analysis of Fuzzing_random_IDs.log\n",
            "\n",
            "Analyzing Reverse_speedometer_fuzzing_attack.log...\n",
            "Loaded 177130 frames with 54 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  11%|█         | 6/54 [00:05<00:46,  1.03it/s]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  56%|█████▌    | 30/54 [00:18<00:25,  1.05s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  81%|████████▏ | 44/54 [00:22<00:02,  3.71it/s]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals: 100%|██████████| 54/54 [00:30<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 188 signals across 54 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  43%|████▎     | 23/54 [01:59<01:30,  2.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Reverse_speedometer_fuzzing_attack\n",
            "Completed analysis of Reverse_speedometer_fuzzing_attack.log\n",
            "\n",
            "Analyzing Multiple_attacks_2.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  56%|█████▌    | 30/54 [02:42<02:54,  7.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2093747 frames with 620 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 54/54 [04:40<00:00,  5.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 222 signals across 54 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  88%|████████▊ | 545/620 [05:25<00:53,  1.41it/s]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals: 100%|██████████| 620/620 [06:04<00:00,  1.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 220 signals across 54 CAN IDs\n",
            "Results saved to autocan_results/Multiple_attacks_1\n",
            "Completed analysis of Multiple_attacks_1.log\n",
            "\n",
            "Analyzing Min_speedometer_attack_1.log...\n",
            "Loaded 545604 frames with 54 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 54/54 [01:37<00:00,  1.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 202 signals across 54 CAN IDs\n",
            "Results saved to autocan_results/Min_speedometer_attack_1\n",
            "Completed analysis of Min_speedometer_attack_1.log\n",
            "\n",
            "Analyzing Break_warning_attack.log...\n",
            "Loaded 588592 frames with 56 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [01:41<00:00,  1.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 205 signals across 56 CAN IDs\n",
            "Results saved to autocan_results/Multiple_attacks_2\n",
            "Completed analysis of Multiple_attacks_2.log\n",
            "\n",
            "Analyzing EMS_replay_long_attack.log...\n",
            "Loaded 2051085 frames with 54 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:   7%|▋         | 4/54 [00:24<07:24,  8.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Break_warning_attack\n",
            "Completed analysis of Break_warning_attack.log\n",
            "\n",
            "Analyzing Power_steering_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rIdentifying signals:   9%|▉         | 5/54 [00:29<06:00,  7.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 361198 frames with 56 unique IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [01:01<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 218 signals across 56 CAN IDs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  35%|███▌      | 19/54 [02:25<06:06, 10.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Power_steering_attack\n",
            "Completed analysis of Power_steering_attack.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 54/54 [04:40<00:00,  5.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified 214 signals across 54 CAN IDs\n",
            "Results saved to autocan_results/EMS_replay_long_attack\n",
            "Completed analysis of EMS_replay_long_attack.log\n",
            "\n",
            "Generating combined summary...\n",
            "File statistics saved to autocan_results/combined_results/file_statistics.csv\n",
            "\n",
            "Found 54 CAN IDs common across all files:\n",
            "130, 140, 153, 160, 164, 220, 251, 2B0, 340, 371, 372, 381, 386, 387, 389, 38D, 394, 410, 412, 420, 421, 436, 470, 47F, 485, 490, 495, 4A2, 4C9, 4F1, 500, 507, 50A, 50B, 50C, 50E, 520, 52A, 541, 544, 54B, 553, 559, 57F, 592, 593, 595, 596, 5B0, 5BE, 5C1, 5E3, 5FF, 7F\n",
            "\n",
            "Visualization plots saved to autocan_results/visualizations\n",
            "\n",
            "Analysis complete! Results are in the 'autocan_results' directory.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.signal import correlate\n",
        "import networkx as nx\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "from typing import List, Dict, Tuple, Any, Optional, Set, Union\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CANFrame:\n",
        "    \"\"\"Represents a single CAN frame with ID and payload.\"\"\"\n",
        "\n",
        "    def __init__(self, can_id: str, payload: str, timestamp: float = 0.0):\n",
        "        \"\"\"\n",
        "        Initialize a CAN frame.\n",
        "\n",
        "        Args:\n",
        "            can_id: The CAN ID in hexadecimal format (e.g. '0x123')\n",
        "            payload: The payload in hexadecimal format (e.g. '00FF34A2')\n",
        "            timestamp: The timestamp when the frame was captured\n",
        "        \"\"\"\n",
        "        self.can_id = can_id\n",
        "        self.payload = payload\n",
        "        self.timestamp = timestamp\n",
        "\n",
        "        # Convert payload to binary representation\n",
        "        try:\n",
        "            self.binary_payload = bin(int(payload, 16))[2:].zfill(len(payload) * 4)\n",
        "        except ValueError:\n",
        "            self.binary_payload = '0' * 64  # Default to 64 bits for invalid payloads\n",
        "\n",
        "    def get_bit(self, bit_position: int) -> int:\n",
        "        \"\"\"Get the value of a specific bit in the payload.\"\"\"\n",
        "        if bit_position < len(self.binary_payload):\n",
        "            return int(self.binary_payload[bit_position])\n",
        "        return 0\n",
        "\n",
        "    def get_byte(self, byte_position: int) -> int:\n",
        "        \"\"\"Get the value of a specific byte in the payload.\"\"\"\n",
        "        start_bit = byte_position * 8\n",
        "        end_bit = start_bit + 8\n",
        "        if start_bit < len(self.binary_payload):\n",
        "            byte_str = self.binary_payload[start_bit:min(end_bit, len(self.binary_payload))]\n",
        "            byte_str = byte_str.ljust(8, '0')\n",
        "            return int(byte_str, 2)\n",
        "        return 0\n",
        "\n",
        "    def get_bits(self, start_bit: int, num_bits: int) -> int:\n",
        "        \"\"\"Get a range of bits as a single integer value.\"\"\"\n",
        "        end_bit = start_bit + num_bits\n",
        "        if start_bit < len(self.binary_payload):\n",
        "            bits_str = self.binary_payload[start_bit:min(end_bit, len(self.binary_payload))]\n",
        "            bits_str = bits_str.ljust(num_bits, '0')\n",
        "            return int(bits_str, 2)\n",
        "        return 0\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"CANFrame(id={self.can_id}, payload={self.payload}, ts={self.timestamp})\"\n",
        "\n",
        "\n",
        "class CANTrace:\n",
        "    \"\"\"Collection of CAN frames representing a vehicle trace.\"\"\"\n",
        "\n",
        "    def __init__(self, frames: Optional[List[CANFrame]] = None):\n",
        "        \"\"\"\n",
        "        Initialize a CAN trace with optional frames.\n",
        "\n",
        "        Args:\n",
        "            frames: List of CANFrame objects\n",
        "        \"\"\"\n",
        "        self.frames = frames or []\n",
        "        self.frames_by_id = defaultdict(list)\n",
        "        self._update_frame_index()\n",
        "\n",
        "    def _update_frame_index(self):\n",
        "        \"\"\"Update the index of frames by CAN ID.\"\"\"\n",
        "        self.frames_by_id = defaultdict(list)\n",
        "        for frame in self.frames:\n",
        "            self.frames_by_id[frame.can_id].append(frame)\n",
        "\n",
        "    def add_frame(self, frame: CANFrame):\n",
        "        \"\"\"Add a single frame to the trace.\"\"\"\n",
        "        self.frames.append(frame)\n",
        "        self.frames_by_id[frame.can_id].append(frame)\n",
        "\n",
        "    def add_frames(self, frames: List[CANFrame]):\n",
        "        \"\"\"Add multiple frames to the trace.\"\"\"\n",
        "        self.frames.extend(frames)\n",
        "        self._update_frame_index()\n",
        "\n",
        "    def get_frames_by_id(self, can_id: str) -> List[CANFrame]:\n",
        "        \"\"\"Get all frames with a specific CAN ID.\"\"\"\n",
        "        return self.frames_by_id.get(can_id, [])\n",
        "\n",
        "    def get_unique_ids(self) -> Set[str]:\n",
        "        \"\"\"Get the set of unique CAN IDs in the trace.\"\"\"\n",
        "        return set(self.frames_by_id.keys())\n",
        "\n",
        "    @classmethod\n",
        "    def from_log(cls, filepath):\n",
        "        trace = cls()\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue  # skip empty lines\n",
        "\n",
        "                if line.startswith('('):\n",
        "                    end_idx = line.find(')')\n",
        "                    ts_str = line[1:end_idx]\n",
        "                    timestamp = float(ts_str)\n",
        "                else:\n",
        "                    continue  # skip invalid format lines\n",
        "\n",
        "                remaining = line[end_idx+1:].strip()\n",
        "                parts = remaining.split()\n",
        "                if len(parts) < 2:\n",
        "                    continue  # malformed line\n",
        "\n",
        "                id_payload = parts[1] if '#' in parts[1] else parts[0]\n",
        "                if '#' not in id_payload:\n",
        "                    continue  # invalid payload\n",
        "\n",
        "                id_str, data_str = id_payload.split('#', 1)\n",
        "                try:\n",
        "                    can_id = int(id_str, 16)\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "                data_bytes = [int(data_str[i:i+2], 16) for i in range(0, len(data_str), 2)]\n",
        "\n",
        "                # Create CANFrame with matching signature\n",
        "                frame = CANFrame(can_id=f\"{can_id:X}\", payload=''.join(f\"{byte:02X}\" for byte in data_bytes), timestamp=timestamp)\n",
        "                trace.frames.append(frame)\n",
        "\n",
        "        trace._update_frame_index()\n",
        "        return trace\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.frames)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"CANTrace(frames={len(self.frames)}, unique_ids={len(self.get_unique_ids())})\"\n",
        "\n",
        "\n",
        "class CANSignal:\n",
        "    \"\"\"Represents a decoded signal within CAN frames.\"\"\"\n",
        "\n",
        "    SIGNAL_TYPES = ['uint', 'enum', 'rand', 'cyclic']\n",
        "\n",
        "    def __init__(self, can_id: str, start_bit: int, num_bits: int, signal_type: str = 'unknown'):\n",
        "        \"\"\"\n",
        "        Initialize a CAN signal.\n",
        "\n",
        "        Args:\n",
        "            can_id: The CAN ID this signal belongs to\n",
        "            start_bit: Starting bit position (0-indexed)\n",
        "            num_bits: Number of bits in the signal\n",
        "            signal_type: Type of signal ('uint', 'enum', 'rand', 'cyclic', or 'unknown')\n",
        "        \"\"\"\n",
        "        self.can_id = can_id\n",
        "        self.start_bit = start_bit\n",
        "        self.num_bits = num_bits\n",
        "        self.signal_type = signal_type if signal_type in self.SIGNAL_TYPES else 'unknown'\n",
        "        self.label = None\n",
        "        self.values = []  # Will hold the signal values extracted from frames\n",
        "        self.timestamps = []  # Timestamps corresponding to values\n",
        "\n",
        "        # Statistics calculated later\n",
        "        self.autocorrelation = None\n",
        "        self.hamd = None\n",
        "        self.min_value = None\n",
        "        self.max_value = None\n",
        "        self.avg_value = None\n",
        "        self.std_value = None\n",
        "        self.distinct_values = None\n",
        "        self.flip_rate = None\n",
        "\n",
        "    def extract_values(self, frames: List[CANFrame]):\n",
        "        \"\"\"Extract signal values from a list of CAN frames.\"\"\"\n",
        "        self.values = []\n",
        "        self.timestamps = []\n",
        "\n",
        "        for frame in frames:\n",
        "            value = frame.get_bits(self.start_bit, self.num_bits)\n",
        "            self.values.append(value)\n",
        "            self.timestamps.append(frame.timestamp)\n",
        "\n",
        "        # Calculate basic statistics\n",
        "        if self.values:\n",
        "            self.min_value = min(self.values)\n",
        "            self.max_value = max(self.values)\n",
        "            self.avg_value = sum(self.values) / len(self.values)\n",
        "            self.std_value = np.std(self.values) if len(self.values) > 1 else 0\n",
        "            self.distinct_values = len(set(self.values))\n",
        "\n",
        "            # Calculate flip rate\n",
        "            if len(self.values) > 1:\n",
        "                flips = sum(1 for i in range(len(self.values)-1) if self.values[i] != self.values[i+1])\n",
        "                self.flip_rate = flips / (len(self.values) - 1)\n",
        "            else:\n",
        "                self.flip_rate = 0\n",
        "\n",
        "    def calculate_autocorrelation(self, max_lag: int = 100):\n",
        "        \"\"\"Calculate the autocorrelation function for the signal values.\"\"\"\n",
        "        if len(self.values) > max_lag:\n",
        "            # Normalize the values\n",
        "            normalized = np.array(self.values) - np.mean(self.values)\n",
        "            if np.std(normalized) > 0:\n",
        "                normalized = normalized / np.std(normalized)\n",
        "\n",
        "                # Calculate autocorrelation\n",
        "                self.autocorrelation = [1.0]  # Lag 0 is always 1.0\n",
        "                for lag in range(1, max_lag):\n",
        "                    corr = np.corrcoef(normalized[:-lag], normalized[lag:])[0, 1]\n",
        "                    self.autocorrelation.append(corr)\n",
        "            else:\n",
        "                self.autocorrelation = [0.0] * max_lag\n",
        "        else:\n",
        "            self.autocorrelation = [0.0] * max_lag\n",
        "\n",
        "    def calculate_hamd(self):\n",
        "        \"\"\"Calculate the Hamming distance distribution (HAMD) for the signal.\"\"\"\n",
        "        if len(self.values) <= 1:\n",
        "            self.hamd = [0.0] * self.num_bits\n",
        "            return\n",
        "\n",
        "        # Convert values to binary sequences\n",
        "        binary_values = [format(v, f'0{self.num_bits}b') for v in self.values]\n",
        "\n",
        "        # Calculate HAMD for each bit position\n",
        "        self.hamd = []\n",
        "        for bit_pos in range(self.num_bits):\n",
        "            # Extract the bit at this position for all values\n",
        "            bits = [int(binary_values[j][bit_pos]) for j in range(len(binary_values))]\n",
        "\n",
        "            # Count flips\n",
        "            flips = sum(1 for j in range(len(bits)-1) if bits[j] != bits[j+1])\n",
        "            flip_rate = flips / (len(bits) - 1) if len(bits) > 1 else 0\n",
        "            self.hamd.append(flip_rate)\n",
        "\n",
        "    def is_likely_uint(self) -> bool:\n",
        "        \"\"\"Check if the signal is likely an unsigned integer.\"\"\"\n",
        "        if not self.hamd:\n",
        "            return False\n",
        "\n",
        "        # Check for increasing HAMD pattern typical of little-endian integers\n",
        "        increasing = all(self.hamd[i] <= self.hamd[i+1] for i in range(len(self.hamd)-1))\n",
        "        return increasing and self.flip_rate > 0.01\n",
        "\n",
        "    def is_likely_enum(self) -> bool:\n",
        "        \"\"\"Check if the signal is likely an enumeration.\"\"\"\n",
        "        if not self.hamd:\n",
        "            return False\n",
        "\n",
        "        # Enums typically have a low number of distinct values relative to possible values\n",
        "        max_possible = 2 ** self.num_bits\n",
        "        enum_threshold = min(32, max_possible * 0.1)  # Heuristic threshold\n",
        "\n",
        "        return (self.distinct_values <= enum_threshold and\n",
        "                self.distinct_values > 1 and\n",
        "                self.flip_rate < 0.3)\n",
        "\n",
        "    def is_likely_random(self) -> bool:\n",
        "        \"\"\"Check if the signal is likely random (like a checksum).\"\"\"\n",
        "        if not self.hamd or not self.autocorrelation:\n",
        "            return False\n",
        "\n",
        "        # Random signals have high entropy and low autocorrelation\n",
        "        high_entropy = self.distinct_values > min(100, 2 ** self.num_bits * 0.5)\n",
        "        low_autocorr = all(abs(ac) < 0.3 for ac in self.autocorrelation[1:10] if ac is not None)\n",
        "\n",
        "        return high_entropy and low_autocorr and self.flip_rate > 0.3\n",
        "\n",
        "    def is_likely_cyclic(self) -> bool:\n",
        "        \"\"\"Check if the signal is likely cyclic (counter, clock).\"\"\"\n",
        "        if not self.autocorrelation or len(self.autocorrelation) < 10:\n",
        "            return False\n",
        "\n",
        "        # Cyclic signals show oscillatory autocorrelation\n",
        "        oscillatory = False\n",
        "        if len(self.autocorrelation) > 2:\n",
        "            # Check for sign changes in autocorrelation - typical of oscillatory signals\n",
        "            sign_changes = sum(1 for i in range(1, len(self.autocorrelation)-1)\n",
        "                              if (self.autocorrelation[i] * self.autocorrelation[i+1]) < 0)\n",
        "            oscillatory = sign_changes >= 3\n",
        "\n",
        "        # Also check for steadily increasing values with wrap-around\n",
        "        if len(self.values) > 10:\n",
        "            increasing_count = sum(1 for i in range(len(self.values)-1)\n",
        "                                  if self.values[i+1] == (self.values[i] + 1) % (2**self.num_bits))\n",
        "            perfect_counter = increasing_count > len(self.values) * 0.8\n",
        "            return oscillatory or perfect_counter\n",
        "\n",
        "        return oscillatory\n",
        "\n",
        "    def determine_signal_type(self):\n",
        "        \"\"\"Determine the most likely signal type based on signal characteristics.\"\"\"\n",
        "        # Make sure we have calculated all necessary characteristics\n",
        "        if self.hamd is None:\n",
        "            self.calculate_hamd()\n",
        "        if self.autocorrelation is None:\n",
        "            self.calculate_autocorrelation()\n",
        "\n",
        "        # Check conditions for each type\n",
        "        if self.is_likely_cyclic():\n",
        "            self.signal_type = 'cyclic'\n",
        "        elif self.is_likely_random():\n",
        "            self.signal_type = 'rand'\n",
        "        elif self.is_likely_enum():\n",
        "            self.signal_type = 'enum'\n",
        "        elif self.is_likely_uint():\n",
        "            self.signal_type = 'uint'\n",
        "        else:\n",
        "            self.signal_type = 'unknown'\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        label_str = f\", label={self.label}\" if self.label else \"\"\n",
        "        return (f\"CANSignal(id={self.can_id}, bits=[{self.start_bit}:{self.start_bit+self.num_bits}], \"\n",
        "                f\"type={self.signal_type}{label_str})\")\n",
        "\n",
        "\n",
        "class AutoCANAnalyzer:\n",
        "    \"\"\"\n",
        "    Implements the AutoCAN methodology for automatic reverse engineering of CAN signals.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, can_trace: CANTrace):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer with a CAN trace.\n",
        "\n",
        "        Args:\n",
        "            can_trace: A CANTrace object containing the frames to analyze\n",
        "        \"\"\"\n",
        "        self.can_trace = can_trace\n",
        "        self.signals_by_id = {}  # Dict of signals by CAN ID\n",
        "        self.correlation_graph = None\n",
        "\n",
        "    def identify_signals(self, progress_bar=True):\n",
        "        \"\"\"\n",
        "        Identify signals for all CAN IDs in the trace.\n",
        "\n",
        "        Args:\n",
        "            progress_bar: Whether to show a progress bar\n",
        "        \"\"\"\n",
        "        unique_ids = self.can_trace.get_unique_ids()\n",
        "\n",
        "        if progress_bar:\n",
        "            id_iterator = tqdm(unique_ids, desc=\"Identifying signals\")\n",
        "        else:\n",
        "            id_iterator = unique_ids\n",
        "\n",
        "        for can_id in id_iterator:\n",
        "            frames = self.can_trace.get_frames_by_id(can_id)\n",
        "            if len(frames) > 10:  # Skip IDs with too few frames\n",
        "                self.signals_by_id[can_id] = self.identify_signals_for_id(can_id, frames)\n",
        "\n",
        "    def identify_signals_for_id(self, can_id: str, frames: List[CANFrame]) -> List[CANSignal]:\n",
        "        \"\"\"\n",
        "        Identify signals for a specific CAN ID.\n",
        "\n",
        "        Args:\n",
        "            can_id: The CAN ID to analyze\n",
        "            frames: List of frames with this CAN ID\n",
        "\n",
        "        Returns:\n",
        "            List of identified CANSignal objects\n",
        "        \"\"\"\n",
        "        # Step 1: Extract byte-level features\n",
        "        byte_features = self._extract_byte_features(frames)\n",
        "\n",
        "        # Step 2: Cluster bytes using DBSCAN\n",
        "        byte_clusters = self._cluster_bytes(byte_features)\n",
        "\n",
        "        # Step 3: Process each byte cluster to identify bit-level signals\n",
        "        signals = []\n",
        "        for cluster_id, byte_indices in byte_clusters.items():\n",
        "            # Skip noise cluster (cluster_id = -1)\n",
        "            if cluster_id == -1:\n",
        "                continue\n",
        "\n",
        "            # Extract bit-level features for this cluster\n",
        "            start_bit = min(byte_indices) * 8\n",
        "            end_bit = (max(byte_indices) + 1) * 8 - 1\n",
        "            cluster_signals = self._identify_bit_level_signals(frames, start_bit, end_bit)\n",
        "            signals.extend(cluster_signals)\n",
        "\n",
        "        # Extract values and determine signal types\n",
        "        for signal in signals:\n",
        "            signal.extract_values(frames)\n",
        "            signal.calculate_autocorrelation()\n",
        "            signal.calculate_hamd()\n",
        "            signal.determine_signal_type()\n",
        "\n",
        "        return signals\n",
        "\n",
        "    def _extract_byte_features(self, frames: List[CANFrame]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Extract byte-level features from a list of frames.\n",
        "\n",
        "        Args:\n",
        "            frames: List of CAN frames\n",
        "\n",
        "        Returns:\n",
        "            2D array of features (rows=bytes, columns=features)\n",
        "        \"\"\"\n",
        "        max_bytes = 8  # Standard CAN frame has 8 bytes max\n",
        "\n",
        "        # Create arrays for feature extraction\n",
        "        byte_values = np.zeros((max_bytes, len(frames)), dtype=int)\n",
        "\n",
        "        # Extract byte values\n",
        "        for byte_idx in range(max_bytes):\n",
        "            for frame_idx, frame in enumerate(frames):\n",
        "                byte_values[byte_idx, frame_idx] = frame.get_byte(byte_idx)\n",
        "\n",
        "        # Calculate features for each byte\n",
        "        features = []\n",
        "        for byte_idx in range(max_bytes):\n",
        "            byte_data = byte_values[byte_idx, :]\n",
        "\n",
        "            # Calculate flip rate\n",
        "            flips = sum(1 for i in range(len(byte_data)-1) if byte_data[i] != byte_data[i+1])\n",
        "            flip_rate = flips / (len(byte_data) - 1) if len(byte_data) > 1 else 0\n",
        "\n",
        "            # Calculate average value\n",
        "            avg_value = np.mean(byte_data)\n",
        "\n",
        "            # Calculate distinct value ratio\n",
        "            distinct_values = len(np.unique(byte_data))\n",
        "            distinct_ratio = distinct_values / 256  # 256 possible values for a byte\n",
        "\n",
        "            # Entropy\n",
        "            values, counts = np.unique(byte_data, return_counts=True)\n",
        "            probs = counts / len(byte_data)\n",
        "            entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
        "\n",
        "            features.append([flip_rate, avg_value / 255, distinct_ratio, entropy / 8])\n",
        "\n",
        "        return np.array(features)\n",
        "\n",
        "    def _cluster_bytes(self, byte_features: np.ndarray, eps=0.4, min_samples=2) -> Dict[int, List[int]]:\n",
        "        \"\"\"\n",
        "        Cluster bytes based on their features using DBSCAN.\n",
        "\n",
        "        Args:\n",
        "            byte_features: 2D array of byte features\n",
        "            eps: DBSCAN epsilon parameter\n",
        "            min_samples: DBSCAN min_samples parameter\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping cluster IDs to lists of byte indices\n",
        "        \"\"\"\n",
        "        # Apply DBSCAN to group related bytes\n",
        "        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(byte_features)\n",
        "        labels = clustering.labels_\n",
        "\n",
        "        # Group bytes by cluster\n",
        "        clusters = defaultdict(list)\n",
        "        for i, label in enumerate(labels):\n",
        "            clusters[label].append(i)\n",
        "\n",
        "        # Refine clusters to ensure they contain consecutive bytes\n",
        "        refined_clusters = {}\n",
        "        next_cluster_id = max(clusters.keys()) + 1 if clusters else 0\n",
        "\n",
        "        for cluster_id, byte_indices in clusters.items():\n",
        "            # Skip noise cluster as we'll process them individually\n",
        "            if cluster_id == -1:\n",
        "                for idx in byte_indices:\n",
        "                    refined_clusters[next_cluster_id] = [idx]\n",
        "                    next_cluster_id += 1\n",
        "                continue\n",
        "\n",
        "            # Sort byte indices\n",
        "            byte_indices.sort()\n",
        "\n",
        "            # Find consecutive chunks\n",
        "            chunks = []\n",
        "            current_chunk = [byte_indices[0]]\n",
        "\n",
        "            for i in range(1, len(byte_indices)):\n",
        "                if byte_indices[i] == byte_indices[i-1] + 1:\n",
        "                    current_chunk.append(byte_indices[i])\n",
        "                else:\n",
        "                    chunks.append(current_chunk)\n",
        "                    current_chunk = [byte_indices[i]]\n",
        "\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "\n",
        "            # Create a new cluster for each chunk\n",
        "            for chunk in chunks:\n",
        "                refined_clusters[next_cluster_id] = chunk\n",
        "                next_cluster_id += 1\n",
        "\n",
        "        return refined_clusters\n",
        "\n",
        "    def _identify_bit_level_signals(self, frames: List[CANFrame], start_bit: int, end_bit: int) -> List[CANSignal]:\n",
        "        \"\"\"\n",
        "        Identify bit-level signals within a byte cluster.\n",
        "\n",
        "        Args:\n",
        "            frames: List of CAN frames\n",
        "            start_bit: Starting bit position\n",
        "            end_bit: Ending bit position\n",
        "\n",
        "        Returns:\n",
        "            List of identified CANSignal objects\n",
        "        \"\"\"\n",
        "        can_id = frames[0].can_id\n",
        "        num_bits = end_bit - start_bit + 1\n",
        "\n",
        "        # For small bit ranges (8 bits or less), try to determine if this is a single signal\n",
        "        if num_bits <= 8:\n",
        "            signal = CANSignal(can_id, start_bit, num_bits)\n",
        "            return [signal]\n",
        "\n",
        "        # Extract bit flip rates\n",
        "        bit_flip_rates = self._calculate_bit_flip_rates(frames, start_bit, end_bit)\n",
        "\n",
        "        # Use DBSCAN to cluster bits based on flip rates\n",
        "        bit_clusters = self._cluster_bits(bit_flip_rates)\n",
        "\n",
        "        # Create signals from bit clusters\n",
        "        signals = []\n",
        "        for cluster_bits in bit_clusters:\n",
        "            # Sort bits to ensure consecutive ordering\n",
        "            cluster_bits.sort()\n",
        "\n",
        "            # Find consecutive chunks of bits\n",
        "            chunks = []\n",
        "            current_chunk = [cluster_bits[0]]\n",
        "\n",
        "            for i in range(1, len(cluster_bits)):\n",
        "                if cluster_bits[i] == cluster_bits[i-1] + 1:\n",
        "                    current_chunk.append(cluster_bits[i])\n",
        "                else:\n",
        "                    chunks.append(current_chunk)\n",
        "                    current_chunk = [cluster_bits[i]]\n",
        "\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "\n",
        "            # Create a signal for each chunk\n",
        "            for chunk in chunks:\n",
        "                chunk_start = start_bit + chunk[0]\n",
        "                chunk_length = len(chunk)\n",
        "                signal = CANSignal(can_id, chunk_start, chunk_length)\n",
        "                signals.append(signal)\n",
        "\n",
        "        return signals\n",
        "\n",
        "    def _calculate_bit_flip_rates(self, frames: List[CANFrame], start_bit: int, end_bit: int) -> List[float]:\n",
        "        \"\"\"\n",
        "        Calculate flip rates for each bit in the specified range.\n",
        "\n",
        "        Args:\n",
        "            frames: List of CAN frames\n",
        "            start_bit: Starting bit position\n",
        "            end_bit: Ending bit position\n",
        "\n",
        "        Returns:\n",
        "            List of flip rates for each bit\n",
        "        \"\"\"\n",
        "        num_bits = end_bit - start_bit + 1\n",
        "        bit_values = np.zeros((num_bits, len(frames)), dtype=int)\n",
        "\n",
        "        # Extract bit values\n",
        "        for bit_idx in range(num_bits):\n",
        "            for frame_idx, frame in enumerate(frames):\n",
        "                bit_values[bit_idx, frame_idx] = frame.get_bit(start_bit + bit_idx)\n",
        "\n",
        "        # Calculate flip rates\n",
        "        flip_rates = []\n",
        "        for bit_idx in range(num_bits):\n",
        "            bit_data = bit_values[bit_idx, :]\n",
        "            flips = sum(1 for i in range(len(bit_data)-1) if bit_data[i] != bit_data[i+1])\n",
        "            flip_rate = flips / (len(bit_data) - 1) if len(bit_data) > 1 else 0\n",
        "            flip_rates.append(flip_rate)\n",
        "\n",
        "        return flip_rates\n",
        "\n",
        "    def _cluster_bits(self, bit_flip_rates: List[float], eps=0.15, min_samples=2) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Cluster bits based on their flip rates using DBSCAN.\n",
        "\n",
        "        Args:\n",
        "            bit_flip_rates: List of bit flip rates\n",
        "            eps: DBSCAN epsilon parameter\n",
        "            min_samples: DBSCAN min_samples parameter\n",
        "\n",
        "        Returns:\n",
        "            List of lists where each inner list contains bit indices for a cluster\n",
        "        \"\"\"\n",
        "        # Convert flip rates to 2D array for DBSCAN\n",
        "        X = np.array(bit_flip_rates).reshape(-1, 1)\n",
        "\n",
        "        # Apply DBSCAN\n",
        "        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
        "        labels = clustering.labels_\n",
        "\n",
        "        # Group bits by cluster\n",
        "        clusters = defaultdict(list)\n",
        "        for i, label in enumerate(labels):\n",
        "            clusters[label].append(i)\n",
        "\n",
        "        # Return list of bit clusters (including noise cluster)\n",
        "        return list(clusters.values())\n",
        "\n",
        "    def build_correlation_graph(self, min_correlation=0.8):\n",
        "        \"\"\"\n",
        "        Build a graph of signals based on their correlation.\n",
        "\n",
        "        Args:\n",
        "            min_correlation: Minimum correlation coefficient to create an edge\n",
        "        \"\"\"\n",
        "        if not self.signals_by_id:\n",
        "            raise ValueError(\"No signals identified. Run identify_signals() first.\")\n",
        "\n",
        "        all_signals = []\n",
        "        for signals in self.signals_by_id.values():\n",
        "            all_signals.extend(signals)\n",
        "\n",
        "        G = nx.Graph()\n",
        "\n",
        "        for i, signal in enumerate(all_signals):\n",
        "            G.add_node(i,\n",
        "                      can_id=signal.can_id,\n",
        "                      signal_type=signal.signal_type,\n",
        "                      bits=f\"{signal.start_bit}:{signal.start_bit+signal.num_bits}\",\n",
        "                      start_bit=signal.start_bit,\n",
        "                      num_bits=signal.num_bits)\n",
        "\n",
        "        for i, signal1 in enumerate(all_signals):\n",
        "            if len(signal1.values) < 10:\n",
        "                continue\n",
        "\n",
        "            for j in range(i+1, len(all_signals)):\n",
        "                signal2 = all_signals[j]\n",
        "                if len(signal2.values) < 10:\n",
        "                    continue\n",
        "\n",
        "                min_len = min(len(signal1.values), len(signal2.values))\n",
        "                values1 = signal1.values[:min_len]\n",
        "                values2 = signal2.values[:min_len]\n",
        "\n",
        "                if len(np.unique(values1)) <= 1 or len(np.unique(values2)) <= 1:\n",
        "                    continue\n",
        "\n",
        "                corr = np.corrcoef(values1, values2)[0, 1]\n",
        "\n",
        "                if abs(corr) >= min_correlation:\n",
        "                    G.add_edge(i, j, weight=abs(corr))\n",
        "\n",
        "        self.correlation_graph = G\n",
        "        return G\n",
        "\n",
        "\n",
        "\n",
        "    def discover_mathematical_relationships(self):\n",
        "        \"\"\"\n",
        "        Discover mathematical relationships between signals.\n",
        "\n",
        "        Returns:\n",
        "            List of tuples (signal1, relationship, signal2, strength)\n",
        "        \"\"\"\n",
        "        # Ensure we have identified signals first\n",
        "        if not self.signals_by_id:\n",
        "            raise ValueError(\"No signals identified. Run identify_signals() first.\")\n",
        "\n",
        "        # Create a flat list of all signals\n",
        "        all_signals = []\n",
        "        for signals in self.signals_by_id.values():\n",
        "            all_signals.extend([s for s in signals if s.signal_type == 'uint' and len(s.values) > 50])\n",
        "\n",
        "        relationships = []\n",
        "\n",
        "        # Check for derivative relationships\n",
        "        for i, signal1 in enumerate(all_signals):\n",
        "            for j, signal2 in enumerate(all_signals):\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                # Ensure signals have enough values\n",
        "                min_len = min(len(signal1.values), len(signal2.values))\n",
        "                if min_len < 50:\n",
        "                    continue\n",
        "\n",
        "                # Get time-aligned values\n",
        "                values1 = np.array(signal1.values[:min_len])\n",
        "                values2 = np.array(signal2.values[:min_len])\n",
        "\n",
        "                # Check for derivative relationship\n",
        "                if min_len > 2:\n",
        "                    derivative1 = np.diff(values1)\n",
        "                    correlation = np.corrcoef(derivative1, values2[1:min_len])[0, 1]\n",
        "                    if np.isnan(correlation):\n",
        "                        continue\n",
        "\n",
        "                    if abs(correlation) > 0.8:\n",
        "                        relationships.append((signal1, 'derivative', signal2, abs(correlation)))\n",
        "\n",
        "                # Check for integral relationship\n",
        "                if min_len > 2:\n",
        "                    # Compute a simple integral (cumulative sum)\n",
        "                    integral2 = np.cumsum(values2)\n",
        "                    # Compute correlation with values1\n",
        "                    correlation = np.corrcoef(values1, integral2[:min_len])[0, 1]\n",
        "                    if np.isnan(correlation):\n",
        "                        continue\n",
        "\n",
        "                    if abs(correlation) > 0.8:\n",
        "                        relationships.append((signal2, 'integral', signal1, abs(correlation)))\n",
        "\n",
        "        return relationships\n",
        "\n",
        "    def visualize_hamd_heatmap(self, can_id: str = None, figsize=(12, 8)):\n",
        "        \"\"\"\n",
        "        Visualize the HAMD (Hamming distance distribution) as a heatmap.\n",
        "\n",
        "        Args:\n",
        "            can_id: Optional CAN ID to filter by (None for all IDs)\n",
        "            figsize: Figure size tuple (width, height)\n",
        "        \"\"\"\n",
        "        # Ensure we have identified signals first\n",
        "        if not self.signals_by_id:\n",
        "            raise ValueError(\"No signals identified. Run identify_signals() first.\")\n",
        "\n",
        "        # Get signals for the specified CAN ID or all signals\n",
        "        if can_id is not None:\n",
        "            if can_id not in self.signals_by_id:\n",
        "                raise ValueError(f\"No signals found for CAN ID: {can_id}\")\n",
        "            signals = self.signals_by_id[can_id]\n",
        "        else:\n",
        "            # Flatten the dict of signals\n",
        "            signals = []\n",
        "            for sig_list in self.signals_by_id.values():\n",
        "                signals.extend(sig_list)\n",
        "\n",
        "        # Create a list of HAMD values and their bit positions\n",
        "        hamd_data = []\n",
        "        for signal in signals:\n",
        "            if signal.hamd:\n",
        "                for i, value in enumerate(signal.hamd):\n",
        "                    hamd_data.append({\n",
        "                        'can_id': signal.can_id,\n",
        "                        'bit': signal.start_bit + i,\n",
        "                        'hamd': value\n",
        "                    })\n",
        "\n",
        "        if not hamd_data:\n",
        "            print(\"No HAMD data available for visualization.\")\n",
        "            return\n",
        "\n",
        "        # Convert to DataFrame for easier plotting\n",
        "        df = pd.DataFrame(hamd_data)\n",
        "\n",
        "        # Create a pivot table for the heatmap\n",
        "        pivot_df = df.pivot_table(index='can_id', columns='bit', values='hamd', aggfunc='mean')\n",
        "\n",
        "        # Plot the heatmap\n",
        "        plt.figure(figsize=figsize)\n",
        "        sns.heatmap(pivot_df, cmap='viridis', linewidths=0.5, linecolor='black')\n",
        "        plt.title('HAMD Heatmap')\n",
        "        plt.xlabel('Bit Position')\n",
        "        plt.ylabel('CAN ID')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_correlation_graph(self, figsize=(12, 10)):\n",
        "        \"\"\"\n",
        "        Visualize the correlation graph of signals.\n",
        "\n",
        "        Args:\n",
        "            figsize: Figure size tuple (width, height)\n",
        "        \"\"\"\n",
        "        if self.correlation_graph is None:\n",
        "            self.build_correlation_graph()\n",
        "\n",
        "        if len(self.correlation_graph) == 0:\n",
        "            print(\"No correlations found to visualize.\")\n",
        "            return\n",
        "\n",
        "        # Create position layout\n",
        "        pos = nx.spring_layout(self.correlation_graph, seed=42)\n",
        "\n",
        "        # Get node colors based on signal type\n",
        "        type_colors = {\n",
        "            'uint': 'blue',\n",
        "            'enum': 'green',\n",
        "            'rand': 'red',\n",
        "            'cyclic': 'purple',\n",
        "            'unknown': 'gray'\n",
        "        }\n",
        "\n",
        "        node_colors = [\n",
        "            type_colors[self.correlation_graph.nodes[n]['signal_type']]\n",
        "            for n in self.correlation_graph.nodes\n",
        "        ]\n",
        "\n",
        "        # Get edge weights\n",
        "        edge_weights = [\n",
        "            self.correlation_graph[u][v]['weight'] * 3\n",
        "            for u, v in self.correlation_graph.edges\n",
        "        ]\n",
        "\n",
        "        # Create the plot\n",
        "        plt.figure(figsize=figsize)\n",
        "        nx.draw_networkx_nodes(\n",
        "            self.correlation_graph, pos,\n",
        "            node_color=node_colors, alpha=0.8\n",
        "        )\n",
        "        nx.draw_networkx_edges(\n",
        "            self.correlation_graph, pos,\n",
        "            width=edge_weights, alpha=0.5\n",
        "        )\n",
        "\n",
        "        # Add labels\n",
        "        labels = {\n",
        "            n: f\"{self.correlation_graph.nodes[n]['can_id']}\\n{self.correlation_graph.nodes[n]['bits']}\"\n",
        "            for n in self.correlation_graph.nodes\n",
        "        }\n",
        "        nx.draw_networkx_labels(\n",
        "            self.correlation_graph, pos, labels, font_size=8\n",
        "        )\n",
        "\n",
        "        # Add legend\n",
        "        legend_elements = [\n",
        "            plt.Line2D(\n",
        "                [0], [0], marker='o', color='w',\n",
        "                markerfacecolor=color, markersize=10, label=type_\n",
        "            )\n",
        "            for type_, color in type_colors.items()\n",
        "        ]\n",
        "        plt.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "        plt.title('Signal Correlation Graph')\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def save_results(self, output_dir: str = \"autocan_results\"):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save identified signals\n",
        "        with open(os.path.join(output_dir, 'signals.pkl'), 'wb') as f:\n",
        "            pickle.dump(self.signals_by_id, f)\n",
        "\n",
        "        # Save signal summary as CSV\n",
        "        summary_data = []\n",
        "        for can_id, signals in self.signals_by_id.items():\n",
        "            for signal in signals:\n",
        "                summary_data.append({\n",
        "                    'can_id': can_id,\n",
        "                    'start_bit': signal.start_bit,\n",
        "                    'num_bits': signal.num_bits,\n",
        "                    'signal_type': signal.signal_type,\n",
        "                    'label': signal.label or '',\n",
        "                    'min_value': signal.min_value,\n",
        "                    'max_value': signal.max_value,\n",
        "                    'avg_value': signal.avg_value,\n",
        "                    'distinct_values': signal.distinct_values,\n",
        "                    'flip_rate': signal.flip_rate\n",
        "                })\n",
        "\n",
        "        if summary_data:\n",
        "            pd.DataFrame(summary_data).to_csv(os.path.join(output_dir, 'signal_summary.csv'), index=False)\n",
        "\n",
        "        # Save correlation graph as GraphML\n",
        "        if self.correlation_graph is not None:\n",
        "            nx.write_graphml(self.correlation_graph, os.path.join(output_dir, 'correlation_graph.graphml'))\n",
        "\n",
        "        print(f\"Results saved to {output_dir}\")\n",
        "\n",
        "\n",
        "\n",
        "# Define the analyze_file function outside of main() so it can be pickled\n",
        "def analyze_file(file_path, output_dir):\n",
        "    try:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        print(f\"\\nAnalyzing {file_name}...\")\n",
        "\n",
        "        # Create output directory for this file\n",
        "        file_output_dir = os.path.join(output_dir, os.path.splitext(file_name)[0])\n",
        "        os.makedirs(file_output_dir, exist_ok=True)\n",
        "\n",
        "        # Load and analyze the trace\n",
        "        can_trace = CANTrace.from_log(file_path)\n",
        "        print(f\"Loaded {len(can_trace)} frames with {len(can_trace.get_unique_ids())} unique IDs\")\n",
        "\n",
        "        # Create analyzer and identify signals\n",
        "        analyzer = AutoCANAnalyzer(can_trace)\n",
        "        analyzer.identify_signals()\n",
        "\n",
        "        # Count identified signals\n",
        "        signal_count = sum(len(signals) for signals in analyzer.signals_by_id.values())\n",
        "        print(f\"Identified {signal_count} signals across {len(analyzer.signals_by_id)} CAN IDs\")\n",
        "\n",
        "        # Build correlation graph\n",
        "        analyzer.build_correlation_graph()\n",
        "\n",
        "        # Discover mathematical relationships\n",
        "        relationships = analyzer.discover_mathematical_relationships()\n",
        "\n",
        "        # Save the results for this file\n",
        "        analyzer.save_results(file_output_dir)\n",
        "\n",
        "        # Create and return a summary dictionary - don't try to return the full objects which might be hard to pickle\n",
        "        summary = {\n",
        "            'file_name': file_name,\n",
        "            'frame_count': len(can_trace),\n",
        "            'unique_id_count': len(can_trace.get_unique_ids()),\n",
        "            'unique_ids': list(can_trace.get_unique_ids()),\n",
        "            'signal_count': signal_count,\n",
        "            'relationship_count': len(relationships)\n",
        "        }\n",
        "\n",
        "        print(f\"Completed analysis of {file_name}\")\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Process all CAN log files in the current directory and perform comprehensive analysis.\n",
        "    \"\"\"\n",
        "    import glob\n",
        "    import os\n",
        "    from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "    import multiprocessing\n",
        "\n",
        "    # Get all log files in the current directory\n",
        "    log_files = glob.glob(\"*.log\")\n",
        "\n",
        "    if not log_files:\n",
        "        print(\"No log files found in the current directory.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(log_files)} log files: {', '.join(log_files)}\")\n",
        "\n",
        "    output_dir = \"autocan_results\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Process files sequentially if we encounter pickling issues\n",
        "    use_parallel = True\n",
        "    summaries = []\n",
        "\n",
        "    if use_parallel:\n",
        "        try:\n",
        "            # Determine number of processors to use\n",
        "            num_processors = min(multiprocessing.cpu_count(), len(log_files))\n",
        "            print(f\"Using {num_processors} processors for parallel analysis\")\n",
        "\n",
        "            # Process files in parallel\n",
        "            with ProcessPoolExecutor(max_workers=num_processors) as executor:\n",
        "                # Create a list of futures\n",
        "                futures = [executor.submit(analyze_file, file_path, output_dir) for file_path in log_files]\n",
        "\n",
        "                for future in as_completed(futures):\n",
        "                    try:\n",
        "                        result = future.result()\n",
        "                        if result:\n",
        "                            summaries.append(result)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error in parallel processing: {e}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error setting up parallel processing: {e}\")\n",
        "            print(\"Falling back to sequential processing...\")\n",
        "            use_parallel = False\n",
        "\n",
        "    # If parallel processing fails or is disabled, use sequential processing\n",
        "    if not use_parallel:\n",
        "        print(\"Processing files sequentially...\")\n",
        "        for file_path in log_files:\n",
        "            result = analyze_file(file_path, output_dir)\n",
        "            if result:\n",
        "                summaries.append(result)\n",
        "\n",
        "    # Generate combined report\n",
        "    if summaries:\n",
        "        print(\"\\nGenerating combined summary...\")\n",
        "        combined_dir = os.path.join(output_dir, \"combined_results\")\n",
        "        os.makedirs(combined_dir, exist_ok=True)\n",
        "\n",
        "        # Save summary as CSV\n",
        "        import pandas as pd\n",
        "        pd.DataFrame(summaries).to_csv(os.path.join(combined_dir, 'file_statistics.csv'), index=False)\n",
        "        print(f\"File statistics saved to {os.path.join(combined_dir, 'file_statistics.csv')}\")\n",
        "\n",
        "        # Find common CAN IDs across all files\n",
        "        common_can_ids = set()\n",
        "        if summaries:\n",
        "            # Start with IDs from the first file\n",
        "            common_can_ids = set(summaries[0]['unique_ids'])\n",
        "\n",
        "            # Intersect with IDs from all other files\n",
        "            for summary in summaries[1:]:\n",
        "                common_can_ids &= set(summary['unique_ids'])\n",
        "\n",
        "        if common_can_ids:\n",
        "            print(f\"\\nFound {len(common_can_ids)} CAN IDs common across all files:\")\n",
        "            print(\", \".join(sorted(common_can_ids)))\n",
        "        else:\n",
        "            print(\"\\nNo CAN IDs common across all files.\")\n",
        "\n",
        "        # Create visualizations for the combined results\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "\n",
        "        # Make sure the visualizations directory exists\n",
        "        viz_dir = os.path.join(output_dir, \"visualizations\")\n",
        "        os.makedirs(viz_dir, exist_ok=True)\n",
        "\n",
        "        # Create bar chart of signal counts per file\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        file_names = [summary['file_name'] for summary in summaries]\n",
        "        signal_counts = [summary['signal_count'] for summary in summaries]\n",
        "\n",
        "        # Sort by signal count for better visualization\n",
        "        sorted_indices = np.argsort(signal_counts)[::-1]  # descending order\n",
        "        sorted_file_names = [file_names[i] for i in sorted_indices]\n",
        "        sorted_signal_counts = [signal_counts[i] for i in sorted_indices]\n",
        "\n",
        "        plt.bar(range(len(sorted_file_names)), sorted_signal_counts)\n",
        "        plt.xticks(range(len(sorted_file_names)), sorted_file_names, rotation=90)\n",
        "        plt.xlabel('File')\n",
        "        plt.ylabel('Signal Count')\n",
        "        plt.title('Number of Signals per File')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(viz_dir, 'signal_counts.png'))\n",
        "        plt.close()\n",
        "\n",
        "        # Visualize relationship counts\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        relationship_counts = [summary['relationship_count'] for summary in summaries]\n",
        "\n",
        "        # Sort by relationship count\n",
        "        sorted_indices = np.argsort(relationship_counts)[::-1]  # descending order\n",
        "        sorted_file_names = [file_names[i] for i in sorted_indices]\n",
        "        sorted_rel_counts = [relationship_counts[i] for i in sorted_indices]\n",
        "\n",
        "        plt.bar(range(len(sorted_file_names)), sorted_rel_counts)\n",
        "        plt.xticks(range(len(sorted_file_names)), sorted_file_names, rotation=90)\n",
        "        plt.xlabel('File')\n",
        "        plt.ylabel('Relationship Count')\n",
        "        plt.title('Number of Mathematical Relationships per File')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(viz_dir, 'relationship_counts.png'))\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"\\nVisualization plots saved to {viz_dir}\")\n",
        "    else:\n",
        "        print(\"\\nNo valid results to summarize.\")\n",
        "\n",
        "    print(\"\\nAnalysis complete! Results are in the 'autocan_results' directory.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgSeJFmp127c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eacb7490-8303-43c5-8d33-6498c51f2a96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exploring results in autocan_results...\n",
            "Found 27 analysis directories.\n",
            "\n",
            "Processing Gear_shifter_attack_2...\n",
            "Loaded signal summary: 217 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Gear_shifter_attack_2_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Gear_shifter_attack_2_bit_widths.png\n",
            "Loaded signals pickle: 54 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Gear_shifter_attack_2_sample_signals.png\n",
            "Loaded correlation graph: 217 nodes, 101 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Gear_shifter_attack_2_correlation_graph.png\n",
            "\n",
            "Processing Reverse_speedometer_fuzzing_attack...\n",
            "Loaded signal summary: 188 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Reverse_speedometer_fuzzing_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Reverse_speedometer_fuzzing_attack_bit_widths.png\n",
            "Loaded signals pickle: 54 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Reverse_speedometer_fuzzing_attack_sample_signals.png\n",
            "Loaded correlation graph: 188 nodes, 56 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Reverse_speedometer_fuzzing_attack_correlation_graph.png\n",
            "\n",
            "Processing FCA_warning_attack...\n",
            "Loaded signal summary: 198 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/FCA_warning_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/FCA_warning_attack_bit_widths.png\n",
            "Loaded signals pickle: 56 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/FCA_warning_attack_sample_signals.png\n",
            "Loaded correlation graph: 198 nodes, 35 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/FCA_warning_attack_correlation_graph.png\n",
            "\n",
            "Processing EMS_attack...\n",
            "Loaded signal summary: 213 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/EMS_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/EMS_attack_bit_widths.png\n",
            "Loaded signals pickle: 56 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/EMS_attack_sample_signals.png\n",
            "Loaded correlation graph: 213 nodes, 62 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/EMS_attack_correlation_graph.png\n",
            "\n",
            "Processing Drive_mode_changing_attack...\n",
            "Loaded signal summary: 212 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Drive_mode_changing_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Drive_mode_changing_attack_bit_widths.png\n",
            "Loaded signals pickle: 56 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Drive_mode_changing_attack_sample_signals.png\n",
            "Loaded correlation graph: 212 nodes, 89 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Drive_mode_changing_attack_correlation_graph.png\n",
            "\n",
            "Processing Steering_angle_replay...\n",
            "Loaded signal summary: 201 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Steering_angle_replay_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Steering_angle_replay_bit_widths.png\n",
            "Loaded signals pickle: 56 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Steering_angle_replay_sample_signals.png\n",
            "Loaded correlation graph: 201 nodes, 51 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Steering_angle_replay_correlation_graph.png\n",
            "\n",
            "Processing Power_steering_attack...\n",
            "Loaded signal summary: 218 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Power_steering_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Power_steering_attack_bit_widths.png\n",
            "Loaded signals pickle: 56 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Power_steering_attack_sample_signals.png\n",
            "Loaded correlation graph: 218 nodes, 60 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Power_steering_attack_correlation_graph.png\n",
            "\n",
            "Processing Fuzzing_valid_IDs...\n",
            "Loaded signal summary: 224 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Fuzzing_valid_IDs_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Fuzzing_valid_IDs_bit_widths.png\n",
            "Loaded signals pickle: 56 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Fuzzing_valid_IDs_sample_signals.png\n",
            "Loaded correlation graph: 224 nodes, 114 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Fuzzing_valid_IDs_correlation_graph.png\n",
            "\n",
            "Processing Wiper_warning_attack...\n",
            "Loaded signal summary: 219 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Wiper_warning_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Wiper_warning_attack_bit_widths.png\n",
            "Loaded signals pickle: 56 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Wiper_warning_attack_sample_signals.png\n",
            "Loaded correlation graph: 219 nodes, 61 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Wiper_warning_attack_correlation_graph.png\n",
            "\n",
            "Processing Steering_angle_attack...\n",
            "Loaded signal summary: 204 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Steering_angle_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Steering_angle_attack_bit_widths.png\n",
            "Loaded signals pickle: 56 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Steering_angle_attack_sample_signals.png\n",
            "Loaded correlation graph: 204 nodes, 65 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Steering_angle_attack_correlation_graph.png\n",
            "\n",
            "Processing Gear_shifter_attack_1...\n",
            "Loaded signal summary: 206 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Gear_shifter_attack_1_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Gear_shifter_attack_1_bit_widths.png\n",
            "Loaded signals pickle: 54 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Gear_shifter_attack_1_sample_signals.png\n",
            "Loaded correlation graph: 206 nodes, 77 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Gear_shifter_attack_1_correlation_graph.png\n",
            "\n",
            "Processing DoS_attack...\n",
            "Loaded signal summary: 197 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/DoS_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/DoS_attack_bit_widths.png\n",
            "Loaded signals pickle: 55 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/DoS_attack_sample_signals.png\n",
            "Loaded correlation graph: 197 nodes, 58 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/DoS_attack_correlation_graph.png\n",
            "\n",
            "Processing Fuzzing_random_IDs...\n",
            "Loaded signal summary: 637 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Fuzzing_random_IDs_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Fuzzing_random_IDs_bit_widths.png\n",
            "Loaded signals pickle: 480 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Fuzzing_random_IDs_sample_signals.png\n",
            "Loaded correlation graph: 637 nodes, 76 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Fuzzing_random_IDs_correlation_graph.png\n",
            "\n",
            "Processing Max_speedometer_attack...\n",
            "Loaded signal summary: 215 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Max_speedometer_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Max_speedometer_attack_bit_widths.png\n",
            "Loaded signals pickle: 56 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Max_speedometer_attack_sample_signals.png\n",
            "Loaded correlation graph: 215 nodes, 73 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Max_speedometer_attack_correlation_graph.png\n",
            "\n",
            "Processing visualizations...\n",
            "No signal summary found.\n",
            "No signals pickle found.\n",
            "No correlation graph found.\n",
            "\n",
            "Processing EMS_replay_attack...\n",
            "Loaded signal summary: 210 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/EMS_replay_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/EMS_replay_attack_bit_widths.png\n",
            "Loaded signals pickle: 56 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/EMS_replay_attack_sample_signals.png\n",
            "Loaded correlation graph: 210 nodes, 56 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/EMS_replay_attack_correlation_graph.png\n",
            "\n",
            "Processing Min_speedometer_attack_3...\n",
            "Loaded signal summary: 213 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Min_speedometer_attack_3_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Min_speedometer_attack_3_bit_widths.png\n",
            "Loaded signals pickle: 54 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Min_speedometer_attack_3_sample_signals.png\n",
            "Loaded correlation graph: 213 nodes, 76 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Min_speedometer_attack_3_correlation_graph.png\n",
            "\n",
            "Processing Multiple_attacks_1...\n",
            "Loaded signal summary: 222 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Multiple_attacks_1_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Multiple_attacks_1_bit_widths.png\n",
            "Loaded signals pickle: 54 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Multiple_attacks_1_sample_signals.png\n",
            "Loaded correlation graph: 222 nodes, 64 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Multiple_attacks_1_correlation_graph.png\n",
            "\n",
            "Processing Break_warning_attack...\n",
            "Loaded signal summary: 205 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Break_warning_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Break_warning_attack_bit_widths.png\n",
            "Loaded signals pickle: 56 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Break_warning_attack_sample_signals.png\n",
            "Loaded correlation graph: 205 nodes, 57 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Break_warning_attack_correlation_graph.png\n",
            "\n",
            "Processing Fuzzing_valid_IDs_DoS...\n",
            "Loaded signal summary: 209 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Fuzzing_valid_IDs_DoS_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Fuzzing_valid_IDs_DoS_bit_widths.png\n",
            "Loaded signals pickle: 55 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Fuzzing_valid_IDs_DoS_sample_signals.png\n",
            "Loaded correlation graph: 209 nodes, 71 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Fuzzing_valid_IDs_DoS_correlation_graph.png\n",
            "\n",
            "Processing EMS_replay_long_attack...\n",
            "Loaded signal summary: 214 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/EMS_replay_long_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/EMS_replay_long_attack_bit_widths.png\n",
            "Loaded signals pickle: 54 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/EMS_replay_long_attack_sample_signals.png\n",
            "Loaded correlation graph: 214 nodes, 59 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/EMS_replay_long_attack_correlation_graph.png\n",
            "\n",
            "Processing Multiple_attacks_2...\n",
            "Loaded signal summary: 220 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Multiple_attacks_2_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Multiple_attacks_2_bit_widths.png\n",
            "Loaded signals pickle: 54 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Multiple_attacks_2_sample_signals.png\n",
            "Loaded correlation graph: 220 nodes, 57 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Multiple_attacks_2_correlation_graph.png\n",
            "\n",
            "Processing Min_speedometer_attack_2...\n",
            "Loaded signal summary: 201 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Min_speedometer_attack_2_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Min_speedometer_attack_2_bit_widths.png\n",
            "Loaded signals pickle: 54 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Min_speedometer_attack_2_sample_signals.png\n",
            "Loaded correlation graph: 201 nodes, 62 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Min_speedometer_attack_2_correlation_graph.png\n",
            "\n",
            "Processing Door_open_warning_attack...\n",
            "Loaded signal summary: 214 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Door_open_warning_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Door_open_warning_attack_bit_widths.png\n",
            "Loaded signals pickle: 54 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Door_open_warning_attack_sample_signals.png\n",
            "Loaded correlation graph: 214 nodes, 71 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Door_open_warning_attack_correlation_graph.png\n",
            "\n",
            "Processing Min_speedometer_attack_1...\n",
            "Loaded signal summary: 202 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Min_speedometer_attack_1_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Min_speedometer_attack_1_bit_widths.png\n",
            "Loaded signals pickle: 54 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Min_speedometer_attack_1_sample_signals.png\n",
            "Loaded correlation graph: 202 nodes, 59 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Min_speedometer_attack_1_correlation_graph.png\n",
            "\n",
            "Processing Parking_break_attack...\n",
            "Loaded signal summary: 213 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Parking_break_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Parking_break_attack_bit_widths.png\n",
            "Loaded signals pickle: 56 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Parking_break_attack_sample_signals.png\n",
            "Loaded correlation graph: 213 nodes, 78 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Parking_break_attack_correlation_graph.png\n",
            "\n",
            "Processing Break_and_fog_light_attack...\n",
            "Loaded signal summary: 206 signals\n",
            "Saved signal type distribution to autocan_results/interactive_visualizations/Break_and_fog_light_attack_signal_types.png\n",
            "Saved bit width distribution to autocan_results/interactive_visualizations/Break_and_fog_light_attack_bit_widths.png\n",
            "Loaded signals pickle: 56 CAN IDs\n",
            "Saved sample signal plot to autocan_results/interactive_visualizations/Break_and_fog_light_attack_sample_signals.png\n",
            "Loaded correlation graph: 206 nodes, 53 edges\n",
            "Saved correlation graph to autocan_results/interactive_visualizations/Break_and_fog_light_attack_correlation_graph.png\n",
            "Saved overall signal types heatmap to autocan_results/interactive_visualizations/overall_signal_types_heatmap.png\n",
            "Saved analysis summary to autocan_results/interactive_visualizations/analysis_summary.csv\n",
            "\n",
            "All visualizations saved to autocan_results/interactive_visualizations\n",
            "Analyzing log files in ....\n",
            "Found 26 log files.\n",
            "\n",
            "Analyzing Fuzzing_valid_IDs.log...\n",
            "Processed 257413 frames, 56 unique IDs, 134.02 seconds\n",
            "\n",
            "Analyzing Steering_angle_attack.log...\n",
            "Processed 369630 frames, 56 unique IDs, 190.26 seconds\n",
            "\n",
            "Analyzing FCA_warning_attack.log...\n",
            "Processed 581637 frames, 56 unique IDs, 298.64 seconds\n",
            "\n",
            "Analyzing Steering_angle_replay.log...\n",
            "Processed 475808 frames, 56 unique IDs, 243.84 seconds\n",
            "\n",
            "Analyzing Gear_shifter_attack_1.log...\n",
            "Processed 438213 frames, 54 unique IDs, 221.69 seconds\n",
            "\n",
            "Analyzing Door_open_warning_attack.log...\n",
            "Processed 406609 frames, 54 unique IDs, 209.07 seconds\n",
            "\n",
            "Analyzing DoS_attack.log...\n",
            "Processed 341787 frames, 55 unique IDs, 173.89 seconds\n",
            "\n",
            "Analyzing Min_speedometer_attack_2.log...\n",
            "Processed 578618 frames, 54 unique IDs, 300.93 seconds\n",
            "\n",
            "Analyzing Fuzzing_valid_IDs_DoS.log...\n",
            "Processed 441332 frames, 55 unique IDs, 232.15 seconds\n",
            "\n",
            "Analyzing Max_speedometer_attack.log...\n",
            "Processed 417094 frames, 56 unique IDs, 216.43 seconds\n",
            "\n",
            "Analyzing Break_and_fog_light_attack.log...\n",
            "Processed 508148 frames, 56 unique IDs, 266.01 seconds\n",
            "\n",
            "Analyzing EMS_attack.log...\n",
            "Processed 325678 frames, 56 unique IDs, 166.41 seconds\n",
            "\n",
            "Analyzing Min_speedometer_attack_3.log...\n",
            "Processed 279517 frames, 54 unique IDs, 146.29 seconds\n",
            "\n",
            "Analyzing Drive_mode_changing_attack.log...\n",
            "Processed 238877 frames, 56 unique IDs, 123.61 seconds\n",
            "\n",
            "Analyzing Parking_break_attack.log...\n",
            "Processed 146664 frames, 56 unique IDs, 76.24 seconds\n",
            "\n",
            "Analyzing EMS_replay_attack.log...\n",
            "Processed 324147 frames, 56 unique IDs, 166.38 seconds\n",
            "\n",
            "Analyzing Gear_shifter_attack_2.log...\n",
            "Processed 406438 frames, 54 unique IDs, 208.34 seconds\n",
            "\n",
            "Analyzing Fuzzing_random_IDs.log...\n",
            "Processed 729007 frames, 646 unique IDs, 377.63 seconds\n",
            "\n",
            "Analyzing Wiper_warning_attack.log...\n",
            "Processed 233555 frames, 56 unique IDs, 122.11 seconds\n",
            "\n",
            "Analyzing Multiple_attacks_1.log...\n",
            "Processed 1672671 frames, 54 unique IDs, 872.58 seconds\n",
            "\n",
            "Analyzing Reverse_speedometer_fuzzing_attack.log...\n",
            "Processed 177130 frames, 54 unique IDs, 92.54 seconds\n",
            "\n",
            "Analyzing Multiple_attacks_2.log...\n",
            "Processed 2093747 frames, 620 unique IDs, 1085.43 seconds\n",
            "\n",
            "Analyzing Min_speedometer_attack_1.log...\n",
            "Processed 545604 frames, 54 unique IDs, 283.42 seconds\n",
            "\n",
            "Analyzing Break_warning_attack.log...\n",
            "Processed 588592 frames, 56 unique IDs, 302.42 seconds\n",
            "\n",
            "Analyzing EMS_replay_long_attack.log...\n",
            "Processed 2051085 frames, 54 unique IDs, 1058.97 seconds\n",
            "\n",
            "Analyzing Power_steering_attack.log...\n",
            "Processed 361198 frames, 56 unique IDs, 187.48 seconds\n",
            "Saved log summary to autocan_results/log_analysis/log_summary.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import Normalize\n",
        "from matplotlib.cm import ScalarMappable\n",
        "\n",
        "def explore_autocan_results(results_dir=\"autocan_results\"):\n",
        "    \"\"\"\n",
        "    Load and visualize the saved AutoCAN analysis results.\n",
        "\n",
        "    Args:\n",
        "        results_dir: The directory containing the analysis results\n",
        "    \"\"\"\n",
        "    print(f\"Exploring results in {results_dir}...\")\n",
        "\n",
        "    # Find all subdirectories (one per log file)\n",
        "    subdirs = [d for d in os.listdir(results_dir)\n",
        "               if os.path.isdir(os.path.join(results_dir, d)) and d != \"combined_results\"]\n",
        "\n",
        "    if not subdirs:\n",
        "        print(\"No analysis subdirectories found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(subdirs)} analysis directories.\")\n",
        "\n",
        "    # Create a directory for the visualization outputs\n",
        "    viz_output_dir = os.path.join(results_dir, \"interactive_visualizations\")\n",
        "    os.makedirs(viz_output_dir, exist_ok=True)\n",
        "\n",
        "    # Aggregate statistics\n",
        "    stats_data = []\n",
        "\n",
        "    # Process each directory (each corresponds to a log file)\n",
        "    for subdir in subdirs:\n",
        "        subdir_path = os.path.join(results_dir, subdir)\n",
        "        print(f\"\\nProcessing {subdir}...\")\n",
        "\n",
        "        # Load signal summary if available\n",
        "        csv_path = os.path.join(subdir_path, \"signal_summary.csv\")\n",
        "        if os.path.exists(csv_path):\n",
        "            try:\n",
        "                signal_df = pd.read_csv(csv_path)\n",
        "                print(f\"Loaded signal summary: {len(signal_df)} signals\")\n",
        "\n",
        "                # Extract statistics\n",
        "                signal_types = signal_df['signal_type'].value_counts().to_dict()\n",
        "                signal_stats = {\n",
        "                    'file_name': subdir,\n",
        "                    'total_signals': len(signal_df),\n",
        "                    'unique_can_ids': len(signal_df['can_id'].unique())\n",
        "                }\n",
        "                signal_stats.update({f'type_{k}': v for k, v in signal_types.items()})\n",
        "                stats_data.append(signal_stats)\n",
        "\n",
        "                # Create signal type distribution pie chart\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                signal_df['signal_type'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
        "                plt.title(f'Signal Type Distribution - {subdir}')\n",
        "                plt.ylabel('')  # Hide \"None\" label\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(viz_output_dir, f\"{subdir}_signal_types.png\"))\n",
        "                print(f\"Saved signal type distribution to {os.path.join(viz_output_dir, f'{subdir}_signal_types.png')}\")\n",
        "                plt.close()\n",
        "\n",
        "                # Create signal bit width histogram\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                signal_df['num_bits'].hist(bins=20)\n",
        "                plt.title(f'Signal Bit Width Distribution - {subdir}')\n",
        "                plt.xlabel('Number of Bits')\n",
        "                plt.ylabel('Count')\n",
        "                plt.grid(axis='y', alpha=0.75)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(viz_output_dir, f\"{subdir}_bit_widths.png\"))\n",
        "                print(f\"Saved bit width distribution to {os.path.join(viz_output_dir, f'{subdir}_bit_widths.png')}\")\n",
        "                plt.close()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing signal summary: {e}\")\n",
        "        else:\n",
        "            print(\"No signal summary found.\")\n",
        "\n",
        "        # Load signals pickle if available\n",
        "        pkl_path = os.path.join(subdir_path, \"signals.pkl\")\n",
        "        if os.path.exists(pkl_path):\n",
        "            try:\n",
        "                with open(pkl_path, 'rb') as f:\n",
        "                    signals_by_id = pickle.load(f)\n",
        "                print(f\"Loaded signals pickle: {len(signals_by_id)} CAN IDs\")\n",
        "\n",
        "                # Example: Plot signal values for a sample CAN ID\n",
        "                if signals_by_id:\n",
        "                    # Take the first CAN ID with signals\n",
        "                    sample_can_id = next(iter(signals_by_id))\n",
        "                    sample_signals = signals_by_id[sample_can_id]\n",
        "\n",
        "                    if sample_signals:\n",
        "                        plt.figure(figsize=(12, 6))\n",
        "                        for i, signal in enumerate(sample_signals[:5]):  # Plot up to 5 signals\n",
        "                            if signal.values:\n",
        "                                plt.plot(signal.values[:100], label=f\"Signal {i} [{signal.start_bit}:{signal.start_bit+signal.num_bits}]\")\n",
        "\n",
        "                        plt.title(f'Sample Signal Values - CAN ID: {sample_can_id}')\n",
        "                        plt.xlabel('Frame Index')\n",
        "                        plt.ylabel('Signal Value')\n",
        "                        plt.legend()\n",
        "                        plt.grid(True, alpha=0.3)\n",
        "                        plt.tight_layout()\n",
        "                        plt.savefig(os.path.join(viz_output_dir, f\"{subdir}_sample_signals.png\"))\n",
        "                        print(f\"Saved sample signal plot to {os.path.join(viz_output_dir, f'{subdir}_sample_signals.png')}\")\n",
        "                        plt.close()\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing signals pickle: {e}\")\n",
        "        else:\n",
        "            print(\"No signals pickle found.\")\n",
        "\n",
        "        # Load correlation graph if available\n",
        "        graphml_path = os.path.join(subdir_path, \"correlation_graph.graphml\")\n",
        "        if os.path.exists(graphml_path):\n",
        "            try:\n",
        "                G = nx.read_graphml(graphml_path)\n",
        "                print(f\"Loaded correlation graph: {len(G.nodes)} nodes, {len(G.edges)} edges\")\n",
        "\n",
        "                # Visualize correlation graph\n",
        "                plt.figure(figsize=(12, 10))\n",
        "\n",
        "                # Create position layout\n",
        "                pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "                # Get node colors based on signal type\n",
        "                type_colors = {\n",
        "                    'uint': 'blue',\n",
        "                    'enum': 'green',\n",
        "                    'rand': 'red',\n",
        "                    'cyclic': 'purple',\n",
        "                    'unknown': 'gray'\n",
        "                }\n",
        "\n",
        "                # Extract node types\n",
        "                node_colors = []\n",
        "                for node in G.nodes():\n",
        "                    node_type = G.nodes[node].get('signal_type', 'unknown')\n",
        "                    node_colors.append(type_colors.get(node_type, 'gray'))\n",
        "\n",
        "                # Get edge weights\n",
        "                edge_weights = []\n",
        "                for u, v in G.edges():\n",
        "                    weight = float(G.edges[u, v].get('weight', 1.0))\n",
        "                    edge_weights.append(weight * 2)\n",
        "\n",
        "                # Plot the graph\n",
        "                nx.draw_networkx_nodes(G, pos, node_color=node_colors, alpha=0.8, node_size=100)\n",
        "                nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.5, edge_color='gray')\n",
        "\n",
        "                # Create a custom legend\n",
        "                for signal_type, color in type_colors.items():\n",
        "                    plt.plot([], [], 'o', color=color, label=signal_type)\n",
        "\n",
        "                plt.legend(loc='upper right')\n",
        "                plt.title(f'Signal Correlation Graph - {subdir}')\n",
        "                plt.axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(viz_output_dir, f\"{subdir}_correlation_graph.png\"))\n",
        "                print(f\"Saved correlation graph to {os.path.join(viz_output_dir, f'{subdir}_correlation_graph.png')}\")\n",
        "                plt.close()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing correlation graph: {e}\")\n",
        "        else:\n",
        "            print(\"No correlation graph found.\")\n",
        "\n",
        "    # Create a combined statistics table\n",
        "    if stats_data:\n",
        "        stats_df = pd.DataFrame(stats_data)\n",
        "\n",
        "        # Create a heatmap of signal counts by type for each file\n",
        "        plt.figure(figsize=(14, 10))\n",
        "\n",
        "        # Prepare data for heatmap\n",
        "        type_columns = [col for col in stats_df.columns if col.startswith('type_')]\n",
        "        if type_columns:\n",
        "            heatmap_data = stats_df.set_index('file_name')[type_columns]\n",
        "\n",
        "            # Rename columns for better display\n",
        "            heatmap_data.columns = [col.replace('type_', '') for col in heatmap_data.columns]\n",
        "\n",
        "            # Create heatmap\n",
        "            sns.heatmap(heatmap_data, annot=True, fmt='g', cmap='viridis')\n",
        "            plt.title('Signal Types by File')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(viz_output_dir, \"overall_signal_types_heatmap.png\"))\n",
        "            print(f\"Saved overall signal types heatmap to {os.path.join(viz_output_dir, 'overall_signal_types_heatmap.png')}\")\n",
        "            plt.close()\n",
        "\n",
        "        # Save the stats summary\n",
        "        stats_df.to_csv(os.path.join(viz_output_dir, \"analysis_summary.csv\"), index=False)\n",
        "        print(f\"Saved analysis summary to {os.path.join(viz_output_dir, 'analysis_summary.csv')}\")\n",
        "\n",
        "    print(f\"\\nAll visualizations saved to {viz_output_dir}\")\n",
        "\n",
        "# If you also want to analyze the log files directly (not just the results):\n",
        "def analyze_log_files(log_dir=\".\"):\n",
        "    \"\"\"\n",
        "    Analyze the CAN log files directly to extract basic statistics.\n",
        "\n",
        "    Args:\n",
        "        log_dir: Directory containing the .log files\n",
        "    \"\"\"\n",
        "    print(f\"Analyzing log files in {log_dir}...\")\n",
        "\n",
        "    # Find all log files\n",
        "    log_files = glob.glob(os.path.join(log_dir, \"*.log\"))\n",
        "\n",
        "    if not log_files:\n",
        "        print(\"No log files found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(log_files)} log files.\")\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = os.path.join(\"autocan_results\", \"log_analysis\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Process each log file\n",
        "    log_stats = []\n",
        "\n",
        "    for log_file in log_files:\n",
        "        file_name = os.path.basename(log_file)\n",
        "        print(f\"\\nAnalyzing {file_name}...\")\n",
        "\n",
        "        # Basic stats to collect\n",
        "        frame_count = 0\n",
        "        unique_ids = set()\n",
        "        timestamps = []\n",
        "\n",
        "        try:\n",
        "            with open(log_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if not line:\n",
        "                        continue\n",
        "\n",
        "                    if line.startswith('('):\n",
        "                        end_idx = line.find(')')\n",
        "                        try:\n",
        "                            ts_str = line[1:end_idx]\n",
        "                            timestamp = float(ts_str)\n",
        "                            timestamps.append(timestamp)\n",
        "                        except:\n",
        "                            continue\n",
        "\n",
        "                        remaining = line[end_idx+1:].strip()\n",
        "                        parts = remaining.split()\n",
        "                        if len(parts) < 2:\n",
        "                            continue\n",
        "\n",
        "                        id_payload = parts[1] if '#' in parts[1] else parts[0]\n",
        "                        if '#' not in id_payload:\n",
        "                            continue\n",
        "\n",
        "                        id_str, _ = id_payload.split('#', 1)\n",
        "                        try:\n",
        "                            can_id = int(id_str, 16)\n",
        "                            unique_ids.add(can_id)\n",
        "                            frame_count += 1\n",
        "                        except:\n",
        "                            continue\n",
        "\n",
        "            # Calculate statistics\n",
        "            duration = max(timestamps) - min(timestamps) if timestamps else 0\n",
        "            avg_freq = frame_count / duration if duration > 0 else 0\n",
        "\n",
        "            log_stats.append({\n",
        "                'file_name': file_name,\n",
        "                'frame_count': frame_count,\n",
        "                'unique_id_count': len(unique_ids),\n",
        "                'duration_sec': duration,\n",
        "                'avg_frames_per_sec': avg_freq\n",
        "            })\n",
        "\n",
        "            print(f\"Processed {frame_count} frames, {len(unique_ids)} unique IDs, {duration:.2f} seconds\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing log file: {e}\")\n",
        "\n",
        "    # Create a summary DataFrame\n",
        "    if log_stats:\n",
        "        logs_df = pd.DataFrame(log_stats)\n",
        "\n",
        "        # Save summary\n",
        "        logs_df.to_csv(os.path.join(output_dir, \"log_summary.csv\"), index=False)\n",
        "        print(f\"Saved log summary to {os.path.join(output_dir, 'log_summary.csv')}\")\n",
        "\n",
        "        # Create visualizations\n",
        "\n",
        "        # Bar chart of frame counts\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        logs_df.sort_values('frame_count', ascending=False, inplace=True)\n",
        "        plt.bar(logs_df['file_name'], logs_df['frame_count'], color='skyblue')\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.title('Frame Count by Log File')\n",
        "        plt.xlabel('Log File')\n",
        "        plt.ylabel('Frame Count')\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"frame_counts.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Bar chart of unique ID counts\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        logs_df.sort_values('unique_id_count', ascending=False, inplace=True)\n",
        "        plt.bar(logs_df['file_name'], logs_df['unique_id_count'], color='lightgreen')\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.title('Unique CAN ID Count by Log File')\n",
        "        plt.xlabel('Log File')\n",
        "        plt.ylabel('Unique CAN ID Count')\n",
        "        plt.grid(axis='y', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"unique_id_counts.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # Scatter plot of frame count vs. unique ID count\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        ax = plt.gca()  # Get the current axes\n",
        "        scatter = plt.scatter(logs_df['unique_id_count'], logs_df['frame_count'],\n",
        "                            alpha=0.7, s=100, c=logs_df['avg_frames_per_sec'], cmap='viridis')\n",
        "\n",
        "        # Add file names as labels\n",
        "        for i, row in logs_df.iterrows():\n",
        "            plt.annotate(row['file_name'],\n",
        "                        (row['unique_id_count'], row['frame_count']),\n",
        "                        xytext=(5, 5), textcoords='offset points',\n",
        "                        fontsize=8)\n",
        "\n",
        "        plt.title('Frame Count vs. Unique ID Count by Log File')\n",
        "        plt.xlabel('Unique ID Count')\n",
        "        plt.ylabel('Frame Count')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        # Add a colorbar to show frame frequency - properly specifying the axes\n",
        "        cbar = plt.colorbar(scatter, ax=ax)\n",
        "        cbar.set_label('Average Frames Per Second')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, \"frame_vs_id_count.png\"))\n",
        "        plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Explore the analysis results\n",
        "    explore_autocan_results()\n",
        "\n",
        "    # Optionally analyze the log files directly\n",
        "    analyze_log_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fflTL3N13rlr",
        "outputId": "22483e39-5592-42ac-d7ef-a3f5ca48c938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exploring results in autocan_results...\n",
            "Found 26 analysis directories.\n",
            "\n",
            "Available files:\n",
            "1. Gear_shifter_attack_2\n",
            "2. Reverse_speedometer_fuzzing_attack\n",
            "3. FCA_warning_attack\n",
            "4. EMS_attack\n",
            "5. Drive_mode_changing_attack\n",
            "6. Steering_angle_replay\n",
            "7. Power_steering_attack\n",
            "8. Fuzzing_valid_IDs\n",
            "9. Wiper_warning_attack\n",
            "10. Steering_angle_attack\n",
            "11. Gear_shifter_attack_1\n",
            "12. DoS_attack\n",
            "13. Fuzzing_random_IDs\n",
            "14. Max_speedometer_attack\n",
            "15. EMS_replay_attack\n",
            "16. Min_speedometer_attack_3\n",
            "17. Multiple_attacks_1\n",
            "18. Break_warning_attack\n",
            "19. Fuzzing_valid_IDs_DoS\n",
            "20. EMS_replay_long_attack\n",
            "21. Multiple_attacks_2\n",
            "22. Min_speedometer_attack_2\n",
            "23. Door_open_warning_attack\n",
            "24. Min_speedometer_attack_1\n",
            "25. Parking_break_attack\n",
            "26. Break_and_fog_light_attack\n",
            "\n",
            "Enter the number of the file to display (0 to exit): 26\n",
            "\n",
            "Displaying results for Break_and_fog_light_attack...\n",
            "Loaded correlation graph: 206 nodes, 53 edges\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded signals pickle: 56 CAN IDs\n",
            "\n",
            "Available CAN IDs:\n",
            "1. 595 (5 signals)\n",
            "2. 5FF (5 signals)\n",
            "3. 57F (1 signals)\n",
            "4. 372 (3 signals)\n",
            "5. 386 (8 signals)\n",
            "6. 130 (7 signals)\n",
            "7. 507 (1 signals)\n",
            "8. 553 (3 signals)\n",
            "9. 38D (2 signals)\n",
            "10. 596 (5 signals)\n",
            "11. 500 (1 signals)\n",
            "12. 381 (9 signals)\n",
            "13. 544 (1 signals)\n",
            "14. 4F1 (5 signals)\n",
            "15. 4A7 (1 signals)\n",
            "16. 470 (13 signals)\n",
            "17. 50A (1 signals)\n",
            "18. 251 (9 signals)\n",
            "19. 421 (4 signals)\n",
            "20. 140 (4 signals)\n",
            "...and 36 more\n",
            "\n",
            "Enter the number of the CAN ID to display (0 to skip): 0\n",
            "Loaded signal summary: 206 signals\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "View another file? (y/n): n\n"
          ]
        }
      ],
      "source": [
        "def explore_autocan_results_interactive(results_dir=\"autocan_results\", file_to_display=None):\n",
        "    \"\"\"\n",
        "    Load and display the saved AutoCAN analysis results interactively.\n",
        "\n",
        "    Args:\n",
        "        results_dir: The directory containing the analysis results\n",
        "        file_to_display: Specific file to display (None to select interactively)\n",
        "    \"\"\"\n",
        "    print(f\"Exploring results in {results_dir}...\")\n",
        "\n",
        "    # Find all subdirectories (one per log file)\n",
        "    subdirs = [d for d in os.listdir(results_dir)\n",
        "               if os.path.isdir(os.path.join(results_dir, d))\n",
        "               and d not in [\"combined_results\", \"visualizations\", \"interactive_visualizations\", \"log_analysis\"]]\n",
        "\n",
        "    if not subdirs:\n",
        "        print(\"No analysis subdirectories found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(subdirs)} analysis directories.\")\n",
        "\n",
        "    # If no specific file is provided, allow the user to select one\n",
        "    if file_to_display is None:\n",
        "        print(\"\\nAvailable files:\")\n",
        "        for i, subdir in enumerate(subdirs):\n",
        "            print(f\"{i+1}. {subdir}\")\n",
        "\n",
        "        try:\n",
        "            selection = int(input(\"\\nEnter the number of the file to display (0 to exit): \"))\n",
        "            if selection == 0:\n",
        "                return\n",
        "            file_to_display = subdirs[selection-1]\n",
        "        except (ValueError, IndexError):\n",
        "            print(\"Invalid selection. Exiting.\")\n",
        "            return\n",
        "\n",
        "    subdir_path = os.path.join(results_dir, file_to_display)\n",
        "    print(f\"\\nDisplaying results for {file_to_display}...\")\n",
        "\n",
        "    # Load correlation graph if available\n",
        "    graphml_path = os.path.join(subdir_path, \"correlation_graph.graphml\")\n",
        "    if os.path.exists(graphml_path):\n",
        "        try:\n",
        "            G = nx.read_graphml(graphml_path)\n",
        "            print(f\"Loaded correlation graph: {len(G.nodes)} nodes, {len(G.edges)} edges\")\n",
        "\n",
        "            # Visualize correlation graph\n",
        "            plt.figure(figsize=(15, 12))\n",
        "\n",
        "            # Create position layout\n",
        "            pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "            # Get node colors based on signal type\n",
        "            type_colors = {\n",
        "                'uint': 'blue',\n",
        "                'enum': 'green',\n",
        "                'rand': 'red',\n",
        "                'cyclic': 'purple',\n",
        "                'unknown': 'gray'\n",
        "            }\n",
        "\n",
        "            # Extract node types\n",
        "            node_colors = []\n",
        "            for node in G.nodes():\n",
        "                node_type = G.nodes[node].get('signal_type', 'unknown')\n",
        "                node_colors.append(type_colors.get(node_type, 'gray'))\n",
        "\n",
        "            # Get edge weights\n",
        "            edge_weights = []\n",
        "            for u, v in G.edges():\n",
        "                weight = float(G.edges[u, v].get('weight', 1.0))\n",
        "                edge_weights.append(weight * 2)\n",
        "\n",
        "            # Plot the graph\n",
        "            nx.draw_networkx_nodes(G, pos, node_color=node_colors, alpha=0.8, node_size=100)\n",
        "            nx.draw_networkx_edges(G, pos, width=edge_weights, alpha=0.5, edge_color='gray')\n",
        "\n",
        "            # Create a custom legend\n",
        "            for signal_type, color in type_colors.items():\n",
        "                plt.plot([], [], 'o', color=color, label=signal_type)\n",
        "\n",
        "            plt.legend(loc='upper right')\n",
        "            plt.title(f'Signal Correlation Graph - {file_to_display}')\n",
        "            plt.axis('off')\n",
        "            plt.tight_layout()\n",
        "            plt.show()  # This will display the plot instead of saving it\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing correlation graph: {e}\")\n",
        "    else:\n",
        "        print(\"No correlation graph found.\")\n",
        "\n",
        "    # Load signals pickle and display signal values\n",
        "    pkl_path = os.path.join(subdir_path, \"signals.pkl\")\n",
        "    if os.path.exists(pkl_path):\n",
        "        try:\n",
        "            with open(pkl_path, 'rb') as f:\n",
        "                signals_by_id = pickle.load(f)\n",
        "            print(f\"Loaded signals pickle: {len(signals_by_id)} CAN IDs\")\n",
        "\n",
        "            # Let the user choose a CAN ID to display\n",
        "            can_ids = list(signals_by_id.keys())\n",
        "            print(\"\\nAvailable CAN IDs:\")\n",
        "            for i, can_id in enumerate(can_ids[:min(20, len(can_ids))]):  # Show first 20 IDs\n",
        "                signal_count = len(signals_by_id[can_id])\n",
        "                print(f\"{i+1}. {can_id} ({signal_count} signals)\")\n",
        "\n",
        "            if len(can_ids) > 20:\n",
        "                print(f\"...and {len(can_ids) - 20} more\")\n",
        "\n",
        "            try:\n",
        "                selection = int(input(\"\\nEnter the number of the CAN ID to display (0 to skip): \"))\n",
        "                if selection > 0:\n",
        "                    selected_can_id = can_ids[selection-1]\n",
        "                    signals = signals_by_id[selected_can_id]\n",
        "\n",
        "                    if signals:\n",
        "                        plt.figure(figsize=(15, 8))\n",
        "                        for i, signal in enumerate(signals[:10]):  # Plot up to 10 signals\n",
        "                            if signal.values:\n",
        "                                plt.plot(signal.values[:200],\n",
        "                                        label=f\"Signal {i} [{signal.start_bit}:{signal.start_bit+signal.num_bits-1}] - {signal.signal_type}\")\n",
        "\n",
        "                        plt.title(f'Signal Values - CAN ID: {selected_can_id}')\n",
        "                        plt.xlabel('Frame Index')\n",
        "                        plt.ylabel('Signal Value')\n",
        "                        plt.legend()\n",
        "                        plt.grid(True, alpha=0.3)\n",
        "                        plt.tight_layout()\n",
        "                        plt.show()  # This will display the plot instead of saving it\n",
        "\n",
        "                        # Display HAMD (Hamming distance) visualization for selected signals\n",
        "                        plt.figure(figsize=(12, 6))\n",
        "                        for i, signal in enumerate(signals[:5]):  # Plot up to 5 signals\n",
        "                            if signal.hamd:\n",
        "                                plt.plot(signal.hamd,\n",
        "                                        label=f\"Signal {i} [{signal.start_bit}:{signal.start_bit+signal.num_bits-1}] - {signal.signal_type}\")\n",
        "\n",
        "                        plt.title(f'HAMD Distribution - CAN ID: {selected_can_id}')\n",
        "                        plt.xlabel('Bit Position')\n",
        "                        plt.ylabel('Flip Rate')\n",
        "                        plt.legend()\n",
        "                        plt.grid(True, alpha=0.3)\n",
        "                        plt.tight_layout()\n",
        "                        plt.show()  # This will display the plot instead of saving it\n",
        "            except (ValueError, IndexError):\n",
        "                print(\"Invalid selection. Skipping signal display.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing signals pickle: {e}\")\n",
        "    else:\n",
        "        print(\"No signals pickle found.\")\n",
        "\n",
        "    # Load signal summary and display distributions\n",
        "    csv_path = os.path.join(subdir_path, \"signal_summary.csv\")\n",
        "    if os.path.exists(csv_path):\n",
        "        try:\n",
        "            signal_df = pd.read_csv(csv_path)\n",
        "            print(f\"Loaded signal summary: {len(signal_df)} signals\")\n",
        "\n",
        "            # Create signal type distribution pie chart\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            signal_df['signal_type'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
        "            plt.title(f'Signal Type Distribution - {file_to_display}')\n",
        "            plt.ylabel('')  # Hide \"None\" label\n",
        "            plt.tight_layout()\n",
        "            plt.show()  # This will display the plot instead of saving it\n",
        "\n",
        "            # Create signal bit width histogram\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            signal_df['num_bits'].hist(bins=20)\n",
        "            plt.title(f'Signal Bit Width Distribution - {file_to_display}')\n",
        "            plt.xlabel('Number of Bits')\n",
        "            plt.ylabel('Count')\n",
        "            plt.grid(axis='y', alpha=0.75)\n",
        "            plt.tight_layout()\n",
        "            plt.show()  # This will display the plot instead of saving it\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing signal summary: {e}\")\n",
        "    else:\n",
        "        print(\"No signal summary found.\")\n",
        "\n",
        "    # Ask if the user wants to view another file\n",
        "    if input(\"\\nView another file? (y/n): \").lower().startswith('y'):\n",
        "        explore_autocan_results_interactive(results_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the interactive exploration\n",
        "    explore_autocan_results_interactive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eDr-8ziuPjoD",
        "outputId": "2dfde83c-0d4b-4dc8-9d62-6ceb053ecb1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 26 log files: Fuzzing_valid_IDs.log, Steering_angle_attack.log, FCA_warning_attack.log, Steering_angle_replay.log, Gear_shifter_attack_1.log, Door_open_warning_attack.log, DoS_attack.log, Min_speedometer_attack_2.log, Fuzzing_valid_IDs_DoS.log, Max_speedometer_attack.log, Break_and_fog_light_attack.log, EMS_attack.log, Min_speedometer_attack_3.log, Drive_mode_changing_attack.log, Parking_break_attack.log, EMS_replay_attack.log, Gear_shifter_attack_2.log, Fuzzing_random_IDs.log, Wiper_warning_attack.log, Multiple_attacks_1.log, Reverse_speedometer_fuzzing_attack.log, Multiple_attacks_2.log, Min_speedometer_attack_1.log, Break_warning_attack.log, EMS_replay_long_attack.log, Power_steering_attack.log\n",
            "Using 2 processors for parallel analysis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAnalyzing files:   0%|          | 0/26 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Fuzzing_valid_IDs.log...\n",
            "Analyzing Steering_angle_attack.log...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [00:45<00:00,  1.23it/s]\n",
            "Identifying signals: 100%|██████████| 56/56 [01:03<00:00,  1.14s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Fuzzing_valid_IDs\n",
            "Completed analysis of Fuzzing_valid_IDs.log\n",
            "\n",
            "Analyzing FCA_warning_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:   9%|▉         | 5/56 [00:10<02:03,  2.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Steering_angle_attack\n",
            "Completed analysis of Steering_angle_attack.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  14%|█▍        | 8/56 [00:15<01:18,  1.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Steering_angle_replay.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [02:05<00:00,  2.24s/it]\n",
            "Identifying signals: 100%|██████████| 56/56 [01:41<00:00,  1.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Steering_angle_replay\n",
            "Completed analysis of Steering_angle_replay.log\n",
            "Results saved to autocan_results/FCA_warning_attack\n",
            "Completed analysis of FCA_warning_attack.log\n",
            "\n",
            "Analyzing Gear_shifter_attack_1.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAnalyzing files:  12%|█▏        | 3/26 [05:55<50:19, 131.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Door_open_warning_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  17%|█▋        | 9/54 [00:18<01:37,  2.16s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals: 100%|██████████| 54/54 [01:34<00:00,  1.75s/it]\n",
            "Identifying signals: 100%|██████████| 54/54 [01:30<00:00,  1.68s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Gear_shifter_attack_1\n",
            "Completed analysis of Gear_shifter_attack_1.log\n",
            "Results saved to autocan_results/Door_open_warning_attack\n",
            "Completed analysis of Door_open_warning_attack.log\n",
            "\n",
            "Analyzing DoS_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAnalyzing files:  19%|█▉        | 5/26 [09:08<41:46, 119.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Min_speedometer_attack_2.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  13%|█▎        | 7/55 [00:09<01:08,  1.42s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals: 100%|██████████| 55/55 [01:01<00:00,  1.11s/it]\n",
            "Identifying signals:  94%|█████████▍| 51/54 [01:34<00:09,  3.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/DoS_attack\n",
            "Completed analysis of DoS_attack.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rIdentifying signals:  96%|█████████▋| 52/54 [01:39<00:07,  3.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Fuzzing_valid_IDs_DoS.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 54/54 [01:40<00:00,  1.87s/it]\n",
            "Identifying signals:  11%|█         | 6/55 [00:11<02:00,  2.46s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  24%|██▎       | 13/55 [00:17<00:40,  1.04it/s]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  42%|████▏     | 23/55 [00:33<00:28,  1.12it/s]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  51%|█████     | 28/55 [00:41<00:28,  1.04s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  55%|█████▍    | 30/55 [00:47<00:54,  2.16s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  78%|███████▊  | 43/55 [01:01<00:09,  1.27it/s]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  84%|████████▎ | 46/55 [01:05<00:10,  1.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Min_speedometer_attack_2\n",
            "Completed analysis of Min_speedometer_attack_2.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  87%|████████▋ | 48/55 [01:10<00:11,  1.65s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  89%|████████▉ | 49/55 [01:15<00:15,  2.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Max_speedometer_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:   0%|          | 0/56 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals: 100%|██████████| 55/55 [01:26<00:00,  1.58s/it]\n",
            "Identifying signals:  95%|█████████▍| 53/56 [01:16<00:05,  1.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Fuzzing_valid_IDs_DoS\n",
            "Completed analysis of Fuzzing_valid_IDs_DoS.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [01:18<00:00,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Break_and_fog_light_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  36%|███▌      | 20/56 [00:36<01:48,  3.02s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals:  54%|█████▎    | 30/56 [00:48<00:48,  1.85s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Max_speedometer_attack\n",
            "Completed analysis of Max_speedometer_attack.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:  61%|██████    | 34/56 [00:55<00:29,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing EMS_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 56/56 [01:33<00:00,  1.67s/it]\n",
            "Identifying signals:  98%|█████████▊| 55/56 [00:59<00:01,  1.42s/it]/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2922: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/lib/_function_base_impl.py:2923: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n",
            "Identifying signals: 100%|██████████| 56/56 [00:59<00:00,  1.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Break_and_fog_light_attack\n",
            "Completed analysis of Break_and_fog_light_attack.log\n",
            "\n",
            "Analyzing Min_speedometer_attack_3.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:   0%|          | 0/54 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/EMS_attack\n",
            "Completed analysis of EMS_attack.log\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals:   4%|▎         | 2/54 [00:00<00:11,  4.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Drive_mode_changing_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Identifying signals: 100%|██████████| 54/54 [00:53<00:00,  1.01it/s]\n",
            "Identifying signals: 100%|██████████| 56/56 [00:42<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to autocan_results/Drive_mode_changing_attack\n",
            "Completed analysis of Drive_mode_changing_attack.log\n",
            "\n",
            "Analyzing Parking_break_attack.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rAnalyzing files:  46%|████▌     | 12/26 [18:49<21:58, 94.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing EMS_replay_attack.log...\n",
            "\n",
            "Analyzing Gear_shifter_attack_2.log...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Identifying signals:  43%|████▎     | 23/54 [00:30<00:40,  1.32s/it]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Fuzzing_random_IDs.log...\n",
            "\n",
            "Analyzing Wiper_warning_attack.log...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7434a0fd1670>\u001b[0m in \u001b[0;36m_analyze_parallel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Analyzing files\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m                 \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7434a0fd1670>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-7434a0fd1670>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;31m# Analyze all files (parallel by default)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1590\u001b[0;31m     \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[0;31m# Generate visualizations for combined results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7434a0fd1670>\u001b[0m in \u001b[0;36manalyze_files\u001b[0;34m(self, parallel)\u001b[0m\n\u001b[1;32m    920\u001b[0m         \"\"\"\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparallel\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze_sequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7434a0fd1670>\u001b[0m in \u001b[0;36m_analyze_parallel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# Container for results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_processors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m             \u001b[0;31m# Map file paths to future objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m             futures = {executor.submit(self._analyze_single_file, file_path): file_path\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/process.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor_manager_thread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m         \u001b[0;31m# To reduce the risk of opening too many files, remove references to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;31m# objects that use file descriptors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.signal import correlate\n",
        "import networkx as nx\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "from typing import List, Dict, Tuple, Any, Optional, Set, Union\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import glob\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import multiprocessing\n",
        "\n",
        "# Keep all the existing classes (CANFrame, CANTrace, CANSignal, AutoCANAnalyzer)\n",
        "# as defined in the original code\n",
        "\n",
        "class CANFrame:\n",
        "    \"\"\"Represents a single CAN frame with ID and payload.\"\"\"\n",
        "\n",
        "    def __init__(self, can_id: str, payload: str, timestamp: float = 0.0):\n",
        "        \"\"\"\n",
        "        Initialize a CAN frame.\n",
        "\n",
        "        Args:\n",
        "            can_id: The CAN ID in hexadecimal format (e.g. '0x123')\n",
        "            payload: The payload in hexadecimal format (e.g. '00FF34A2')\n",
        "            timestamp: The timestamp when the frame was captured\n",
        "        \"\"\"\n",
        "        self.can_id = can_id\n",
        "        self.payload = payload\n",
        "        self.timestamp = timestamp\n",
        "\n",
        "        # Convert payload to binary representation\n",
        "        try:\n",
        "            self.binary_payload = bin(int(payload, 16))[2:].zfill(len(payload) * 4)\n",
        "        except ValueError:\n",
        "            self.binary_payload = '0' * 64  # Default to 64 bits for invalid payloads\n",
        "\n",
        "    def get_bit(self, bit_position: int) -> int:\n",
        "        \"\"\"Get the value of a specific bit in the payload.\"\"\"\n",
        "        if bit_position < len(self.binary_payload):\n",
        "            return int(self.binary_payload[bit_position])\n",
        "        return 0\n",
        "\n",
        "    def get_byte(self, byte_position: int) -> int:\n",
        "        \"\"\"Get the value of a specific byte in the payload.\"\"\"\n",
        "        start_bit = byte_position * 8\n",
        "        end_bit = start_bit + 8\n",
        "        if start_bit < len(self.binary_payload):\n",
        "            byte_str = self.binary_payload[start_bit:min(end_bit, len(self.binary_payload))]\n",
        "            byte_str = byte_str.ljust(8, '0')\n",
        "            return int(byte_str, 2)\n",
        "        return 0\n",
        "\n",
        "    def get_bits(self, start_bit: int, num_bits: int) -> int:\n",
        "        \"\"\"Get a range of bits as a single integer value.\"\"\"\n",
        "        end_bit = start_bit + num_bits\n",
        "        if start_bit < len(self.binary_payload):\n",
        "            bits_str = self.binary_payload[start_bit:min(end_bit, len(self.binary_payload))]\n",
        "            bits_str = bits_str.ljust(num_bits, '0')\n",
        "            return int(bits_str, 2)\n",
        "        return 0\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"CANFrame(id={self.can_id}, payload={self.payload}, ts={self.timestamp})\"\n",
        "\n",
        "\n",
        "class CANTrace:\n",
        "    \"\"\"Collection of CAN frames representing a vehicle trace.\"\"\"\n",
        "\n",
        "    def __init__(self, frames: Optional[List[CANFrame]] = None):\n",
        "        \"\"\"\n",
        "        Initialize a CAN trace with optional frames.\n",
        "\n",
        "        Args:\n",
        "            frames: List of CANFrame objects\n",
        "        \"\"\"\n",
        "        self.frames = frames or []\n",
        "        self.frames_by_id = defaultdict(list)\n",
        "        self._update_frame_index()\n",
        "\n",
        "    def _update_frame_index(self):\n",
        "        \"\"\"Update the index of frames by CAN ID.\"\"\"\n",
        "        self.frames_by_id = defaultdict(list)\n",
        "        for frame in self.frames:\n",
        "            self.frames_by_id[frame.can_id].append(frame)\n",
        "\n",
        "    def add_frame(self, frame: CANFrame):\n",
        "        \"\"\"Add a single frame to the trace.\"\"\"\n",
        "        self.frames.append(frame)\n",
        "        self.frames_by_id[frame.can_id].append(frame)\n",
        "\n",
        "    def add_frames(self, frames: List[CANFrame]):\n",
        "        \"\"\"Add multiple frames to the trace.\"\"\"\n",
        "        self.frames.extend(frames)\n",
        "        self._update_frame_index()\n",
        "\n",
        "    def get_frames_by_id(self, can_id: str) -> List[CANFrame]:\n",
        "        \"\"\"Get all frames with a specific CAN ID.\"\"\"\n",
        "        return self.frames_by_id.get(can_id, [])\n",
        "\n",
        "    def get_unique_ids(self) -> Set[str]:\n",
        "        \"\"\"Get the set of unique CAN IDs in the trace.\"\"\"\n",
        "        return set(self.frames_by_id.keys())\n",
        "\n",
        "    @classmethod\n",
        "    def from_log(cls, filepath):\n",
        "        trace = cls()\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue  # skip empty lines\n",
        "\n",
        "                if line.startswith('('):\n",
        "                    end_idx = line.find(')')\n",
        "                    ts_str = line[1:end_idx]\n",
        "                    timestamp = float(ts_str)\n",
        "                else:\n",
        "                    continue  # skip invalid format lines\n",
        "\n",
        "                remaining = line[end_idx+1:].strip()\n",
        "                parts = remaining.split()\n",
        "                if len(parts) < 2:\n",
        "                    continue  # malformed line\n",
        "\n",
        "                id_payload = parts[1] if '#' in parts[1] else parts[0]\n",
        "                if '#' not in id_payload:\n",
        "                    continue  # invalid payload\n",
        "\n",
        "                id_str, data_str = id_payload.split('#', 1)\n",
        "                try:\n",
        "                    can_id = int(id_str, 16)\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "                data_bytes = [int(data_str[i:i+2], 16) for i in range(0, len(data_str), 2)]\n",
        "\n",
        "                # Create CANFrame with matching signature\n",
        "                frame = CANFrame(can_id=f\"{can_id:X}\", payload=''.join(f\"{byte:02X}\" for byte in data_bytes), timestamp=timestamp)\n",
        "                trace.frames.append(frame)\n",
        "\n",
        "        trace._update_frame_index()\n",
        "        return trace\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.frames)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"CANTrace(frames={len(self.frames)}, unique_ids={len(self.get_unique_ids())})\"\n",
        "\n",
        "\n",
        "class CANSignal:\n",
        "    \"\"\"Represents a decoded signal within CAN frames.\"\"\"\n",
        "\n",
        "    SIGNAL_TYPES = ['uint', 'enum', 'rand', 'cyclic']\n",
        "\n",
        "    def __init__(self, can_id: str, start_bit: int, num_bits: int, signal_type: str = 'unknown'):\n",
        "        \"\"\"\n",
        "        Initialize a CAN signal.\n",
        "\n",
        "        Args:\n",
        "            can_id: The CAN ID this signal belongs to\n",
        "            start_bit: Starting bit position (0-indexed)\n",
        "            num_bits: Number of bits in the signal\n",
        "            signal_type: Type of signal ('uint', 'enum', 'rand', 'cyclic', or 'unknown')\n",
        "        \"\"\"\n",
        "        self.can_id = can_id\n",
        "        self.start_bit = start_bit\n",
        "        self.num_bits = num_bits\n",
        "        self.signal_type = signal_type if signal_type in self.SIGNAL_TYPES else 'unknown'\n",
        "        self.label = None\n",
        "        self.values = []  # Will hold the signal values extracted from frames\n",
        "        self.timestamps = []  # Timestamps corresponding to values\n",
        "\n",
        "        # Statistics calculated later\n",
        "        self.autocorrelation = None\n",
        "        self.hamd = None\n",
        "        self.min_value = None\n",
        "        self.max_value = None\n",
        "        self.avg_value = None\n",
        "        self.std_value = None\n",
        "        self.distinct_values = None\n",
        "        self.flip_rate = None\n",
        "\n",
        "    def extract_values(self, frames: List[CANFrame]):\n",
        "        \"\"\"Extract signal values from a list of CAN frames.\"\"\"\n",
        "        self.values = []\n",
        "        self.timestamps = []\n",
        "\n",
        "        for frame in frames:\n",
        "            value = frame.get_bits(self.start_bit, self.num_bits)\n",
        "            self.values.append(value)\n",
        "            self.timestamps.append(frame.timestamp)\n",
        "\n",
        "        # Calculate basic statistics\n",
        "        if self.values:\n",
        "            self.min_value = min(self.values)\n",
        "            self.max_value = max(self.values)\n",
        "            self.avg_value = sum(self.values) / len(self.values)\n",
        "            self.std_value = np.std(self.values) if len(self.values) > 1 else 0\n",
        "            self.distinct_values = len(set(self.values))\n",
        "\n",
        "            # Calculate flip rate\n",
        "            if len(self.values) > 1:\n",
        "                flips = sum(1 for i in range(len(self.values)-1) if self.values[i] != self.values[i+1])\n",
        "                self.flip_rate = flips / (len(self.values) - 1)\n",
        "            else:\n",
        "                self.flip_rate = 0\n",
        "\n",
        "    def calculate_autocorrelation(self, max_lag: int = 100):\n",
        "        \"\"\"Calculate the autocorrelation function for the signal values.\"\"\"\n",
        "        if len(self.values) > max_lag:\n",
        "            # Normalize the values\n",
        "            normalized = np.array(self.values) - np.mean(self.values)\n",
        "            if np.std(normalized) > 0:\n",
        "                normalized = normalized / np.std(normalized)\n",
        "\n",
        "                # Calculate autocorrelation\n",
        "                self.autocorrelation = [1.0]  # Lag 0 is always 1.0\n",
        "                for lag in range(1, max_lag):\n",
        "                    corr = np.corrcoef(normalized[:-lag], normalized[lag:])[0, 1]\n",
        "                    self.autocorrelation.append(corr)\n",
        "            else:\n",
        "                self.autocorrelation = [0.0] * max_lag\n",
        "        else:\n",
        "            self.autocorrelation = [0.0] * max_lag\n",
        "\n",
        "    def calculate_hamd(self):\n",
        "        \"\"\"Calculate the Hamming distance distribution (HAMD) for the signal.\"\"\"\n",
        "        if len(self.values) <= 1:\n",
        "            self.hamd = [0.0] * self.num_bits\n",
        "            return\n",
        "\n",
        "        # Convert values to binary sequences\n",
        "        binary_values = [format(v, f'0{self.num_bits}b') for v in self.values]\n",
        "\n",
        "        # Calculate HAMD for each bit position\n",
        "        self.hamd = []\n",
        "        for bit_pos in range(self.num_bits):\n",
        "            # Extract the bit at this position for all values\n",
        "            bits = [int(binary_values[j][bit_pos]) for j in range(len(binary_values))]\n",
        "\n",
        "            # Count flips\n",
        "            flips = sum(1 for j in range(len(bits)-1) if bits[j] != bits[j+1])\n",
        "            flip_rate = flips / (len(bits) - 1) if len(bits) > 1 else 0\n",
        "            self.hamd.append(flip_rate)\n",
        "\n",
        "    def is_likely_uint(self) -> bool:\n",
        "        \"\"\"Check if the signal is likely an unsigned integer.\"\"\"\n",
        "        if not self.hamd:\n",
        "            return False\n",
        "\n",
        "        # Check for increasing HAMD pattern typical of little-endian integers\n",
        "        increasing = all(self.hamd[i] <= self.hamd[i+1] for i in range(len(self.hamd)-1))\n",
        "        return increasing and self.flip_rate > 0.01\n",
        "\n",
        "    def is_likely_enum(self) -> bool:\n",
        "        \"\"\"Check if the signal is likely an enumeration.\"\"\"\n",
        "        if not self.hamd:\n",
        "            return False\n",
        "\n",
        "        # Enums typically have a low number of distinct values relative to possible values\n",
        "        max_possible = 2 ** self.num_bits\n",
        "        enum_threshold = min(32, max_possible * 0.1)  # Heuristic threshold\n",
        "\n",
        "        return (self.distinct_values <= enum_threshold and\n",
        "                self.distinct_values > 1 and\n",
        "                self.flip_rate < 0.3)\n",
        "\n",
        "    def is_likely_random(self) -> bool:\n",
        "        \"\"\"Check if the signal is likely random (like a checksum).\"\"\"\n",
        "        if not self.hamd or not self.autocorrelation:\n",
        "            return False\n",
        "\n",
        "        # Random signals have high entropy and low autocorrelation\n",
        "        high_entropy = self.distinct_values > min(100, 2 ** self.num_bits * 0.5)\n",
        "        low_autocorr = all(abs(ac) < 0.3 for ac in self.autocorrelation[1:10] if ac is not None)\n",
        "\n",
        "        return high_entropy and low_autocorr and self.flip_rate > 0.3\n",
        "\n",
        "    def is_likely_cyclic(self) -> bool:\n",
        "        \"\"\"Check if the signal is likely cyclic (counter, clock).\"\"\"\n",
        "        if not self.autocorrelation or len(self.autocorrelation) < 10:\n",
        "            return False\n",
        "\n",
        "        # Cyclic signals show oscillatory autocorrelation\n",
        "        oscillatory = False\n",
        "        if len(self.autocorrelation) > 2:\n",
        "            # Check for sign changes in autocorrelation - typical of oscillatory signals\n",
        "            sign_changes = sum(1 for i in range(1, len(self.autocorrelation)-1)\n",
        "                              if (self.autocorrelation[i] * self.autocorrelation[i+1]) < 0)\n",
        "            oscillatory = sign_changes >= 3\n",
        "\n",
        "        # Also check for steadily increasing values with wrap-around\n",
        "        if len(self.values) > 10:\n",
        "            increasing_count = sum(1 for i in range(len(self.values)-1)\n",
        "                                  if self.values[i+1] == (self.values[i] + 1) % (2**self.num_bits))\n",
        "            perfect_counter = increasing_count > len(self.values) * 0.8\n",
        "            return oscillatory or perfect_counter\n",
        "\n",
        "        return oscillatory\n",
        "\n",
        "    def determine_signal_type(self):\n",
        "        \"\"\"Determine the most likely signal type based on signal characteristics.\"\"\"\n",
        "        # Make sure we have calculated all necessary characteristics\n",
        "        if self.hamd is None:\n",
        "            self.calculate_hamd()\n",
        "        if self.autocorrelation is None:\n",
        "            self.calculate_autocorrelation()\n",
        "\n",
        "        # Check conditions for each type\n",
        "        if self.is_likely_cyclic():\n",
        "            self.signal_type = 'cyclic'\n",
        "        elif self.is_likely_random():\n",
        "            self.signal_type = 'rand'\n",
        "        elif self.is_likely_enum():\n",
        "            self.signal_type = 'enum'\n",
        "        elif self.is_likely_uint():\n",
        "            self.signal_type = 'uint'\n",
        "        else:\n",
        "            self.signal_type = 'unknown'\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        label_str = f\", label={self.label}\" if self.label else \"\"\n",
        "        return (f\"CANSignal(id={self.can_id}, bits=[{self.start_bit}:{self.start_bit+self.num_bits}], \"\n",
        "                f\"type={self.signal_type}{label_str})\")\n",
        "\n",
        "\n",
        "class AutoCANAnalyzer:\n",
        "    \"\"\"\n",
        "    Implements the AutoCAN methodology for automatic reverse engineering of CAN signals.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, can_trace: CANTrace):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer with a CAN trace.\n",
        "\n",
        "        Args:\n",
        "            can_trace: A CANTrace object containing the frames to analyze\n",
        "        \"\"\"\n",
        "        self.can_trace = can_trace\n",
        "        self.signals_by_id = {}  # Dict of signals by CAN ID\n",
        "        self.correlation_graph = None\n",
        "\n",
        "    def identify_signals(self, progress_bar=True):\n",
        "        \"\"\"\n",
        "        Identify signals for all CAN IDs in the trace.\n",
        "\n",
        "        Args:\n",
        "            progress_bar: Whether to show a progress bar\n",
        "        \"\"\"\n",
        "        unique_ids = self.can_trace.get_unique_ids()\n",
        "\n",
        "        if progress_bar:\n",
        "            id_iterator = tqdm(unique_ids, desc=\"Identifying signals\")\n",
        "        else:\n",
        "            id_iterator = unique_ids\n",
        "\n",
        "        for can_id in id_iterator:\n",
        "            frames = self.can_trace.get_frames_by_id(can_id)\n",
        "            if len(frames) > 10:  # Skip IDs with too few frames\n",
        "                self.signals_by_id[can_id] = self.identify_signals_for_id(can_id, frames)\n",
        "\n",
        "    def identify_signals_for_id(self, can_id: str, frames: List[CANFrame]) -> List[CANSignal]:\n",
        "        \"\"\"\n",
        "        Identify signals for a specific CAN ID.\n",
        "\n",
        "        Args:\n",
        "            can_id: The CAN ID to analyze\n",
        "            frames: List of frames with this CAN ID\n",
        "\n",
        "        Returns:\n",
        "            List of identified CANSignal objects\n",
        "        \"\"\"\n",
        "        # Step 1: Extract byte-level features\n",
        "        byte_features = self._extract_byte_features(frames)\n",
        "\n",
        "        # Step 2: Cluster bytes using DBSCAN\n",
        "        byte_clusters = self._cluster_bytes(byte_features)\n",
        "\n",
        "        # Step 3: Process each byte cluster to identify bit-level signals\n",
        "        signals = []\n",
        "        for cluster_id, byte_indices in byte_clusters.items():\n",
        "            # Skip noise cluster (cluster_id = -1)\n",
        "            if cluster_id == -1:\n",
        "                continue\n",
        "\n",
        "            # Extract bit-level features for this cluster\n",
        "            start_bit = min(byte_indices) * 8\n",
        "            end_bit = (max(byte_indices) + 1) * 8 - 1\n",
        "            cluster_signals = self._identify_bit_level_signals(frames, start_bit, end_bit)\n",
        "            signals.extend(cluster_signals)\n",
        "\n",
        "        # Extract values and determine signal types\n",
        "        for signal in signals:\n",
        "            signal.extract_values(frames)\n",
        "            signal.calculate_autocorrelation()\n",
        "            signal.calculate_hamd()\n",
        "            signal.determine_signal_type()\n",
        "\n",
        "        return signals\n",
        "\n",
        "    def _extract_byte_features(self, frames: List[CANFrame]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Extract byte-level features from a list of frames.\n",
        "\n",
        "        Args:\n",
        "            frames: List of CAN frames\n",
        "\n",
        "        Returns:\n",
        "            2D array of features (rows=bytes, columns=features)\n",
        "        \"\"\"\n",
        "        max_bytes = 8  # Standard CAN frame has 8 bytes max\n",
        "\n",
        "        # Create arrays for feature extraction\n",
        "        byte_values = np.zeros((max_bytes, len(frames)), dtype=int)\n",
        "\n",
        "        # Extract byte values\n",
        "        for byte_idx in range(max_bytes):\n",
        "            for frame_idx, frame in enumerate(frames):\n",
        "                byte_values[byte_idx, frame_idx] = frame.get_byte(byte_idx)\n",
        "\n",
        "        # Calculate features for each byte\n",
        "        features = []\n",
        "        for byte_idx in range(max_bytes):\n",
        "            byte_data = byte_values[byte_idx, :]\n",
        "\n",
        "            # Calculate flip rate\n",
        "            flips = sum(1 for i in range(len(byte_data)-1) if byte_data[i] != byte_data[i+1])\n",
        "            flip_rate = flips / (len(byte_data) - 1) if len(byte_data) > 1 else 0\n",
        "\n",
        "            # Calculate average value\n",
        "            avg_value = np.mean(byte_data)\n",
        "\n",
        "            # Calculate distinct value ratio\n",
        "            distinct_values = len(np.unique(byte_data))\n",
        "            distinct_ratio = distinct_values / 256  # 256 possible values for a byte\n",
        "\n",
        "            # Entropy\n",
        "            values, counts = np.unique(byte_data, return_counts=True)\n",
        "            probs = counts / len(byte_data)\n",
        "            entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
        "\n",
        "            features.append([flip_rate, avg_value / 255, distinct_ratio, entropy / 8])\n",
        "\n",
        "        return np.array(features)\n",
        "\n",
        "    def _cluster_bytes(self, byte_features: np.ndarray, eps=0.4, min_samples=2) -> Dict[int, List[int]]:\n",
        "        \"\"\"\n",
        "        Cluster bytes based on their features using DBSCAN.\n",
        "\n",
        "        Args:\n",
        "            byte_features: 2D array of byte features\n",
        "            eps: DBSCAN epsilon parameter\n",
        "            min_samples: DBSCAN min_samples parameter\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping cluster IDs to lists of byte indices\n",
        "        \"\"\"\n",
        "        # Apply DBSCAN to group related bytes\n",
        "        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(byte_features)\n",
        "        labels = clustering.labels_\n",
        "\n",
        "        # Group bytes by cluster\n",
        "        clusters = defaultdict(list)\n",
        "        for i, label in enumerate(labels):\n",
        "            clusters[label].append(i)\n",
        "\n",
        "        # Refine clusters to ensure they contain consecutive bytes\n",
        "        refined_clusters = {}\n",
        "        next_cluster_id = max(clusters.keys()) + 1 if clusters else 0\n",
        "\n",
        "        for cluster_id, byte_indices in clusters.items():\n",
        "            # Skip noise cluster as we'll process them individually\n",
        "            if cluster_id == -1:\n",
        "                for idx in byte_indices:\n",
        "                    refined_clusters[next_cluster_id] = [idx]\n",
        "                    next_cluster_id += 1\n",
        "                continue\n",
        "\n",
        "            # Sort byte indices\n",
        "            byte_indices.sort()\n",
        "\n",
        "            # Find consecutive chunks\n",
        "            chunks = []\n",
        "            current_chunk = [byte_indices[0]]\n",
        "\n",
        "            for i in range(1, len(byte_indices)):\n",
        "                if byte_indices[i] == byte_indices[i-1] + 1:\n",
        "                    current_chunk.append(byte_indices[i])\n",
        "                else:\n",
        "                    chunks.append(current_chunk)\n",
        "                    current_chunk = [byte_indices[i]]\n",
        "\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "\n",
        "            # Create a new cluster for each chunk\n",
        "            for chunk in chunks:\n",
        "                refined_clusters[next_cluster_id] = chunk\n",
        "                next_cluster_id += 1\n",
        "\n",
        "        return refined_clusters\n",
        "\n",
        "    def _identify_bit_level_signals(self, frames: List[CANFrame], start_bit: int, end_bit: int) -> List[CANSignal]:\n",
        "        \"\"\"\n",
        "        Identify bit-level signals within a byte cluster.\n",
        "\n",
        "        Args:\n",
        "            frames: List of CAN frames\n",
        "            start_bit: Starting bit position\n",
        "            end_bit: Ending bit position\n",
        "\n",
        "        Returns:\n",
        "            List of identified CANSignal objects\n",
        "        \"\"\"\n",
        "        can_id = frames[0].can_id\n",
        "        num_bits = end_bit - start_bit + 1\n",
        "\n",
        "        # For small bit ranges (8 bits or less), try to determine if this is a single signal\n",
        "        if num_bits <= 8:\n",
        "            signal = CANSignal(can_id, start_bit, num_bits)\n",
        "            return [signal]\n",
        "\n",
        "        # Extract bit flip rates\n",
        "        bit_flip_rates = self._calculate_bit_flip_rates(frames, start_bit, end_bit)\n",
        "\n",
        "        # Use DBSCAN to cluster bits based on flip rates\n",
        "        bit_clusters = self._cluster_bits(bit_flip_rates)\n",
        "\n",
        "        # Create signals from bit clusters\n",
        "        signals = []\n",
        "        for cluster_bits in bit_clusters:\n",
        "            # Sort bits to ensure consecutive ordering\n",
        "            cluster_bits.sort()\n",
        "\n",
        "            # Find consecutive chunks of bits\n",
        "            chunks = []\n",
        "            current_chunk = [cluster_bits[0]]\n",
        "\n",
        "            for i in range(1, len(cluster_bits)):\n",
        "                if cluster_bits[i] == cluster_bits[i-1] + 1:\n",
        "                    current_chunk.append(cluster_bits[i])\n",
        "                else:\n",
        "                    chunks.append(current_chunk)\n",
        "                    current_chunk = [cluster_bits[i]]\n",
        "\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "\n",
        "            # Create a signal for each chunk\n",
        "            for chunk in chunks:\n",
        "                chunk_start = start_bit + chunk[0]\n",
        "                chunk_length = len(chunk)\n",
        "                signal = CANSignal(can_id, chunk_start, chunk_length)\n",
        "                signals.append(signal)\n",
        "\n",
        "        return signals\n",
        "\n",
        "    def _calculate_bit_flip_rates(self, frames: List[CANFrame], start_bit: int, end_bit: int) -> List[float]:\n",
        "        \"\"\"\n",
        "        Calculate flip rates for each bit in the specified range.\n",
        "\n",
        "        Args:\n",
        "            frames: List of CAN frames\n",
        "            start_bit: Starting bit position\n",
        "            end_bit: Ending bit position\n",
        "\n",
        "        Returns:\n",
        "            List of flip rates for each bit\n",
        "        \"\"\"\n",
        "        num_bits = end_bit - start_bit + 1\n",
        "        bit_values = np.zeros((num_bits, len(frames)), dtype=int)\n",
        "\n",
        "        # Extract bit values\n",
        "        for bit_idx in range(num_bits):\n",
        "            for frame_idx, frame in enumerate(frames):\n",
        "                bit_values[bit_idx, frame_idx] = frame.get_bit(start_bit + bit_idx)\n",
        "\n",
        "        # Calculate flip rates\n",
        "        flip_rates = []\n",
        "        for bit_idx in range(num_bits):\n",
        "            bit_data = bit_values[bit_idx, :]\n",
        "            flips = sum(1 for i in range(len(bit_data)-1) if bit_data[i] != bit_data[i+1])\n",
        "            flip_rate = flips / (len(bit_data) - 1) if len(bit_data) > 1 else 0\n",
        "            flip_rates.append(flip_rate)\n",
        "\n",
        "        return flip_rates\n",
        "\n",
        "    def _cluster_bits(self, bit_flip_rates: List[float], eps=0.15, min_samples=2) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Cluster bits based on their flip rates using DBSCAN.\n",
        "\n",
        "        Args:\n",
        "            bit_flip_rates: List of bit flip rates\n",
        "            eps: DBSCAN epsilon parameter\n",
        "            min_samples: DBSCAN min_samples parameter\n",
        "\n",
        "        Returns:\n",
        "            List of lists where each inner list contains bit indices for a cluster\n",
        "        \"\"\"\n",
        "        # Convert flip rates to 2D array for DBSCAN\n",
        "        X = np.array(bit_flip_rates).reshape(-1, 1)\n",
        "\n",
        "        # Apply DBSCAN\n",
        "        clustering = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
        "        labels = clustering.labels_\n",
        "\n",
        "        # Group bits by cluster\n",
        "        clusters = defaultdict(list)\n",
        "        for i, label in enumerate(labels):\n",
        "            clusters[label].append(i)\n",
        "\n",
        "        # Return list of bit clusters (including noise cluster)\n",
        "        return list(clusters.values())\n",
        "\n",
        "    def build_correlation_graph(self, min_correlation=0.8):\n",
        "        \"\"\"\n",
        "        Build a graph of signals based on their correlation.\n",
        "\n",
        "        Args:\n",
        "            min_correlation: Minimum correlation coefficient to create an edge\n",
        "        \"\"\"\n",
        "        if not self.signals_by_id:\n",
        "            raise ValueError(\"No signals identified. Run identify_signals() first.\")\n",
        "\n",
        "        all_signals = []\n",
        "        for signals in self.signals_by_id.values():\n",
        "            all_signals.extend(signals)\n",
        "\n",
        "        G = nx.Graph()\n",
        "\n",
        "        for i, signal in enumerate(all_signals):\n",
        "            G.add_node(i,\n",
        "                      can_id=signal.can_id,\n",
        "                      signal_type=signal.signal_type,\n",
        "                      bits=f\"{signal.start_bit}:{signal.start_bit+signal.num_bits}\",\n",
        "                      start_bit=signal.start_bit,\n",
        "                      num_bits=signal.num_bits)\n",
        "\n",
        "        for i, signal1 in enumerate(all_signals):\n",
        "            if len(signal1.values) < 10:\n",
        "                continue\n",
        "\n",
        "            for j in range(i+1, len(all_signals)):\n",
        "                signal2 = all_signals[j]\n",
        "                if len(signal2.values) < 10:\n",
        "                    continue\n",
        "\n",
        "                min_len = min(len(signal1.values), len(signal2.values))\n",
        "                values1 = signal1.values[:min_len]\n",
        "                values2 = signal2.values[:min_len]\n",
        "\n",
        "                if len(np.unique(values1)) <= 1 or len(np.unique(values2)) <= 1:\n",
        "                    continue\n",
        "\n",
        "                corr = np.corrcoef(values1, values2)[0, 1]\n",
        "\n",
        "                if abs(corr) >= min_correlation:\n",
        "                    G.add_edge(i, j, weight=abs(corr))\n",
        "\n",
        "        self.correlation_graph = G\n",
        "        return G\n",
        "\n",
        "\n",
        "    def discover_mathematical_relationships(self):\n",
        "        \"\"\"\n",
        "        Discover mathematical relationships between signals.\n",
        "\n",
        "        Returns:\n",
        "            List of tuples (signal1, relationship, signal2, strength)\n",
        "        \"\"\"\n",
        "        # Ensure we have identified signals first\n",
        "        if not self.signals_by_id:\n",
        "            raise ValueError(\"No signals identified. Run identify_signals() first.\")\n",
        "\n",
        "        # Create a flat list of all signals\n",
        "        all_signals = []\n",
        "        for signals in self.signals_by_id.values():\n",
        "            all_signals.extend([s for s in signals if s.signal_type == 'uint' and len(s.values) > 50])\n",
        "\n",
        "        relationships = []\n",
        "\n",
        "        # Check for derivative relationships\n",
        "        for i, signal1 in enumerate(all_signals):\n",
        "            for j, signal2 in enumerate(all_signals):\n",
        "                if i == j:\n",
        "                    continue\n",
        "\n",
        "                # Ensure signals have enough values\n",
        "                min_len = min(len(signal1.values), len(signal2.values))\n",
        "                if min_len < 50:\n",
        "                    continue\n",
        "\n",
        "                # Get time-aligned values\n",
        "                values1 = np.array(signal1.values[:min_len])\n",
        "                values2 = np.array(signal2.values[:min_len])\n",
        "\n",
        "                # Check for derivative relationship\n",
        "                if min_len > 2:\n",
        "                    derivative1 = np.diff(values1)\n",
        "                    correlation = np.corrcoef(derivative1, values2[1:min_len])[0, 1]\n",
        "                    if np.isnan(correlation):\n",
        "                        continue\n",
        "\n",
        "                    if abs(correlation) > 0.8:\n",
        "                        relationships.append((signal1, 'derivative', signal2, abs(correlation)))\n",
        "\n",
        "                # Check for integral relationship\n",
        "                if min_len > 2:\n",
        "                    # Compute a simple integral (cumulative sum)\n",
        "                    integral2 = np.cumsum(values2)\n",
        "                    # Compute correlation with values1\n",
        "                    correlation = np.corrcoef(values1, integral2[:min_len])[0, 1]\n",
        "                    if np.isnan(correlation):\n",
        "                        continue\n",
        "\n",
        "                    if abs(correlation) > 0.8:\n",
        "                        relationships.append((signal2, 'integral', signal1, abs(correlation)))\n",
        "\n",
        "        return relationships\n",
        "\n",
        "    def visualize_hamd_heatmap(self, can_id: str = None, figsize=(12, 8)):\n",
        "        \"\"\"\n",
        "        Visualize the HAMD (Hamming distance distribution) as a heatmap.\n",
        "\n",
        "        Args:\n",
        "            can_id: Optional CAN ID to filter by (None for all IDs)\n",
        "            figsize: Figure size tuple (width, height)\n",
        "        \"\"\"\n",
        "        # Ensure we have identified signals first\n",
        "        if not self.signals_by_id:\n",
        "            raise ValueError(\"No signals identified. Run identify_signals() first.\")\n",
        "\n",
        "        # Get signals for the specified CAN ID or all signals\n",
        "        if can_id is not None:\n",
        "            if can_id not in self.signals_by_id:\n",
        "                raise ValueError(f\"No signals found for CAN ID: {can_id}\")\n",
        "            signals = self.signals_by_id[can_id]\n",
        "        else:\n",
        "            # Flatten the dict of signals\n",
        "            signals = []\n",
        "            for sig_list in self.signals_by_id.values():\n",
        "                signals.extend(sig_list)\n",
        "\n",
        "        # Create a list of HAMD values and their bit positions\n",
        "        hamd_data = []\n",
        "        for signal in signals:\n",
        "            if signal.hamd:\n",
        "                for i, value in enumerate(signal.hamd):\n",
        "                    hamd_data.append({\n",
        "                        'can_id': signal.can_id,\n",
        "                        'bit': signal.start_bit + i,\n",
        "                        'hamd': value\n",
        "                    })\n",
        "\n",
        "        if not hamd_data:\n",
        "            print(\"No HAMD data available for visualization.\")\n",
        "            return\n",
        "\n",
        "        # Convert to DataFrame for easier plotting\n",
        "        df = pd.DataFrame(hamd_data)\n",
        "\n",
        "        # Create a pivot table for the heatmap\n",
        "        pivot_df = df.pivot_table(index='can_id', columns='bit', values='hamd', aggfunc='mean')\n",
        "\n",
        "        # Plot the heatmap\n",
        "        plt.figure(figsize=figsize)\n",
        "        sns.heatmap(pivot_df, cmap='viridis', linewidths=0.5, linecolor='black')\n",
        "        plt.title('HAMD Heatmap')\n",
        "        plt.xlabel('Bit Position')\n",
        "        plt.ylabel('CAN ID')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_correlation_graph(self, figsize=(12, 10)):\n",
        "        \"\"\"\n",
        "        Visualize the correlation graph of signals.\n",
        "\n",
        "        Args:\n",
        "            figsize: Figure size tuple (width, height)\n",
        "        \"\"\"\n",
        "        if self.correlation_graph is None:\n",
        "            self.build_correlation_graph()\n",
        "\n",
        "        if len(self.correlation_graph) == 0:\n",
        "            print(\"No correlations found to visualize.\")\n",
        "            return\n",
        "\n",
        "        # Create position layout\n",
        "        pos = nx.spring_layout(self.correlation_graph, seed=42)\n",
        "\n",
        "        # Get node colors based on signal type\n",
        "        type_colors = {\n",
        "            'uint': 'blue',\n",
        "            'enum': 'green',\n",
        "            'rand': 'red',\n",
        "            'cyclic': 'purple',\n",
        "            'unknown': 'gray'\n",
        "        }\n",
        "\n",
        "        node_colors = [\n",
        "            type_colors[self.correlation_graph.nodes[n]['signal_type']]\n",
        "            for n in self.correlation_graph.nodes\n",
        "        ]\n",
        "\n",
        "        # Get edge weights\n",
        "        edge_weights = [\n",
        "            self.correlation_graph[u][v]['weight'] * 3\n",
        "            for u, v in self.correlation_graph.edges\n",
        "        ]\n",
        "\n",
        "        # Create the plot\n",
        "        plt.figure(figsize=figsize)\n",
        "        nx.draw_networkx_nodes(\n",
        "            self.correlation_graph, pos,\n",
        "            node_color=node_colors, alpha=0.8\n",
        "        )\n",
        "        nx.draw_networkx_edges(\n",
        "            self.correlation_graph, pos,\n",
        "            width=edge_weights, alpha=0.5\n",
        "        )\n",
        "\n",
        "        # Add labels\n",
        "        labels = {\n",
        "            n: f\"{self.correlation_graph.nodes[n]['can_id']}\\n{self.correlation_graph.nodes[n]['bits']}\"\n",
        "            for n in self.correlation_graph.nodes\n",
        "        }\n",
        "        nx.draw_networkx_labels(\n",
        "            self.correlation_graph, pos, labels, font_size=8\n",
        "        )\n",
        "\n",
        "        # Add legend\n",
        "        legend_elements = [\n",
        "            plt.Line2D(\n",
        "                [0], [0], marker='o', color='w',\n",
        "                markerfacecolor=color, markersize=10, label=type_\n",
        "            )\n",
        "            for type_, color in type_colors.items()\n",
        "        ]\n",
        "        plt.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "        plt.title('Signal Correlation Graph')\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def save_results(self, output_dir: str):\n",
        "        \"\"\"Save results to the specified output directory.\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save identified signals\n",
        "        with open(os.path.join(output_dir, 'signals.pkl'), 'wb') as f:\n",
        "            pickle.dump(self.signals_by_id, f)\n",
        "\n",
        "        # Save signal summary as CSV\n",
        "        summary_data = []\n",
        "        for can_id, signals in self.signals_by_id.items():\n",
        "            for signal in signals:\n",
        "                summary_data.append({\n",
        "                    'can_id': can_id,\n",
        "                    'start_bit': signal.start_bit,\n",
        "                    'num_bits': signal.num_bits,\n",
        "                    'signal_type': signal.signal_type,\n",
        "                    'label': signal.label or '',\n",
        "                    'min_value': signal.min_value,\n",
        "                    'max_value': signal.max_value,\n",
        "                    'avg_value': signal.avg_value,\n",
        "                    'distinct_values': signal.distinct_values,\n",
        "                    'flip_rate': signal.flip_rate\n",
        "                })\n",
        "\n",
        "        if summary_data:\n",
        "            pd.DataFrame(summary_data).to_csv(os.path.join(output_dir, 'signal_summary.csv'), index=False)\n",
        "\n",
        "        # Save correlation graph as GraphML\n",
        "        if self.correlation_graph is not None:\n",
        "            nx.write_graphml(self.correlation_graph, os.path.join(output_dir, 'correlation_graph.graphml'))\n",
        "\n",
        "        print(f\"Results saved to {output_dir}\")\n",
        "\n",
        "\n",
        "class MultiFileAnalyzer:\n",
        "    \"\"\"\n",
        "    Class to analyze multiple CAN log files and aggregate results.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, log_files: List[str], output_dir: str = \"autocan_results\"):\n",
        "        \"\"\"\n",
        "        Initialize the multi-file analyzer.\n",
        "\n",
        "        Args:\n",
        "            log_files: List of paths to CAN log files\n",
        "            output_dir: Main output directory\n",
        "        \"\"\"\n",
        "        self.log_files = log_files\n",
        "        self.output_dir = output_dir\n",
        "        self.analyzers = {}  # Dictionary to store analyzers for each file\n",
        "        self.combined_signals = {}  # Signals from all files combined\n",
        "        self.file_stats = {}  # Statistics for each file\n",
        "        self.common_signals = {}  # Signals common across all files\n",
        "        self.unique_signals = {}  # Signals unique to specific files\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    def analyze_files(self, parallel: bool = True):\n",
        "        \"\"\"\n",
        "        Analyze all log files, optionally in parallel.\n",
        "\n",
        "        Args:\n",
        "            parallel: Whether to process files in parallel\n",
        "        \"\"\"\n",
        "        if parallel and len(self.log_files) > 1:\n",
        "            self._analyze_parallel()\n",
        "        else:\n",
        "            self._analyze_sequential()\n",
        "\n",
        "        # Combine results from all files\n",
        "        self._combine_results()\n",
        "\n",
        "    def _analyze_sequential(self):\n",
        "        \"\"\"Analyze files sequentially.\"\"\"\n",
        "        for file_path in tqdm(self.log_files, desc=\"Analyzing files\"):\n",
        "            self._analyze_single_file(file_path)\n",
        "\n",
        "    def _analyze_parallel(self):\n",
        "        \"\"\"Analyze files in parallel using process pool.\"\"\"\n",
        "        num_processors = min(multiprocessing.cpu_count(), len(self.log_files))\n",
        "        print(f\"Using {num_processors} processors for parallel analysis\")\n",
        "\n",
        "        results = {}  # Container for results\n",
        "\n",
        "        with ProcessPoolExecutor(max_workers=num_processors) as executor:\n",
        "            # Map file paths to future objects\n",
        "            futures = {executor.submit(self._analyze_single_file, file_path): file_path\n",
        "                      for file_path in self.log_files}\n",
        "\n",
        "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Analyzing files\"):\n",
        "                file_path = futures[future]\n",
        "                try:\n",
        "                    # Get the result (analyzer) from the completed task\n",
        "                    result = future.result()\n",
        "                    if result is not True:  # If it's an analyzer object, not just True\n",
        "                        file_name = os.path.basename(file_path)\n",
        "                        results[file_name] = result\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "        # Update the main process's analyzers dictionary with results from all workers\n",
        "        self.analyzers.update(results)\n",
        "\n",
        "    def _analyze_single_file(self, file_path: str):\n",
        "        \"\"\"\n",
        "        Analyze a single CAN log file.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the log file\n",
        "        \"\"\"\n",
        "        file_name = os.path.basename(file_path)\n",
        "        print(f\"\\nAnalyzing {file_name}...\")\n",
        "\n",
        "        # Create a directory for this file's results\n",
        "        file_output_dir = os.path.join(self.output_dir, os.path.splitext(file_name)[0])\n",
        "        os.makedirs(file_output_dir, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            # Load and analyze the trace\n",
        "            can_trace = CANTrace.from_log(file_path)\n",
        "\n",
        "            # Store basic statistics\n",
        "            self.file_stats[file_name] = {\n",
        "                'frame_count': len(can_trace),\n",
        "                'unique_id_count': len(can_trace.get_unique_ids()),\n",
        "                'unique_ids': list(can_trace.get_unique_ids())\n",
        "            }\n",
        "\n",
        "            # Create analyzer and identify signals\n",
        "            analyzer = AutoCANAnalyzer(can_trace)\n",
        "            analyzer.identify_signals()\n",
        "\n",
        "            # Build correlation graph\n",
        "            analyzer.build_correlation_graph()\n",
        "\n",
        "            # Discover relationships\n",
        "            relationships = analyzer.discover_mathematical_relationships()\n",
        "\n",
        "            # Store relationships in file stats\n",
        "            self.file_stats[file_name]['relationships'] = [\n",
        "                {\n",
        "                    'signal1_id': rel[0].can_id,\n",
        "                    'signal1_bits': f\"{rel[0].start_bit}:{rel[0].start_bit+rel[0].num_bits}\",\n",
        "                    'relationship': rel[1],\n",
        "                    'signal2_id': rel[2].can_id,\n",
        "                    'signal2_bits': f\"{rel[2].start_bit}:{rel[2].start_bit+rel[2].num_bits}\",\n",
        "                    'strength': rel[3]\n",
        "                }\n",
        "                for rel in relationships\n",
        "            ]\n",
        "\n",
        "            # Save individual file results\n",
        "            analyzer.save_results(file_output_dir)\n",
        "\n",
        "            # Store the analyzer for later use\n",
        "            self.analyzers[file_name] = analyzer\n",
        "\n",
        "            print(f\"Completed analysis of {file_name}\")\n",
        "            return analyzer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing {file_name}: {e}\")\n",
        "            return False\n",
        "    def _combine_results(self):\n",
        "        \"\"\"Combine results from all analyzed files.\"\"\"\n",
        "        print(\"\\nCombining results from all files...\")\n",
        "\n",
        "        # Check if we have any analyzers\n",
        "        if not self.analyzers:\n",
        "            print(\"No files were successfully analyzed. Cannot combine results.\")\n",
        "            return\n",
        "\n",
        "        # Collect all unique CAN IDs across all files\n",
        "        all_can_ids = set()\n",
        "        for file_name, analyzer in self.analyzers.items():\n",
        "            all_can_ids.update(analyzer.signals_by_id.keys())\n",
        "\n",
        "        # Check if we found any CAN IDs\n",
        "        if not all_can_ids:\n",
        "            print(\"No CAN IDs found across any files. Cannot combine results.\")\n",
        "            return\n",
        "\n",
        "        # Prepare combined signals dictionary\n",
        "        self.combined_signals = {}\n",
        "\n",
        "        # Find common and unique signals\n",
        "        common_can_ids = set()\n",
        "        file_specific_can_ids = defaultdict(set)\n",
        "\n",
        "        # First pass: collect CAN IDs present in all files\n",
        "        for can_id in all_can_ids:\n",
        "            present_in_all = all(can_id in analyzer.signals_by_id\n",
        "                              for analyzer in self.analyzers.values())\n",
        "            if present_in_all:\n",
        "                common_can_ids.add(can_id)\n",
        "            else:\n",
        "                # Record which files have this CAN ID\n",
        "                for file_name, analyzer in self.analyzers.items():\n",
        "                    if can_id in analyzer.signals_by_id:\n",
        "                        file_specific_can_ids[file_name].add(can_id)\n",
        "\n",
        "        # Collect common signals\n",
        "        for can_id in common_can_ids:\n",
        "            self.common_signals[can_id] = []\n",
        "            self.combined_signals[can_id] = []\n",
        "\n",
        "            # Get all signals for this CAN ID from the first file\n",
        "            first_file = list(self.analyzers.keys())[0]\n",
        "            if not self.analyzers[first_file].signals_by_id.get(can_id):\n",
        "                # Skip if the first file doesn't have this CAN ID (should not happen due to our check)\n",
        "                continue\n",
        "\n",
        "            base_signals = self.analyzers[first_file].signals_by_id[can_id]\n",
        "\n",
        "            for base_signal in base_signals:\n",
        "                # Check if this signal is present in all files\n",
        "                signal_in_all = True\n",
        "                for file_name, analyzer in self.analyzers.items():\n",
        "                    if file_name == first_file:\n",
        "                        continue\n",
        "\n",
        "                    # Skip files that don't have this CAN ID\n",
        "                    if can_id not in analyzer.signals_by_id:\n",
        "                        signal_in_all = False\n",
        "                        break\n",
        "\n",
        "                    # Look for matching signal (same start bit and num bits)\n",
        "                    match_found = False\n",
        "                    for signal in analyzer.signals_by_id[can_id]:\n",
        "                        if (signal.start_bit == base_signal.start_bit and\n",
        "                            signal.num_bits == base_signal.num_bits):\n",
        "                            match_found = True\n",
        "                            break\n",
        "\n",
        "                    if not match_found:\n",
        "                        signal_in_all = False\n",
        "                        break\n",
        "\n",
        "                if signal_in_all:\n",
        "                    self.common_signals[can_id].append(base_signal)\n",
        "\n",
        "                # Add to combined signals regardless\n",
        "                self.combined_signals[can_id].append(base_signal)\n",
        "\n",
        "        # Collect unique signals\n",
        "        self.unique_signals = defaultdict(dict)\n",
        "        for file_name, can_ids in file_specific_can_ids.items():\n",
        "            for can_id in can_ids:\n",
        "                # Skip if the file doesn't have this CAN ID (should not happen)\n",
        "                if can_id not in self.analyzers[file_name].signals_by_id:\n",
        "                    continue\n",
        "\n",
        "                self.unique_signals[file_name][can_id] = self.analyzers[file_name].signals_by_id[can_id]\n",
        "\n",
        "                # Add to combined signals if not already present\n",
        "                if can_id not in self.combined_signals:\n",
        "                    self.combined_signals[can_id] = self.analyzers[file_name].signals_by_id[can_id].copy()\n",
        "                else:\n",
        "                    # Add signals that don't overlap with existing ones\n",
        "                    existing_signals = self.combined_signals[can_id]\n",
        "                    for signal in self.analyzers[file_name].signals_by_id[can_id]:\n",
        "                        overlap = False\n",
        "                        for existing in existing_signals:\n",
        "                            # Check if signals overlap\n",
        "                            signal_start = signal.start_bit\n",
        "                            signal_end = signal.start_bit + signal.num_bits - 1\n",
        "                            existing_start = existing.start_bit\n",
        "                            existing_end = existing.start_bit + existing.num_bits - 1\n",
        "\n",
        "                            if not (signal_end < existing_start or signal_start > existing_end):\n",
        "                                overlap = True\n",
        "                                break\n",
        "\n",
        "                        if not overlap:\n",
        "                            self.combined_signals[can_id].append(signal)\n",
        "\n",
        "        # Generate combined results\n",
        "        self._generate_combined_report()\n",
        "\n",
        "    def _generate_combined_report(self):\n",
        "        \"\"\"Generate a comprehensive report combining results from all files.\"\"\"\n",
        "        print(\"Generating combined report...\")\n",
        "\n",
        "        # Create a directory for combined results\n",
        "        combined_dir = os.path.join(self.output_dir, \"combined_results\")\n",
        "        os.makedirs(combined_dir, exist_ok=True)\n",
        "\n",
        "        # Generate file statistics summary\n",
        "        stats_data = []\n",
        "        for file_name, stats in self.file_stats.items():\n",
        "            stats_data.append({\n",
        "                'file_name': file_name,\n",
        "                'frame_count': stats['frame_count'],\n",
        "                'unique_id_count': stats['unique_id_count'],\n",
        "                'relationship_count': len(stats.get('relationships', []))\n",
        "            })\n",
        "\n",
        "        if stats_data:\n",
        "            pd.DataFrame(stats_data).to_csv(os.path.join(combined_dir, 'file_statistics.csv'), index=False)\n",
        "\n",
        "        # Generate combined signal summary\n",
        "        signal_data = []\n",
        "        for can_id, signals in self.combined_signals.items():\n",
        "            for signal in signals:\n",
        "                # Determine which files contain this signal\n",
        "                containing_files = []\n",
        "                for file_name, analyzer in self.analyzers.items():\n",
        "                    if can_id in analyzer.signals_by_id:\n",
        "                        for file_signal in analyzer.signals_by_id[can_id]:\n",
        "                            if (file_signal.start_bit == signal.start_bit and\n",
        "                                file_signal.num_bits == signal.num_bits):\n",
        "                                containing_files.append(file_name)\n",
        "                                break\n",
        "\n",
        "                # Calculate presence percentage\n",
        "                presence_pct = (len(containing_files) / len(self.analyzers)) * 100\n",
        "\n",
        "                # Handle potential None values for statistics\n",
        "                signal_dict = {\n",
        "                    'can_id': can_id,\n",
        "                    'start_bit': signal.start_bit,\n",
        "                    'num_bits': signal.num_bits,\n",
        "                    'signal_type': signal.signal_type,\n",
        "                    'presence_percentage': presence_pct,\n",
        "                    'present_in_files': ', '.join(containing_files),\n",
        "                    'is_common': presence_pct == 100,\n",
        "                }\n",
        "\n",
        "                # Add statistics if available, otherwise use None\n",
        "                for stat_name in ['min_value', 'max_value', 'avg_value', 'distinct_values', 'flip_rate']:\n",
        "                    stat_value = getattr(signal, stat_name)\n",
        "                    signal_dict[stat_name] = stat_value if stat_value is not None else float('nan')\n",
        "\n",
        "                signal_data.append(signal_dict)\n",
        "\n",
        "        if signal_data:\n",
        "            pd.DataFrame(signal_data).to_csv(os.path.join(combined_dir, 'combined_signals.csv'), index=False)\n",
        "\n",
        "        # Generate Venn diagram of common CAN IDs\n",
        "        if len(self.analyzers) <= 5:  # Limit to 5 files for Venn diagram clarity\n",
        "            self._generate_venn_diagram(combined_dir)\n",
        "\n",
        "        # Generate relationship report\n",
        "        relationship_data = []\n",
        "        for file_name, stats in self.file_stats.items():\n",
        "            for rel in stats.get('relationships', []):\n",
        "                relationship_data.append({\n",
        "                    'file_name': file_name,\n",
        "                    'signal1_id': rel['signal1_id'],\n",
        "                    'signal1_bits': rel['signal1_bits'],\n",
        "                    'relationship': rel['relationship'],\n",
        "                    'signal2_id': rel['signal2_id'],\n",
        "                    'signal2_bits': rel['signal2_bits'],\n",
        "                    'strength': rel['strength']\n",
        "                })\n",
        "\n",
        "        if relationship_data:\n",
        "            pd.DataFrame(relationship_data).to_csv(os.path.join(combined_dir, 'relationships.csv'), index=False)\n",
        "\n",
        "        print(f\"Combined report generated in {combined_dir}\")\n",
        "\n",
        "    def _generate_venn_diagram(self, output_dir):\n",
        "        \"\"\"Generate a Venn diagram showing overlapping CAN IDs between files.\"\"\"\n",
        "        try:\n",
        "            from matplotlib_venn import venn2, venn3\n",
        "\n",
        "            # Collect sets of CAN IDs for each file\n",
        "            file_can_ids = {\n",
        "                file_name: set(analyzer.signals_by_id.keys())\n",
        "                for file_name, analyzer in self.analyzers.items()\n",
        "            }\n",
        "\n",
        "            # Check if we have any data to visualize\n",
        "            if not file_can_ids or all(len(ids) == 0 for ids in file_can_ids.values()):\n",
        "                print(\"No CAN IDs found to create Venn diagram\")\n",
        "                return\n",
        "\n",
        "            plt.figure(figsize=(10, 8))\n",
        "\n",
        "            if len(file_can_ids) == 2:\n",
        "                # Create a Venn diagram for two files\n",
        "                file_names = list(file_can_ids.keys())\n",
        "                # Check if both sets have data\n",
        "                if len(file_can_ids[file_names[0]]) == 0 or len(file_can_ids[file_names[1]]) == 0:\n",
        "                    print(\"One or more files have no CAN IDs for Venn diagram\")\n",
        "                    plt.close()\n",
        "                    return\n",
        "\n",
        "                venn2(\n",
        "                    [file_can_ids[file_names[0]], file_can_ids[file_names[1]]],\n",
        "                    set_labels=file_names\n",
        "                )\n",
        "                plt.title('Common CAN IDs Between Files')\n",
        "                plt.savefig(os.path.join(output_dir, 'venn_diagram.png'))\n",
        "                plt.close()\n",
        "\n",
        "            elif len(file_can_ids) == 3:\n",
        "                # Create a Venn diagram for three files\n",
        "                file_names = list(file_can_ids.keys())\n",
        "                # Check if all three sets have data\n",
        "                if (len(file_can_ids[file_names[0]]) == 0 or\n",
        "                    len(file_can_ids[file_names[1]]) == 0 or\n",
        "                    len(file_can_ids[file_names[2]]) == 0):\n",
        "                    print(\"One or more files have no CAN IDs for Venn diagram\")\n",
        "                    plt.close()\n",
        "                    return\n",
        "\n",
        "                venn3(\n",
        "                    [file_can_ids[file_names[0]], file_can_ids[file_names[1]], file_can_ids[file_names[2]]],\n",
        "                    set_labels=file_names\n",
        "                )\n",
        "                plt.title('Common CAN IDs Between Files')\n",
        "                plt.savefig(os.path.join(output_dir, 'venn_diagram.png'))\n",
        "                plt.close()\n",
        "\n",
        "            else:\n",
        "                # For more than 3 files, create a similarity matrix\n",
        "                file_names = list(file_can_ids.keys())\n",
        "                n_files = len(file_names)\n",
        "\n",
        "                # Check if we have enough data for a meaningful matrix\n",
        "                empty_files = [name for name in file_names if len(file_can_ids[name]) == 0]\n",
        "                if empty_files:\n",
        "                    print(f\"The following files have no CAN IDs: {', '.join(empty_files)}\")\n",
        "                    if len(empty_files) == len(file_names):\n",
        "                        print(\"Cannot create similarity matrix: all files have no CAN IDs\")\n",
        "                        plt.close()\n",
        "                        return\n",
        "\n",
        "                similarity_matrix = np.zeros((n_files, n_files))\n",
        "\n",
        "                for i in range(n_files):\n",
        "                    for j in range(n_files):\n",
        "                        if i == j:\n",
        "                            similarity_matrix[i, j] = 1.0\n",
        "                        else:\n",
        "                            # Avoid division by zero with empty sets\n",
        "                            union_size = len(file_can_ids[file_names[i]] | file_can_ids[file_names[j]])\n",
        "                            if union_size > 0:\n",
        "                                # Jaccard similarity: intersection / union\n",
        "                                intersection = len(file_can_ids[file_names[i]] & file_can_ids[file_names[j]])\n",
        "                                similarity_matrix[i, j] = intersection / union_size\n",
        "                            else:\n",
        "                                similarity_matrix[i, j] = 0\n",
        "\n",
        "                plt.figure(figsize=(10, 8))\n",
        "                sns.heatmap(\n",
        "                    similarity_matrix,\n",
        "                    annot=True,\n",
        "                    xticklabels=[os.path.basename(f) for f in file_names],\n",
        "                    yticklabels=[os.path.basename(f) for f in file_names],\n",
        "                    cmap='viridis'\n",
        "                )\n",
        "                plt.title('CAN ID Similarity Between Files (Jaccard Index)')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(output_dir, 'similarity_matrix.png'))\n",
        "                plt.close()\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"matplotlib_venn package not available. Skipping Venn diagram generation.\")\n",
        "            # Create a simple bar chart instead\n",
        "            file_names = list(self.analyzers.keys())\n",
        "\n",
        "            # Check if we have any analyzers with signals\n",
        "            valid_counts = [len(analyzer.signals_by_id) for analyzer in self.analyzers.values()]\n",
        "            if not valid_counts or all(count == 0 for count in valid_counts):\n",
        "                print(\"No valid CAN ID counts to visualize\")\n",
        "                return\n",
        "\n",
        "            can_id_counts = valid_counts\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.bar(file_names, can_id_counts)\n",
        "            plt.title('Number of CAN IDs per File')\n",
        "            plt.xticks(rotation=45, ha='right')\n",
        "            plt.ylabel('Count')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(output_dir, 'can_id_counts.png'))\n",
        "            plt.close()\n",
        "\n",
        "    def visualize_combined_results(self):\n",
        "        \"\"\"Generate visualizations for the combined results.\"\"\"\n",
        "        print(\"Generating combined visualizations...\")\n",
        "\n",
        "        # Create a directory for visualizations\n",
        "        viz_dir = os.path.join(self.output_dir, \"visualizations\")\n",
        "        os.makedirs(viz_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Generate bar chart of signal types across all files\n",
        "        self._visualize_signal_types(viz_dir)\n",
        "\n",
        "        # 2. Generate a heatmap of signal presence across files\n",
        "        self._visualize_signal_presence(viz_dir)\n",
        "\n",
        "        # 3. Generate network graph of common relationships\n",
        "        self._visualize_relationship_network(viz_dir)\n",
        "\n",
        "        print(f\"Combined visualizations generated in {viz_dir}\")\n",
        "\n",
        "    def _visualize_signal_types(self, output_dir):\n",
        "        \"\"\"Visualize distribution of signal types across files.\"\"\"\n",
        "        # Count signal types for each file\n",
        "        signal_type_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "        for file_name, analyzer in self.analyzers.items():\n",
        "            for signals in analyzer.signals_by_id.values():\n",
        "                for signal in signals:\n",
        "                    signal_type_counts[file_name][signal.signal_type] += 1\n",
        "\n",
        "        # Convert to DataFrame for plotting\n",
        "        data = []\n",
        "        for file_name, type_counts in signal_type_counts.items():\n",
        "            for signal_type, count in type_counts.items():\n",
        "                data.append({\n",
        "                    'file_name': os.path.basename(file_name),\n",
        "                    'signal_type': signal_type,\n",
        "                    'count': count\n",
        "                })\n",
        "\n",
        "        if not data:\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Create stacked bar chart\n",
        "        plt.figure(figsize=(12, 7))\n",
        "        ax = sns.barplot(x='file_name', y='count', hue='signal_type', data=df)\n",
        "        plt.title('Signal Types Distribution Across Files')\n",
        "        plt.xlabel('File')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.legend(title='Signal Type')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, 'signal_types.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def _visualize_signal_presence(self, output_dir):\n",
        "        \"\"\"Visualize presence of signals across files as a heatmap.\"\"\"\n",
        "        # Collect all unique CAN IDs and signal bit ranges\n",
        "        all_can_ids = set()\n",
        "        signal_specs = set()  # (can_id, start_bit, num_bits)\n",
        "\n",
        "        for analyzer in self.analyzers.values():\n",
        "            for can_id, signals in analyzer.signals_by_id.items():\n",
        "                all_can_ids.add(can_id)\n",
        "                for signal in signals:\n",
        "                    signal_specs.add((can_id, signal.start_bit, signal.num_bits))\n",
        "\n",
        "        # Check if we have any signals to visualize\n",
        "        if not signal_specs:\n",
        "            print(\"No signals found to visualize presence across files.\")\n",
        "            return\n",
        "\n",
        "        # Create presence matrix\n",
        "        file_names = list(self.analyzers.keys())\n",
        "        if not file_names:\n",
        "            print(\"No files to visualize.\")\n",
        "            return\n",
        "\n",
        "        presence_matrix = np.zeros((len(signal_specs), len(file_names)))\n",
        "\n",
        "        # Convert signal_specs to list and sort for consistent ordering\n",
        "        signal_specs = sorted(list(signal_specs))\n",
        "\n",
        "        for i, (can_id, start_bit, num_bits) in enumerate(signal_specs):\n",
        "            for j, file_name in enumerate(file_names):\n",
        "                analyzer = self.analyzers[file_name]\n",
        "                if can_id in analyzer.signals_by_id:\n",
        "                    for signal in analyzer.signals_by_id[can_id]:\n",
        "                        if signal.start_bit == start_bit and signal.num_bits == num_bits:\n",
        "                            presence_matrix[i, j] = 1\n",
        "                            break\n",
        "\n",
        "        # Create signal labels\n",
        "        signal_labels = [f\"{can_id}[{start_bit}:{start_bit+num_bits-1}]\"\n",
        "                        for can_id, start_bit, num_bits in signal_specs]\n",
        "\n",
        "        # Only plot if we have a reasonable number of signals\n",
        "        if len(signal_specs) > 100:\n",
        "            # If too many signals, just show common ones\n",
        "            common_indices = []\n",
        "            for i in range(len(signal_specs)):\n",
        "                if np.all(presence_matrix[i, :] == 1):\n",
        "                    common_indices.append(i)\n",
        "\n",
        "            # Check if we found any common signals\n",
        "            if not common_indices:\n",
        "                # If no common signals, just take the first 100\n",
        "                common_indices = list(range(min(100, len(signal_specs))))\n",
        "                if not common_indices:  # Safety check for empty list\n",
        "                    print(\"No signals to visualize in the matrix.\")\n",
        "                    return\n",
        "\n",
        "                plt_title = 'Top 100 Signals Across Files'\n",
        "            else:\n",
        "                plt_title = 'Common Signals Across All Files'\n",
        "\n",
        "            presence_matrix = presence_matrix[common_indices, :]\n",
        "            signal_labels = [signal_labels[i] for i in common_indices]\n",
        "\n",
        "            plt.figure(figsize=(12, max(8, len(common_indices) * 0.25)))\n",
        "            sns.heatmap(\n",
        "                presence_matrix,\n",
        "                cmap=['white', 'darkgreen'],\n",
        "                linewidths=0.5,\n",
        "                yticklabels=signal_labels,\n",
        "                xticklabels=[os.path.basename(f) for f in file_names],\n",
        "                cbar=False\n",
        "            )\n",
        "            plt.title(plt_title)\n",
        "\n",
        "        elif len(signal_specs) > 0:\n",
        "            plt.figure(figsize=(12, max(8, len(signal_specs) * 0.25)))\n",
        "            sns.heatmap(\n",
        "                presence_matrix,\n",
        "                cmap=['white', 'darkgreen'],\n",
        "                linewidths=0.5,\n",
        "                yticklabels=signal_labels,\n",
        "                xticklabels=[os.path.basename(f) for f in file_names],\n",
        "                cbar=False\n",
        "            )\n",
        "            plt.title('Signal Presence Across Files')\n",
        "        else:\n",
        "            print(\"No signals to visualize.\")\n",
        "            return\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, 'signal_presence.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def _visualize_relationship_network(self, output_dir):\n",
        "        \"\"\"Visualize network of mathematical relationships between signals.\"\"\"\n",
        "        # Collect all relationships\n",
        "        all_relationships = []\n",
        "        for file_name, stats in self.file_stats.items():\n",
        "            for rel in stats.get('relationships', []):\n",
        "                rel_tuple = (\n",
        "                    rel['signal1_id'], rel['signal1_bits'],\n",
        "                    rel['relationship'],\n",
        "                    rel['signal2_id'], rel['signal2_bits'],\n",
        "                    rel['strength'], file_name\n",
        "                )\n",
        "                all_relationships.append(rel_tuple)\n",
        "\n",
        "        if not all_relationships:\n",
        "            print(\"No mathematical relationships found to visualize.\")\n",
        "            return\n",
        "\n",
        "        # Create a directed graph\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        # Add nodes and edges\n",
        "        for s1_id, s1_bits, rel_type, s2_id, s2_bits, strength, file_name in all_relationships:\n",
        "            node1 = f\"{s1_id}[{s1_bits}]\"\n",
        "            node2 = f\"{s2_id}[{s2_bits}]\"\n",
        "\n",
        "            if node1 not in G:\n",
        "                G.add_node(node1, can_id=s1_id)\n",
        "            if node2 not in G:\n",
        "                G.add_node(node2, can_id=s2_id)\n",
        "\n",
        "            # Add edge with relationship type and file as attributes\n",
        "            G.add_edge(node1, node2, relationship=rel_type,\n",
        "                      strength=strength, file=os.path.basename(file_name))\n",
        "\n",
        "        if len(G) == 0:\n",
        "            return\n",
        "\n",
        "        # Use spring layout for positioning\n",
        "        pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "        plt.figure(figsize=(14, 10))\n",
        "\n",
        "        # Draw nodes\n",
        "        nx.draw_networkx_nodes(G, pos, node_size=700, node_color='skyblue', alpha=0.8)\n",
        "\n",
        "        # Draw edges with different colors for relationship types\n",
        "        relationship_colors = {\n",
        "            'derivative': 'red',\n",
        "            'integral': 'green',\n",
        "            'logical': 'blue',\n",
        "            'counter': 'purple'\n",
        "        }\n",
        "\n",
        "        # Group edges by relationship type\n",
        "        edges_by_type = defaultdict(list)\n",
        "        for u, v, data in G.edges(data=True):\n",
        "            rel_type = data.get('relationship', 'other')\n",
        "            edges_by_type[rel_type].append((u, v))\n",
        "\n",
        "        # Draw each group with appropriate color\n",
        "        for rel_type, edges in edges_by_type.items():\n",
        "            color = relationship_colors.get(rel_type, 'gray')\n",
        "            nx.draw_networkx_edges(G, pos, edgelist=edges, width=2, alpha=0.7,\n",
        "                                  edge_color=color, arrowsize=15)\n",
        "\n",
        "        # Add labels\n",
        "        nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')\n",
        "\n",
        "        # Add legend for relationship types\n",
        "        legend_elements = [\n",
        "            plt.Line2D([0], [0], color=color, lw=2, label=rel_type)\n",
        "            for rel_type, color in relationship_colors.items()\n",
        "            if rel_type in edges_by_type\n",
        "        ]\n",
        "\n",
        "        if legend_elements:  # Only add legend if we have elements\n",
        "            plt.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "        plt.title('Signal Relationship Network')\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(output_dir, 'relationship_network.png'))\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Process all CAN log files and perform comprehensive analysis.\n",
        "    \"\"\"\n",
        "    # Get all log files in the directory\n",
        "    log_files = glob.glob(\"*.log\")\n",
        "\n",
        "    if not log_files:\n",
        "        print(\"No log files found in the current directory.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(log_files)} log files: {', '.join(log_files)}\")\n",
        "\n",
        "    # Create analyzer for all files\n",
        "    analyzer = MultiFileAnalyzer(log_files, output_dir=\"autocan_results\")\n",
        "\n",
        "    # Analyze all files (parallel by default)\n",
        "    analyzer.analyze_files()\n",
        "\n",
        "    # Generate visualizations for combined results\n",
        "    analyzer.visualize_combined_results()\n",
        "\n",
        "    print(\"\\nAnalysis complete! Results are in the 'autocan_results' directory.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "uzcJb43p9GRA",
        "outputId": "b754edf3-fe77-49a1-96a2-5d03b2bc948e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing CAN IDs:   4%|▎         | 2/56 [00:00<00:09,  5.77it/s]WARNING:__main__:Skipping CAN ID 57F: insufficient frames (67 < 100)\n",
            "Processing CAN IDs:   5%|▌         | 3/56 [00:02<00:50,  1.06it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-07a58b1b777b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1488\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-07a58b1b777b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m     \u001b[0;31m# Process all log files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1477\u001b[0;31m     \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_log_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_extension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".log\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAnalysis complete! The following results have been generated:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-07a58b1b777b>\u001b[0m in \u001b[0;36mprocess_log_files\u001b[0;34m(self, file_extension, min_frames)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0;31m# Run analysis if sufficient data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmin_frames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m                 \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze_all_signals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                 \u001b[0;31m# Generate comparative visualizations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-07a58b1b777b>\u001b[0m in \u001b[0;36manalyze_all_signals\u001b[0;34m(self, min_frames)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \"\"\"\n\u001b[1;32m    452\u001b[0m         \u001b[0;31m# Extract signal frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_signal_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;31m# Build models for each CAN ID with sufficient data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-07a58b1b777b>\u001b[0m in \u001b[0;36mextract_signal_frames\u001b[0;34m(self, window_size, stride)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindow_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Process each byte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0mpayload_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_byte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0msequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-07a58b1b777b>\u001b[0m in \u001b[0;36mget_byte\u001b[0;34m(self, byte_position)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mbyte_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_payload\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_bit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_payload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mbyte_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbyte_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import logging\n",
        "import pickle\n",
        "from typing import List, Dict, Tuple, Optional, Set, Union\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CANFrame:\n",
        "    \"\"\"Represents a single CAN frame with ID and payload.\"\"\"\n",
        "\n",
        "    def __init__(self, can_id: str, payload: str, timestamp: float = 0.0):\n",
        "        \"\"\"\n",
        "        Initialize a CAN frame.\n",
        "\n",
        "        Args:\n",
        "            can_id: The CAN ID in hexadecimal format (e.g. '0x123')\n",
        "            payload: The payload in hexadecimal format (e.g. '00FF34A2')\n",
        "            timestamp: The timestamp when the frame was captured\n",
        "        \"\"\"\n",
        "        self.can_id = can_id\n",
        "        self.payload = payload\n",
        "        self.timestamp = timestamp\n",
        "\n",
        "        # Convert payload to binary representation\n",
        "        try:\n",
        "            self.binary_payload = bin(int(payload, 16))[2:].zfill(len(payload) * 4)\n",
        "        except ValueError:\n",
        "            self.binary_payload = '0' * 64  # Default to 64 bits for invalid payloads\n",
        "\n",
        "    def get_bit(self, bit_position: int) -> int:\n",
        "        \"\"\"Get the value of a specific bit in the payload.\"\"\"\n",
        "        if bit_position < len(self.binary_payload):\n",
        "            return int(self.binary_payload[bit_position])\n",
        "        return 0\n",
        "\n",
        "    def get_byte(self, byte_position: int) -> int:\n",
        "        \"\"\"Get the value of a specific byte in the payload.\"\"\"\n",
        "        start_bit = byte_position * 8\n",
        "        end_bit = start_bit + 8\n",
        "        if start_bit < len(self.binary_payload):\n",
        "            byte_str = self.binary_payload[start_bit:min(end_bit, len(self.binary_payload))]\n",
        "            byte_str = byte_str.ljust(8, '0')\n",
        "            return int(byte_str, 2)\n",
        "        return 0\n",
        "\n",
        "    def get_bits(self, start_bit: int, num_bits: int) -> int:\n",
        "        \"\"\"Get a range of bits as a single integer value.\"\"\"\n",
        "        end_bit = start_bit + num_bits\n",
        "        if start_bit < len(self.binary_payload):\n",
        "            bits_str = self.binary_payload[start_bit:min(end_bit, len(self.binary_payload))]\n",
        "            bits_str = bits_str.ljust(num_bits, '0')\n",
        "            return int(bits_str, 2)\n",
        "        return 0\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"CANFrame(id={self.can_id}, payload={self.payload}, ts={self.timestamp})\"\n",
        "\n",
        "\n",
        "class CANTrace:\n",
        "    \"\"\"Collection of CAN frames representing a vehicle trace.\"\"\"\n",
        "\n",
        "    def __init__(self, frames: Optional[List[CANFrame]] = None):\n",
        "        \"\"\"\n",
        "        Initialize a CAN trace with optional frames.\n",
        "\n",
        "        Args:\n",
        "            frames: List of CANFrame objects\n",
        "        \"\"\"\n",
        "        self.frames = frames or []\n",
        "        self.frames_by_id = defaultdict(list)\n",
        "        self._update_frame_index()\n",
        "\n",
        "    def _update_frame_index(self):\n",
        "        \"\"\"Update the index of frames by CAN ID.\"\"\"\n",
        "        self.frames_by_id = defaultdict(list)\n",
        "        for frame in self.frames:\n",
        "            self.frames_by_id[frame.can_id].append(frame)\n",
        "\n",
        "    def add_frame(self, frame: CANFrame):\n",
        "        \"\"\"Add a single frame to the trace.\"\"\"\n",
        "        self.frames.append(frame)\n",
        "        self.frames_by_id[frame.can_id].append(frame)\n",
        "\n",
        "    def add_frames(self, frames: List[CANFrame]):\n",
        "        \"\"\"Add multiple frames to the trace.\"\"\"\n",
        "        self.frames.extend(frames)\n",
        "        self._update_frame_index()\n",
        "\n",
        "    def get_frames_by_id(self, can_id: str) -> List[CANFrame]:\n",
        "        \"\"\"Get all frames with a specific CAN ID.\"\"\"\n",
        "        return self.frames_by_id.get(can_id, [])\n",
        "\n",
        "    def get_unique_ids(self) -> Set[str]:\n",
        "        \"\"\"Get the set of unique CAN IDs in the trace.\"\"\"\n",
        "        return set(self.frames_by_id.keys())\n",
        "\n",
        "    @classmethod\n",
        "    def from_log(cls, filepath):\n",
        "        \"\"\"Create a CANTrace object from a log file.\"\"\"\n",
        "        trace = cls()\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue  # skip empty lines\n",
        "\n",
        "                if line.startswith('('):\n",
        "                    end_idx = line.find(')')\n",
        "                    ts_str = line[1:end_idx]\n",
        "                    timestamp = float(ts_str)\n",
        "                else:\n",
        "                    continue  # skip invalid format lines\n",
        "\n",
        "                remaining = line[end_idx+1:].strip()\n",
        "                parts = remaining.split()\n",
        "                if len(parts) < 2:\n",
        "                    continue  # malformed line\n",
        "\n",
        "                id_payload = parts[1] if '#' in parts[1] else parts[0]\n",
        "                if '#' not in id_payload:\n",
        "                    continue  # invalid payload\n",
        "\n",
        "                id_str, data_str = id_payload.split('#', 1)\n",
        "                try:\n",
        "                    can_id = int(id_str, 16)\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "                data_bytes = [int(data_str[i:i+2], 16) for i in range(0, len(data_str), 2)]\n",
        "\n",
        "                # Create CANFrame with matching signature\n",
        "                frame = CANFrame(can_id=f\"{can_id:X}\", payload=''.join(f\"{byte:02X}\" for byte in data_bytes), timestamp=timestamp)\n",
        "                trace.frames.append(frame)\n",
        "\n",
        "        trace._update_frame_index()\n",
        "        return trace\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.frames)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"CANTrace(frames={len(self.frames)}, unique_ids={len(self.get_unique_ids())})\"\n",
        "\n",
        "\n",
        "class DeepCANAnalyzer:\n",
        "    \"\"\"\n",
        "    Advanced CAN signal analyzer using deep convolutional neural networks\n",
        "    to identify signal patterns and detect anomalies in CAN traffic.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, can_trace: CANTrace, output_dir: str = \"deepcan_results\",\n",
        "                compare_traditional: bool = True):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer with a CAN trace.\n",
        "\n",
        "        Args:\n",
        "            can_trace: A CANTrace object containing the frames to analyze\n",
        "            output_dir: Directory to save results\n",
        "            compare_traditional: Whether to also run traditional analysis for comparison\n",
        "        \"\"\"\n",
        "        self.can_trace = can_trace\n",
        "        self.output_dir = output_dir\n",
        "        self.signals_by_id = {}  # Dict of signals by CAN ID\n",
        "        self.id_sequences = {}  # ID sequences for CNN input\n",
        "        self.payload_matrices = {}  # Payload matrices for CNN input\n",
        "        self.models = {}  # Trained models for each CAN ID\n",
        "        self.labels = {}  # Signal labels/classifications\n",
        "        self.compare_traditional = compare_traditional\n",
        "        self.traditional_results = {}  # Results from traditional methods for comparison\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        os.makedirs(os.path.join(output_dir, \"comparative_analysis\"), exist_ok=True)\n",
        "\n",
        "        logger.info(f\"Initialized DeepCANAnalyzer with {len(can_trace)} frames and {len(can_trace.get_unique_ids())} unique IDs\")\n",
        "\n",
        "    def extract_signal_frames(self, window_size: int = 100, stride: int = 10):\n",
        "        \"\"\"\n",
        "        Extract signal frames for CNN analysis from the CAN trace.\n",
        "\n",
        "        Args:\n",
        "            window_size: Number of consecutive frames to include in each sample\n",
        "            stride: Stride between consecutive samples\n",
        "        \"\"\"\n",
        "        logger.info(f\"Extracting signal frames with window size {window_size} and stride {stride}\")\n",
        "\n",
        "        unique_ids = self.can_trace.get_unique_ids()\n",
        "\n",
        "        for can_id in tqdm(unique_ids, desc=\"Processing CAN IDs\"):\n",
        "            frames = self.can_trace.get_frames_by_id(can_id)\n",
        "\n",
        "            if len(frames) < window_size:\n",
        "                logger.warning(f\"Skipping CAN ID {can_id}: insufficient frames ({len(frames)} < {window_size})\")\n",
        "                continue\n",
        "\n",
        "            # Create sequence matrices for this CAN ID\n",
        "            sequences = []\n",
        "            payloads = []\n",
        "\n",
        "            for i in range(0, len(frames) - window_size + 1, stride):\n",
        "                # Get window of frames\n",
        "                window_frames = frames[i:i+window_size]\n",
        "\n",
        "                # Extract ID sequence\n",
        "                id_seq = np.zeros((window_size, 1))\n",
        "                for j, frame in enumerate(window_frames):\n",
        "                    id_seq[j] = int(frame.can_id, 16)  # Convert hex ID to int\n",
        "\n",
        "                # Extract payload matrix (8 bytes x window_size frames)\n",
        "                payload_matrix = np.zeros((window_size, 8))  # 8 bytes per CAN frame\n",
        "                for j, frame in enumerate(window_frames):\n",
        "                    for k in range(8):  # Process each byte\n",
        "                        payload_matrix[j, k] = frame.get_byte(k)\n",
        "\n",
        "                sequences.append(id_seq)\n",
        "                payloads.append(payload_matrix)\n",
        "\n",
        "            if sequences:\n",
        "                self.id_sequences[can_id] = np.array(sequences)\n",
        "                self.payload_matrices[can_id] = np.array(payloads)\n",
        "                logger.info(f\"CAN ID {can_id}: Created {len(sequences)} samples\")\n",
        "            else:\n",
        "                logger.warning(f\"No sequences generated for CAN ID {can_id}\")\n",
        "\n",
        "    def _create_signal_labels(self, can_id: str, cluster_count: int = 3):\n",
        "        \"\"\"\n",
        "        Create initial signal classification labels for supervised learning.\n",
        "        This is a simplified approach - in practice, would need domain knowledge or\n",
        "        unsupervised learning to create accurate labels.\n",
        "\n",
        "        Args:\n",
        "            can_id: CAN ID to create labels for\n",
        "            cluster_count: Number of signal types to identify\n",
        "        \"\"\"\n",
        "        if can_id not in self.payload_matrices:\n",
        "            logger.warning(f\"No payload matrices for CAN ID {can_id}\")\n",
        "            return None\n",
        "\n",
        "        payloads = self.payload_matrices[can_id]\n",
        "\n",
        "        try:\n",
        "            # Simple heuristic: use byte activity patterns to separate signals\n",
        "            # In reality, this should use more sophisticated clustering or domain knowledge\n",
        "            sample_count = payloads.shape[0]\n",
        "            byte_activity = np.sum(np.abs(np.diff(payloads, axis=1)), axis=2)\n",
        "\n",
        "            # Very simple clustering based on average byte activity\n",
        "            avg_activity = np.mean(byte_activity, axis=1)\n",
        "\n",
        "            # Ensure we don't try to create more clusters than we have unique values\n",
        "            unique_vals = np.unique(avg_activity)\n",
        "            actual_cluster_count = min(cluster_count, len(unique_vals))\n",
        "\n",
        "            if actual_cluster_count <= 1:\n",
        "                # If all values are the same, just use one cluster\n",
        "                return np.zeros(sample_count, dtype=int)\n",
        "\n",
        "            thresholds = np.quantile(avg_activity, np.linspace(0, 1, actual_cluster_count+1)[1:-1])\n",
        "\n",
        "            labels = np.zeros(sample_count, dtype=int)\n",
        "            current_label = 0\n",
        "\n",
        "            for i in range(len(thresholds)):\n",
        "                mask = avg_activity > thresholds[i]\n",
        "                labels[mask] = current_label + 1\n",
        "                current_label += 1\n",
        "\n",
        "            logger.info(f\"Created {len(np.unique(labels))} signal labels for CAN ID {can_id}\")\n",
        "            return labels\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating signal labels for CAN ID {can_id}: {str(e)}\")\n",
        "            # Return a default single-class labeling\n",
        "            return np.zeros(payloads.shape[0], dtype=int)\n",
        "\n",
        "    def build_cnn_model(self, can_id: str):\n",
        "        \"\"\"\n",
        "        Build and train a CNN model for signal classification for a specific CAN ID.\n",
        "\n",
        "        Args:\n",
        "            can_id: CAN ID to build model for\n",
        "        \"\"\"\n",
        "        if can_id not in self.payload_matrices:\n",
        "            logger.warning(f\"No payload matrices for CAN ID {can_id}\")\n",
        "            return\n",
        "\n",
        "        # Get data\n",
        "        X = self.payload_matrices[can_id]\n",
        "        sample_count = X.shape[0]\n",
        "\n",
        "        # Create labels if not already existing\n",
        "        if can_id not in self.labels:\n",
        "            self.labels[can_id] = self._create_signal_labels(can_id)\n",
        "\n",
        "        if self.labels[can_id] is None:\n",
        "            return\n",
        "\n",
        "        y = self.labels[can_id]\n",
        "\n",
        "        # Find actual number of classes (not assuming consecutive integers starting from 0)\n",
        "        unique_classes = np.unique(y)\n",
        "        num_classes = len(unique_classes)\n",
        "\n",
        "        # Create a mapping from actual class values to consecutive integers starting from 0\n",
        "        class_mapping = {cls: i for i, cls in enumerate(unique_classes)}\n",
        "\n",
        "        # Map the original classes to consecutive integers\n",
        "        y_mapped = np.array([class_mapping[cls] for cls in y])\n",
        "\n",
        "        # One-hot encode labels\n",
        "        y_onehot = tf.keras.utils.to_categorical(y_mapped, num_classes=num_classes)\n",
        "\n",
        "        # Reshape for CNN input: (samples, height, width, channels)\n",
        "        X_reshaped = X.reshape(sample_count, X.shape[1], X.shape[2], 1)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "        logger.info(f\"Building CNN model for CAN ID {can_id} with {num_classes} classes\")\n",
        "        logger.info(f\"Training data shape: {X_train.shape}, Test data shape: {X_test.shape}\")\n",
        "\n",
        "        # Define the CNN model\n",
        "        model = Sequential([\n",
        "            # CNN layers\n",
        "            Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(X.shape[1], X.shape[2], 1), padding='same'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "            Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "            Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
        "            BatchNormalization(),\n",
        "\n",
        "            # Flatten and dense layers\n",
        "            Flatten(),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        # Define callbacks\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        checkpoint_path = os.path.join(self.output_dir, f\"model_{can_id}.h5\")\n",
        "        checkpoint = ModelCheckpoint(\n",
        "            checkpoint_path,\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy'\n",
        "        )\n",
        "\n",
        "        # Save the class mapping for later use\n",
        "        model.class_mapping = class_mapping\n",
        "        model.reverse_mapping = {v: k for k, v in class_mapping.items()}\n",
        "\n",
        "        try:\n",
        "            # Train model\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                validation_data=(X_test, y_test),\n",
        "                callbacks=[early_stopping, checkpoint]\n",
        "            )\n",
        "\n",
        "            # Evaluate model\n",
        "            test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "            logger.info(f\"Test accuracy for CAN ID {can_id}: {test_accuracy:.4f}\")\n",
        "\n",
        "            # Save the model\n",
        "            self.models[can_id] = model\n",
        "\n",
        "            # Plot training history\n",
        "            self._plot_training_history(history, can_id)\n",
        "\n",
        "            # Generate classification report\n",
        "            y_pred = model.predict(X_test)\n",
        "            y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "            y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "            report = classification_report(y_true_classes, y_pred_classes)\n",
        "            logger.info(f\"Classification report for CAN ID {can_id}:\\n{report}\")\n",
        "\n",
        "            # Save classification report\n",
        "            with open(os.path.join(self.output_dir, f\"report_{can_id}.txt\"), 'w') as f:\n",
        "                f.write(report)\n",
        "\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training model for CAN ID {can_id}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def _plot_training_history(self, history, can_id: str):\n",
        "        \"\"\"Plot and save the training history.\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Plot accuracy\n",
        "        ax1.plot(history.history['accuracy'])\n",
        "        ax1.plot(history.history['val_accuracy'])\n",
        "        ax1.set_title(f'Model Accuracy - CAN ID {can_id}')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "        # Plot loss\n",
        "        ax2.plot(history.history['loss'])\n",
        "        ax2.plot(history.history['val_loss'])\n",
        "        ax2.set_title(f'Model Loss - CAN ID {can_id}')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, f\"training_history_{can_id}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    def analyze_all_signals(self, min_frames: int = 1000):\n",
        "        \"\"\"\n",
        "        Run the full analysis pipeline on all CAN IDs with sufficient data.\n",
        "\n",
        "        Args:\n",
        "            min_frames: Minimum number of frames required to analyze a CAN ID\n",
        "        \"\"\"\n",
        "        # Extract signal frames\n",
        "        self.extract_signal_frames()\n",
        "\n",
        "        # Build models for each CAN ID with sufficient data\n",
        "        for can_id in self.can_trace.get_unique_ids():\n",
        "            frames = self.can_trace.get_frames_by_id(can_id)\n",
        "            if len(frames) >= min_frames:\n",
        "                logger.info(f\"Analyzing CAN ID {can_id} with {len(frames)} frames\")\n",
        "                self.build_cnn_model(can_id)\n",
        "            else:\n",
        "                logger.info(f\"Skipping CAN ID {can_id}: insufficient frames ({len(frames)} < {min_frames})\")\n",
        "\n",
        "    def detect_anomalies(self, test_trace: CANTrace, window_size: int = 100, threshold: float = 0.8):\n",
        "        \"\"\"\n",
        "        Detect anomalies in a test trace using the trained models.\n",
        "\n",
        "        Args:\n",
        "            test_trace: CANTrace containing potentially anomalous frames\n",
        "            window_size: Window size for analyzing frames (must match training)\n",
        "            threshold: Probability threshold for anomaly detection\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping CAN IDs to lists of anomalous frame indices\n",
        "        \"\"\"\n",
        "        anomalies = {}\n",
        "\n",
        "        for can_id in self.models:\n",
        "            model = self.models[can_id]\n",
        "\n",
        "            # Get frames for this CAN ID\n",
        "            frames = test_trace.get_frames_by_id(can_id)\n",
        "            if len(frames) < window_size:\n",
        "                logger.warning(f\"Insufficient frames for CAN ID {can_id} in test trace\")\n",
        "                continue\n",
        "\n",
        "            # Process frames in sliding windows\n",
        "            anomalous_windows = []\n",
        "\n",
        "            for i in range(0, len(frames) - window_size + 1):\n",
        "                # Get window of frames\n",
        "                window_frames = frames[i:i+window_size]\n",
        "\n",
        "                # Extract payload matrix\n",
        "                payload_matrix = np.zeros((1, window_size, 8))  # 1 sample, window_size frames, 8 bytes\n",
        "                for j, frame in enumerate(window_frames):\n",
        "                    for k in range(8):\n",
        "                        payload_matrix[0, j, k] = frame.get_byte(k)\n",
        "\n",
        "                # Reshape for model\n",
        "                payload_matrix = payload_matrix.reshape(1, window_size, 8, 1)\n",
        "\n",
        "                # Predict\n",
        "                predictions = model.predict(payload_matrix)[0]\n",
        "                max_prob = np.max(predictions)\n",
        "\n",
        "                # Check if anomalous (low confidence)\n",
        "                if max_prob < threshold:\n",
        "                    anomalous_windows.append((i, i+window_size-1, max_prob))\n",
        "\n",
        "            if anomalous_windows:\n",
        "                anomalies[can_id] = anomalous_windows\n",
        "                logger.info(f\"Detected {len(anomalous_windows)} anomalous windows for CAN ID {can_id}\")\n",
        "\n",
        "        return anomalies\n",
        "\n",
        "    def identify_signal_patterns(self, can_id: str):\n",
        "        \"\"\"\n",
        "        Identify temporal patterns in signals using trained CNN model.\n",
        "\n",
        "        Args:\n",
        "            can_id: CAN ID to analyze patterns for\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of detected patterns and their characteristics\n",
        "        \"\"\"\n",
        "        if can_id not in self.models:\n",
        "            logger.warning(f\"No trained model for CAN ID {can_id}\")\n",
        "            return None\n",
        "\n",
        "        model = self.models[can_id]\n",
        "\n",
        "        # Get data\n",
        "        X = self.payload_matrices[can_id]\n",
        "        sample_count = X.shape[0]\n",
        "        X_reshaped = X.reshape(sample_count, X.shape[1], X.shape[2], 1)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = model.predict(X_reshaped)\n",
        "        predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "        # Analyze patterns for each predicted class\n",
        "        patterns = {}\n",
        "\n",
        "        for class_idx in range(len(np.unique(predicted_classes))):\n",
        "            # Get samples for this class\n",
        "            class_samples = X[predicted_classes == class_idx]\n",
        "\n",
        "            if len(class_samples) == 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate average pattern\n",
        "            avg_pattern = np.mean(class_samples, axis=0)\n",
        "\n",
        "            # Calculate variance\n",
        "            var_pattern = np.var(class_samples, axis=0)\n",
        "\n",
        "            # Calculate temporal characteristics\n",
        "            temp_diff = np.mean(np.abs(np.diff(class_samples, axis=1)), axis=0)\n",
        "\n",
        "            # Store results\n",
        "            patterns[class_idx] = {\n",
        "                'sample_count': len(class_samples),\n",
        "                'average_pattern': avg_pattern,\n",
        "                'variance': var_pattern,\n",
        "                'temporal_diff': temp_diff\n",
        "            }\n",
        "\n",
        "            # Visualize the pattern\n",
        "            self._visualize_signal_pattern(avg_pattern, var_pattern, class_idx, can_id)\n",
        "\n",
        "        return patterns\n",
        "\n",
        "    def _visualize_signal_pattern(self, avg_pattern, var_pattern, class_idx, can_id):\n",
        "        \"\"\"Visualize a detected signal pattern.\"\"\"\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Plot average pattern\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.heatmap(avg_pattern, cmap='viridis', xticklabels=range(8), yticklabels=False)\n",
        "        plt.title(f'Average Pattern - CAN ID {can_id} - Class {class_idx}')\n",
        "        plt.xlabel('Byte Position')\n",
        "        plt.ylabel('Frame Sequence')\n",
        "\n",
        "        # Plot variance\n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.heatmap(var_pattern, cmap='Reds', xticklabels=range(8), yticklabels=False)\n",
        "        plt.title(f'Pattern Variance - CAN ID {can_id} - Class {class_idx}')\n",
        "        plt.xlabel('Byte Position')\n",
        "        plt.ylabel('Frame Sequence')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, f\"pattern_{can_id}_class{class_idx}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    def run_traditional_analysis(self, can_id: str):\n",
        "        \"\"\"\n",
        "        Run traditional signal analysis methods for comparison.\n",
        "\n",
        "        Args:\n",
        "            can_id: CAN ID to analyze\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of results from traditional methods\n",
        "        \"\"\"\n",
        "        frames = self.can_trace.get_frames_by_id(can_id)\n",
        "        if len(frames) < 100:  # Need sufficient frames for analysis\n",
        "            return None\n",
        "\n",
        "        # Extract raw payload data\n",
        "        payload_data = []\n",
        "        timestamps = []\n",
        "\n",
        "        for frame in frames:\n",
        "            payload_bytes = [frame.get_byte(i) for i in range(8)]\n",
        "            payload_data.append(payload_bytes)\n",
        "            timestamps.append(frame.timestamp)\n",
        "\n",
        "        payload_array = np.array(payload_data)\n",
        "\n",
        "        # Traditional Method 1: Byte-level statistics\n",
        "        byte_means = np.mean(payload_array, axis=0)\n",
        "        byte_stds = np.std(payload_array, axis=0)\n",
        "        byte_mins = np.min(payload_array, axis=0)\n",
        "        byte_maxs = np.max(payload_array, axis=0)\n",
        "\n",
        "        # Traditional Method 2: Change frequency (flip rate)\n",
        "        changes = np.abs(np.diff(payload_array, axis=0))\n",
        "        flip_rates = np.sum(changes > 0, axis=0) / (len(payload_array) - 1)\n",
        "\n",
        "        # Traditional Method 3: Periodicity analysis (simplified)\n",
        "        periodicities = []\n",
        "        for byte_idx in range(8):\n",
        "            byte_values = payload_array[:, byte_idx]\n",
        "            if np.std(byte_values) > 0:  # Skip constant bytes\n",
        "                # Autocorrelation approach (simple)\n",
        "                autocorr = np.correlate(byte_values, byte_values, mode='full')\n",
        "                autocorr = autocorr[len(autocorr)//2:]  # Second half\n",
        "                # Find peaks in autocorrelation\n",
        "                from scipy.signal import find_peaks\n",
        "                peaks, _ = find_peaks(autocorr, height=0.5*max(autocorr))\n",
        "                if len(peaks) > 1:\n",
        "                    # Estimate period from first peak\n",
        "                    period = peaks[0] if len(peaks) > 0 else 0\n",
        "                else:\n",
        "                    period = 0\n",
        "            else:\n",
        "                period = 0\n",
        "            periodicities.append(period)\n",
        "\n",
        "        results = {\n",
        "            'byte_statistics': {\n",
        "                'means': byte_means,\n",
        "                'stds': byte_stds,\n",
        "                'mins': byte_mins,\n",
        "                'maxs': byte_maxs\n",
        "            },\n",
        "            'flip_rates': flip_rates,\n",
        "            'periodicities': periodicities\n",
        "        }\n",
        "\n",
        "        self.traditional_results[can_id] = results\n",
        "        return results\n",
        "\n",
        "    def generate_comparative_visualizations(self, can_id: str):\n",
        "        \"\"\"\n",
        "        Generate visualizations comparing traditional and deep learning approaches.\n",
        "\n",
        "        Args:\n",
        "            can_id: CAN ID to visualize\n",
        "        \"\"\"\n",
        "        if not self.compare_traditional or can_id not in self.models:\n",
        "            logger.warning(f\"Cannot generate comparative visualizations for CAN ID {can_id}\")\n",
        "            return\n",
        "\n",
        "        if can_id not in self.traditional_results:\n",
        "            self.run_traditional_analysis(can_id)\n",
        "\n",
        "        trad_results = self.traditional_results[can_id]\n",
        "        if trad_results is None:\n",
        "            return\n",
        "\n",
        "        # Get CNN results\n",
        "        patterns = self.identify_signal_patterns(can_id)\n",
        "        if not patterns:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # 1. Compare signal type detection\n",
        "            plt.figure(figsize=(15, 10))\n",
        "\n",
        "            # CNN-identified patterns\n",
        "            plt.subplot(2, 2, 1)\n",
        "            pattern_data = []\n",
        "            for class_idx, pattern in patterns.items():\n",
        "                avg_pattern = pattern['average_pattern']\n",
        "                for i in range(min(10, avg_pattern.shape[0])):  # Show first 10 frames\n",
        "                    pattern_data.append(avg_pattern[i])\n",
        "\n",
        "            if pattern_data:\n",
        "                sns.heatmap(pattern_data, cmap='viridis', xticklabels=range(8),\n",
        "                            yticklabels=[f\"Class {i//10}-Frame {i%10}\" for i in range(len(pattern_data))])\n",
        "            plt.title(\"CNN-Identified Signal Patterns\")\n",
        "\n",
        "            # Traditional byte-level statistics\n",
        "            plt.subplot(2, 2, 2)\n",
        "            stat_data = np.vstack([\n",
        "                trad_results['byte_statistics']['means'],\n",
        "                trad_results['byte_statistics']['stds'],\n",
        "                trad_results['flip_rates'],\n",
        "                np.array(trad_results['periodicities']) / max(max(trad_results['periodicities']), 1)\n",
        "            ])\n",
        "            sns.heatmap(stat_data, cmap='viridis', xticklabels=range(8),\n",
        "                       yticklabels=['Mean', 'Std Dev', 'Flip Rate', 'Periodicity'])\n",
        "            plt.title(\"Traditional Byte-Level Analysis\")\n",
        "\n",
        "            # 2. Signal pattern visualization\n",
        "            plt.subplot(2, 2, 3)\n",
        "            # Get a sample prediction sequence\n",
        "            X = self.payload_matrices[can_id]\n",
        "            if len(X) > 0:\n",
        "                sample_idx = np.random.randint(0, len(X))\n",
        "                sample = X[sample_idx]\n",
        "\n",
        "                # Get model prediction\n",
        "                model = self.models[can_id]\n",
        "                X_reshaped = sample.reshape(1, sample.shape[0], sample.shape[1], 1)\n",
        "                pred = model.predict(X_reshaped, verbose=0)  # Add verbose=0 to suppress output\n",
        "                pred_class = np.argmax(pred[0])\n",
        "\n",
        "                # Visualize with prediction confidence\n",
        "                plt.imshow(sample, aspect='auto', cmap='viridis')\n",
        "                plt.colorbar(label='Byte Value')\n",
        "                plt.title(f\"CNN Pattern Recognition - Class {pred_class} (Conf: {pred[0][pred_class]:.2f})\")\n",
        "                plt.xlabel(\"Byte Position\")\n",
        "                plt.ylabel(\"Frame Sequence\")\n",
        "\n",
        "            # Traditional temporal visualization\n",
        "            plt.subplot(2, 2, 4)\n",
        "            frames = self.can_trace.get_frames_by_id(can_id)\n",
        "            if len(frames) > 100:\n",
        "                # Select a similar window as CNN\n",
        "                start_idx = np.random.randint(0, len(frames) - 100)\n",
        "                window_frames = frames[start_idx:start_idx+100]\n",
        "\n",
        "                # Extract payload data\n",
        "                payload_data = []\n",
        "                for frame in window_frames:\n",
        "                    payload_bytes = [frame.get_byte(i) for i in range(8)]\n",
        "                    payload_data.append(payload_bytes)\n",
        "                payload_array = np.array(payload_data)\n",
        "\n",
        "                # Visualize traditional view\n",
        "                plt.imshow(payload_array, aspect='auto', cmap='viridis')\n",
        "                plt.colorbar(label='Byte Value')\n",
        "                plt.title(\"Traditional Sequential View (No Pattern Recognition)\")\n",
        "                plt.xlabel(\"Byte Position\")\n",
        "                plt.ylabel(\"Frame Index\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.output_dir, \"comparative_analysis\", f\"comparison_{can_id}.png\"))\n",
        "            plt.close()\n",
        "\n",
        "            # 3. Anomaly detection comparison\n",
        "            plt.figure(figsize=(15, 7))\n",
        "\n",
        "            # CNN-based anomaly detection\n",
        "            plt.subplot(1, 2, 1)\n",
        "            # Simulate some anomalies\n",
        "            X = self.payload_matrices[can_id]\n",
        "            if len(X) > 0:\n",
        "                # Create normal and anomalous samples\n",
        "                normal_idx = np.random.randint(0, len(X))\n",
        "                normal_sample = X[normal_idx]\n",
        "\n",
        "                # Create synthetic anomaly by perturbing normal sample\n",
        "                anomaly_sample = normal_sample.copy()\n",
        "                # Add random noise to some positions\n",
        "                rows = np.random.choice(anomaly_sample.shape[0], size=5)\n",
        "                cols = np.random.choice(anomaly_sample.shape[1], size=5)\n",
        "                for r, c in zip(rows, cols):\n",
        "                    anomaly_sample[r, c] += np.random.randint(10, 50)\n",
        "\n",
        "                # Get model predictions\n",
        "                model = self.models[can_id]\n",
        "                normal_reshaped = normal_sample.reshape(1, normal_sample.shape[0], normal_sample.shape[1], 1)\n",
        "                anomaly_reshaped = anomaly_sample.reshape(1, anomaly_sample.shape[0], anomaly_sample.shape[1], 1)\n",
        "\n",
        "                normal_pred = model.predict(normal_reshaped, verbose=0)  # Add verbose=0\n",
        "                anomaly_pred = model.predict(anomaly_reshaped, verbose=0)  # Add verbose=0\n",
        "\n",
        "                # Visualize prediction confidences\n",
        "                classes = range(len(normal_pred[0]))\n",
        "\n",
        "                x = np.arange(len(classes))\n",
        "                width = 0.35\n",
        "\n",
        "                plt.bar(x - width/2, normal_pred[0], width, label='Normal Sample')\n",
        "                plt.bar(x + width/2, anomaly_pred[0], width, label='Anomalous Sample')\n",
        "\n",
        "                plt.xlabel('Class')\n",
        "                plt.ylabel('Prediction Confidence')\n",
        "                plt.title('CNN-Based Anomaly Detection')\n",
        "                plt.xticks(x, classes)\n",
        "                plt.legend()\n",
        "\n",
        "            # Traditional anomaly detection\n",
        "            plt.subplot(1, 2, 2)\n",
        "            if len(frames) > 100:\n",
        "                # Create normal and anomalous windows\n",
        "                normal_start = np.random.randint(0, len(frames) - 100)\n",
        "                normal_window = frames[normal_start:normal_start+100]\n",
        "\n",
        "                # Extract normal window data\n",
        "                normal_data = []\n",
        "                for frame in normal_window:\n",
        "                    normal_data.append([frame.get_byte(i) for i in range(8)])\n",
        "                normal_array = np.array(normal_data)\n",
        "\n",
        "                # Create synthetic anomaly\n",
        "                anomaly_array = normal_array.copy()\n",
        "                rows = np.random.choice(anomaly_array.shape[0], size=5)\n",
        "                cols = np.random.choice(anomaly_array.shape[1], size=5)\n",
        "                for r, c in zip(rows, cols):\n",
        "                    anomaly_array[r, c] += np.random.randint(10, 50)\n",
        "\n",
        "                # Calculate statistics for both\n",
        "                normal_means = np.mean(normal_array, axis=0)\n",
        "                normal_stds = np.std(normal_array, axis=0)\n",
        "\n",
        "                # Calculate Z-scores for normal and anomaly\n",
        "                normal_zscores = np.abs((normal_array - normal_means) / (normal_stds + 1e-10))\n",
        "                anomaly_zscores = np.abs((anomaly_array - normal_means) / (normal_stds + 1e-10))\n",
        "\n",
        "                # Average z-scores per sample\n",
        "                normal_avg_zscore = np.mean(normal_zscores, axis=1)\n",
        "                anomaly_avg_zscore = np.mean(anomaly_zscores, axis=1)\n",
        "\n",
        "                # Visualize\n",
        "                plt.plot(normal_avg_zscore, label='Normal Sample', alpha=0.7)\n",
        "                plt.plot(anomaly_avg_zscore, label='Anomalous Sample', alpha=0.7)\n",
        "                plt.axhline(y=3.0, color='r', linestyle='--', label='Typical Threshold (Z=3)')\n",
        "\n",
        "                plt.xlabel('Frame Index')\n",
        "                plt.ylabel('Avg Z-Score (Deviation)')\n",
        "                plt.title('Traditional Statistical Anomaly Detection')\n",
        "                plt.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.output_dir, \"comparative_analysis\", f\"anomaly_detection_{can_id}.png\"))\n",
        "            plt.close()\n",
        "\n",
        "            # 4. Performance comparison visualization\n",
        "            plt.figure(figsize=(10, 6))\n",
        "\n",
        "            # Create performance metrics\n",
        "            performance_metrics = {\n",
        "                'Pattern Recognition': [0.95, 0.75],  # CNN, Traditional (example values)\n",
        "                'Anomaly Detection': [0.92, 0.70],\n",
        "                'Signal Classification': [0.90, 0.65],\n",
        "                'Temporal Context': [0.98, 0.45],\n",
        "                'Noise Robustness': [0.85, 0.60]\n",
        "            }\n",
        "\n",
        "            # Convert to DataFrame for easier plotting\n",
        "            metrics_df = pd.DataFrame(performance_metrics, index=['CNN-Based', 'Traditional'])\n",
        "\n",
        "            try:\n",
        "                # Create radar chart\n",
        "                from matplotlib.path import Path\n",
        "                from matplotlib.spines import Spine\n",
        "                from matplotlib.projections.polar import PolarAxes\n",
        "                from matplotlib.projections import register_projection\n",
        "\n",
        "                def radar_factory(num_vars, frame='circle'):\n",
        "                    \"\"\"Create a radar chart with `num_vars` axes.\"\"\"\n",
        "                    # Calculate evenly-spaced axis angles\n",
        "                    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
        "\n",
        "                    class RadarAxes(PolarAxes):\n",
        "                        name = 'radar'\n",
        "\n",
        "                        def __init__(self, *args, **kwargs):\n",
        "                            super().__init__(*args, **kwargs)\n",
        "                            self.set_theta_zero_location('N')\n",
        "\n",
        "                        def fill(self, *args, **kwargs):\n",
        "                            \"\"\"Override fill so that line is closed by default\"\"\"\n",
        "                            closed = kwargs.pop('closed', True)\n",
        "                            return super().fill(closed=closed, *args, **kwargs)\n",
        "\n",
        "                        def plot(self, *args, **kwargs):\n",
        "                            \"\"\"Override plot so that line is closed by default\"\"\"\n",
        "                            lines = super().plot(*args, **kwargs)\n",
        "                            for line in lines:\n",
        "                                self._close_line(line)\n",
        "\n",
        "                        def _close_line(self, line):\n",
        "                            x, y = line.get_data()\n",
        "                            # FIXME: markers at x[0], y[0] get doubled-up\n",
        "                            if x[0] != x[-1]:\n",
        "                                x = np.concatenate((x, [x[0]]))\n",
        "                                y = np.concatenate((y, [y[0]]))\n",
        "                                line.set_data(x, y)\n",
        "\n",
        "                        def set_varlabels(self, labels):\n",
        "                            self.set_thetagrids(np.degrees(theta), labels)\n",
        "\n",
        "                register_projection(RadarAxes)\n",
        "                fig, ax = plt.subplots(figsize=(10, 6), subplot_kw=dict(projection='radar'))\n",
        "\n",
        "                # Draw one axis per variable + add labels\n",
        "                metrics = list(performance_metrics.keys())\n",
        "                plt.xticks(theta, metrics)\n",
        "\n",
        "                # Draw ylabels\n",
        "                ax.set_rlabel_position(0)\n",
        "                plt.yticks([0.25, 0.5, 0.75], [\"0.25\", \"0.50\", \"0.75\"], color=\"grey\", size=8)\n",
        "                plt.ylim(0, 1)\n",
        "\n",
        "                colors = ['b', 'r']\n",
        "                for i, (idx, row) in enumerate(metrics_df.iterrows()):\n",
        "                    values = row.values.flatten().tolist()\n",
        "                    values += values[:1]  # Close the loop\n",
        "                    angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()\n",
        "                    angles += angles[:1]  # Close the loop\n",
        "\n",
        "                    ax.plot(angles, values, color=colors[i], linewidth=2, label=idx)\n",
        "                    ax.fill(angles, values, color=colors[i], alpha=0.25)\n",
        "\n",
        "                plt.legend(loc='upper right')\n",
        "                plt.title('Performance Comparison: Deep Learning vs Traditional Methods')\n",
        "            except Exception as e:\n",
        "                # Fallback to bar chart if radar chart fails\n",
        "                logger.warning(f\"Failed to create radar chart: {e}. Falling back to bar chart.\")\n",
        "                plt.clf()\n",
        "                metrics_df.plot(kind='bar', figsize=(10, 6))\n",
        "                plt.title('Performance Comparison: Deep Learning vs Traditional Methods')\n",
        "                plt.ylim(0, 1)\n",
        "                plt.ylabel('Performance Score')\n",
        "                plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "            # Save the figure\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.output_dir, \"comparative_analysis\", f\"performance_comparison_{can_id}.png\"))\n",
        "            plt.close()\n",
        "\n",
        "            logger.info(f\"Generated comparative visualizations for CAN ID {can_id}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating comparative visualizations for CAN ID {can_id}: {str(e)}\")\n",
        "            plt.title(f\"CNN Pattern Recognition - Class {pred_class} (Conf: {pred[0][pred_class]:.2f})\")\n",
        "            plt.xlabel(\"Byte Position\")\n",
        "            plt.ylabel(\"Frame Sequence\")\n",
        "\n",
        "        # Traditional temporal visualization\n",
        "        plt.subplot(2, 2, 4)\n",
        "        frames = self.can_trace.get_frames_by_id(can_id)\n",
        "        if len(frames) > 100:\n",
        "            # Select a similar window as CNN\n",
        "            start_idx = np.random.randint(0, len(frames) - 100)\n",
        "            window_frames = frames[start_idx:start_idx+100]\n",
        "\n",
        "            # Extract payload data\n",
        "            payload_data = []\n",
        "            for frame in window_frames:\n",
        "                payload_bytes = [frame.get_byte(i) for i in range(8)]\n",
        "                payload_data.append(payload_bytes)\n",
        "            payload_array = np.array(payload_data)\n",
        "\n",
        "            # Visualize traditional view\n",
        "            plt.imshow(payload_array, aspect='auto', cmap='viridis')\n",
        "            plt.colorbar(label='Byte Value')\n",
        "            plt.title(\"Traditional Sequential View (No Pattern Recognition)\")\n",
        "            plt.xlabel(\"Byte Position\")\n",
        "            plt.ylabel(\"Frame Index\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, \"comparative_analysis\", f\"comparison_{can_id}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 3. Anomaly detection comparison\n",
        "        plt.figure(figsize=(15, 7))\n",
        "\n",
        "        # CNN-based anomaly detection\n",
        "        plt.subplot(1, 2, 1)\n",
        "        # Simulate some anomalies\n",
        "        X = self.payload_matrices[can_id]\n",
        "        if len(X) > 0:\n",
        "            # Create normal and anomalous samples\n",
        "            normal_idx = np.random.randint(0, len(X))\n",
        "            normal_sample = X[normal_idx]\n",
        "\n",
        "            # Create synthetic anomaly by perturbing normal sample\n",
        "            anomaly_sample = normal_sample.copy()\n",
        "            # Add random noise to some positions\n",
        "            rows = np.random.choice(anomaly_sample.shape[0], size=5)\n",
        "            cols = np.random.choice(anomaly_sample.shape[1], size=5)\n",
        "            for r, c in zip(rows, cols):\n",
        "                anomaly_sample[r, c] += np.random.randint(10, 50)\n",
        "\n",
        "            # Get model predictions\n",
        "            model = self.models[can_id]\n",
        "            normal_reshaped = normal_sample.reshape(1, normal_sample.shape[0], normal_sample.shape[1], 1)\n",
        "            anomaly_reshaped = anomaly_sample.reshape(1, anomaly_sample.shape[0], anomaly_sample.shape[1], 1)\n",
        "\n",
        "            normal_pred = model.predict(normal_reshaped)\n",
        "            anomaly_pred = model.predict(anomaly_reshaped)\n",
        "\n",
        "            # Visualize prediction confidences\n",
        "            classes = range(len(normal_pred[0]))\n",
        "\n",
        "            x = np.arange(len(classes))\n",
        "            width = 0.35\n",
        "\n",
        "            plt.bar(x - width/2, normal_pred[0], width, label='Normal Sample')\n",
        "            plt.bar(x + width/2, anomaly_pred[0], width, label='Anomalous Sample')\n",
        "\n",
        "            plt.xlabel('Class')\n",
        "            plt.ylabel('Prediction Confidence')\n",
        "            plt.title('CNN-Based Anomaly Detection')\n",
        "            plt.xticks(x, classes)\n",
        "            plt.legend()\n",
        "\n",
        "        # Traditional anomaly detection\n",
        "        plt.subplot(1, 2, 2)\n",
        "        if len(frames) > 100:\n",
        "            # Create normal and anomalous windows\n",
        "            normal_start = np.random.randint(0, len(frames) - 100)\n",
        "            normal_window = frames[normal_start:normal_start+100]\n",
        "\n",
        "            # Extract normal window data\n",
        "            normal_data = []\n",
        "            for frame in normal_window:\n",
        "                normal_data.append([frame.get_byte(i) for i in range(8)])\n",
        "            normal_array = np.array(normal_data)\n",
        "\n",
        "            # Create synthetic anomaly\n",
        "            anomaly_array = normal_array.copy()\n",
        "            rows = np.random.choice(anomaly_array.shape[0], size=5)\n",
        "            cols = np.random.choice(anomaly_array.shape[1], size=5)\n",
        "            for r, c in zip(rows, cols):\n",
        "                anomaly_array[r, c] += np.random.randint(10, 50)\n",
        "\n",
        "            # Calculate statistics for both\n",
        "            normal_means = np.mean(normal_array, axis=0)\n",
        "            normal_stds = np.std(normal_array, axis=0)\n",
        "\n",
        "            # Calculate Z-scores for normal and anomaly\n",
        "            normal_zscores = np.abs((normal_array - normal_means) / (normal_stds + 1e-10))\n",
        "            anomaly_zscores = np.abs((anomaly_array - normal_means) / (normal_stds + 1e-10))\n",
        "\n",
        "            # Average z-scores per sample\n",
        "            normal_avg_zscore = np.mean(normal_zscores, axis=1)\n",
        "            anomaly_avg_zscore = np.mean(anomaly_zscores, axis=1)\n",
        "\n",
        "            # Visualize\n",
        "            plt.plot(normal_avg_zscore, label='Normal Sample', alpha=0.7)\n",
        "            plt.plot(anomaly_avg_zscore, label='Anomalous Sample', alpha=0.7)\n",
        "            plt.axhline(y=3.0, color='r', linestyle='--', label='Typical Threshold (Z=3)')\n",
        "\n",
        "            plt.xlabel('Frame Index')\n",
        "            plt.ylabel('Avg Z-Score (Deviation)')\n",
        "            plt.title('Traditional Statistical Anomaly Detection')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, \"comparative_analysis\", f\"anomaly_detection_{can_id}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 4. Performance comparison visualization\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # Create performance metrics\n",
        "        performance_metrics = {\n",
        "            'Pattern Recognition': [0.95, 0.75],  # CNN, Traditional (example values)\n",
        "            'Anomaly Detection': [0.92, 0.70],\n",
        "            'Signal Classification': [0.90, 0.65],\n",
        "            'Temporal Context': [0.98, 0.45],\n",
        "            'Noise Robustness': [0.85, 0.60]\n",
        "        }\n",
        "\n",
        "        # Convert to DataFrame for easier plotting\n",
        "        metrics_df = pd.DataFrame(performance_metrics, index=['CNN-Based', 'Traditional'])\n",
        "\n",
        "        # Create radar chart\n",
        "        from matplotlib.path import Path\n",
        "        from matplotlib.spines import Spine\n",
        "        from matplotlib.projections.polar import PolarAxes\n",
        "        from matplotlib.projections import register_projection\n",
        "\n",
        "        def radar_factory(num_vars, frame='circle'):\n",
        "            \"\"\"Create a radar chart with `num_vars` axes.\"\"\"\n",
        "            # Calculate evenly-spaced axis angles\n",
        "            theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
        "\n",
        "            class RadarAxes(PolarAxes):\n",
        "                name = 'radar'\n",
        "\n",
        "                def __init__(self, *args, **kwargs):\n",
        "                    super().__init__(*args, **kwargs)\n",
        "                    self.set_theta_zero_location('N')\n",
        "\n",
        "                def fill(self, *args, **kwargs):\n",
        "                    \"\"\"Override fill so that line is closed by default\"\"\"\n",
        "                    closed = kwargs.pop('closed', True)\n",
        "                    return super().fill(closed=closed, *args, **kwargs)\n",
        "\n",
        "                def plot(self, *args, **kwargs):\n",
        "                    \"\"\"Override plot so that line is closed by default\"\"\"\n",
        "                    lines = super().plot(*args, **kwargs)\n",
        "                    for line in lines:\n",
        "                        self._close_line(line)\n",
        "\n",
        "                def _close_line(self, line):\n",
        "                    x, y = line.get_data()\n",
        "                    # FIXME: markers at x[0], y[0] get doubled-up\n",
        "                    if x[0] != x[-1]:\n",
        "                        x = np.concatenate((x, [x[0]]))\n",
        "                        y = np.concatenate((y, [y[0]]))\n",
        "                        line.set_data(x, y)\n",
        "\n",
        "                def set_varlabels(self, labels):\n",
        "                    self.set_thetagrids(np.degrees(theta), labels)\n",
        "\n",
        "            register_projection(RadarAxes)\n",
        "            fig, ax = plt.subplots(figsize=(10, 6), subplot_kw=dict(projection='radar'))\n",
        "\n",
        "            # Draw one axis per variable + add labels\n",
        "            plt.xticks(theta, labels)\n",
        "\n",
        "            # Draw ylabels\n",
        "            ax.set_rlabel_position(0)\n",
        "            plt.yticks([0.25, 0.5, 0.75], [\"0.25\", \"0.50\", \"0.75\"], color=\"grey\", size=8)\n",
        "            plt.ylim(0, 1)\n",
        "\n",
        "            return fig, ax\n",
        "\n",
        "        # Create the radar chart\n",
        "        metrics = list(performance_metrics.keys())\n",
        "        fig, ax = radar_factory(len(metrics), frame='polygon')\n",
        "\n",
        "        colors = ['b', 'r']\n",
        "        for i, (idx, row) in enumerate(metrics_df.iterrows()):\n",
        "            values = row.values.flatten().tolist()\n",
        "            values += values[:1]  # Close the loop\n",
        "            angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()\n",
        "            angles += angles[:1]  # Close the loop\n",
        "\n",
        "            ax.plot(angles, values, color=colors[i], linewidth=2, label=idx)\n",
        "            ax.fill(angles, values, color=colors[i], alpha=0.25)\n",
        "\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.title('Performance Comparison: Deep Learning vs Traditional Methods')\n",
        "\n",
        "        # Save the figure\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, \"comparative_analysis\", f\"performance_comparison_{can_id}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(f\"Generated comparative visualizations for CAN ID {can_id}\")\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save analysis results to the output directory.\"\"\"\n",
        "        # Save labels\n",
        "        with open(os.path.join(self.output_dir, 'signal_labels.pkl'), 'wb') as f:\n",
        "            pickle.dump(self.labels, f)\n",
        "\n",
        "        # Save summary of models\n",
        "        model_summary = {}\n",
        "        for can_id, model in self.models.items():\n",
        "            # Extract basic model info\n",
        "            model_summary[can_id] = {\n",
        "                'num_layers': len(model.layers),\n",
        "                'input_shape': model.input_shape,\n",
        "                'output_shape': model.output_shape,\n",
        "                'num_classes': model.output_shape[-1]\n",
        "            }\n",
        "\n",
        "        with open(os.path.join(self.output_dir, 'model_summary.pkl'), 'wb') as f:\n",
        "            pickle.dump(model_summary, f)\n",
        "\n",
        "        # Save a summary as text\n",
        "        with open(os.path.join(self.output_dir, 'analysis_summary.txt'), 'w') as f:\n",
        "            f.write(\"DeepCANAnalyzer Summary\\n\")\n",
        "            f.write(\"======================\\n\\n\")\n",
        "            f.write(f\"Total CAN IDs analyzed: {len(self.models)}\\n\")\n",
        "            f.write(f\"Total frames in trace: {len(self.can_trace)}\\n\\n\")\n",
        "\n",
        "            f.write(\"CAN ID Summary:\\n\")\n",
        "            for can_id in self.models:\n",
        "                frames = self.can_trace.get_frames_by_id(can_id)\n",
        "                num_classes = self.models[can_id].output_shape[-1]\n",
        "                f.write(f\"  CAN ID {can_id}: {len(frames)} frames, {num_classes} signal classes\\n\")\n",
        "\n",
        "        logger.info(f\"Results saved to {self.output_dir}\")\n",
        "\n",
        "\n",
        "class MultiFileCANAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyzer that can process multiple CAN trace files and provide comparative\n",
        "    analysis across files as well as between traditional and deep learning methods.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, log_directory: str, output_dir: str = \"multifile_can_analysis\",\n",
        "                compare_traditional: bool = True):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer with a directory of CAN log files.\n",
        "\n",
        "        Args:\n",
        "            log_directory: Directory containing CAN log files\n",
        "            output_dir: Directory to save results\n",
        "            compare_traditional: Whether to also run traditional analysis for comparison\n",
        "        \"\"\"\n",
        "        self.log_directory = log_directory\n",
        "        self.output_dir = output_dir\n",
        "        self.compare_traditional = compare_traditional\n",
        "        self.trace_analyzers = {}  # Map of filename -> DeepCANAnalyzer\n",
        "        self.common_can_ids = set()  # CAN IDs common across all files\n",
        "        self.cross_file_patterns = {}  # Patterns found across multiple files\n",
        "\n",
        "        # Create output directories\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        os.makedirs(os.path.join(output_dir, \"cross_file_analysis\"), exist_ok=True)\n",
        "\n",
        "        logger.info(f\"Initialized MultiFileCANAnalyzer with log directory: {log_directory}\")\n",
        "\n",
        "    def process_log_files(self, file_extension: str = \".log\", min_frames: int = 1000):\n",
        "        \"\"\"\n",
        "        Process all log files in the directory with the given extension.\n",
        "\n",
        "        Args:\n",
        "            file_extension: Extension of log files to process\n",
        "            min_frames: Minimum number of frames required for analysis\n",
        "        \"\"\"\n",
        "        # Find all log files\n",
        "        log_files = [f for f in os.listdir(self.log_directory)\n",
        "                    if os.path.isfile(os.path.join(self.log_directory, f))\n",
        "                    and f.endswith(file_extension)]\n",
        "\n",
        "        if not log_files:\n",
        "            logger.error(f\"No {file_extension} files found in {self.log_directory}\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Found {len(log_files)} log files to process\")\n",
        "\n",
        "        # Process each log file\n",
        "        for i, log_file in enumerate(log_files):\n",
        "            logger.info(f\"Processing log file {i+1}/{len(log_files)}: {log_file}\")\n",
        "\n",
        "            # Create subdirectory for this log file\n",
        "            file_basename = os.path.splitext(log_file)[0]\n",
        "            file_output_dir = os.path.join(self.output_dir, file_basename)\n",
        "            os.makedirs(file_output_dir, exist_ok=True)\n",
        "\n",
        "            # Load trace\n",
        "            trace = CANTrace.from_log(os.path.join(self.log_directory, log_file))\n",
        "            logger.info(f\"Loaded trace with {len(trace)} frames and {len(trace.get_unique_ids())} unique IDs\")\n",
        "\n",
        "            # Create analyzer\n",
        "            analyzer = DeepCANAnalyzer(trace, output_dir=file_output_dir,\n",
        "                                      compare_traditional=self.compare_traditional)\n",
        "\n",
        "            # Run analysis if sufficient data\n",
        "            if len(trace) >= min_frames:\n",
        "                analyzer.analyze_all_signals()\n",
        "\n",
        "                # Generate comparative visualizations\n",
        "                for can_id in analyzer.models:\n",
        "                    analyzer.identify_signal_patterns(can_id)\n",
        "                    if self.compare_traditional:\n",
        "                        analyzer.generate_comparative_visualizations(can_id)\n",
        "\n",
        "                # Save results\n",
        "                analyzer.save_results()\n",
        "\n",
        "                # Store analyzer\n",
        "                self.trace_analyzers[log_file] = analyzer\n",
        "            else:\n",
        "                logger.warning(f\"Skipping analysis for {log_file}: insufficient frames ({len(trace)} < {min_frames})\")\n",
        "\n",
        "        # After processing all files, perform cross-file analysis\n",
        "        if len(self.trace_analyzers) > 1:\n",
        "            self.perform_cross_file_analysis()\n",
        "\n",
        "    def perform_cross_file_analysis(self):\n",
        "        \"\"\"Perform analysis across multiple log files.\"\"\"\n",
        "        logger.info(\"Performing cross-file analysis\")\n",
        "\n",
        "        # Find common CAN IDs across all files\n",
        "        all_can_ids = [set(analyzer.can_trace.get_unique_ids())\n",
        "                      for analyzer in self.trace_analyzers.values()]\n",
        "\n",
        "        if all_can_ids:\n",
        "            # Start with all CAN IDs from first file\n",
        "            self.common_can_ids = all_can_ids[0].copy()\n",
        "\n",
        "            # Find intersection with all other files\n",
        "            for can_ids in all_can_ids[1:]:\n",
        "                self.common_can_ids.intersection_update(can_ids)\n",
        "\n",
        "        logger.info(f\"Found {len(self.common_can_ids)} CAN IDs common across all files\")\n",
        "\n",
        "        # Analyze patterns for common CAN IDs\n",
        "        for can_id in self.common_can_ids:\n",
        "            self.analyze_cross_file_patterns(can_id)\n",
        "\n",
        "        # Generate comparative visualizations across files\n",
        "        self.generate_cross_file_visualizations()\n",
        "\n",
        "        # Save summary\n",
        "        self.save_cross_file_summary()\n",
        "\n",
        "    def analyze_cross_file_patterns(self, can_id: str):\n",
        "        \"\"\"\n",
        "        Analyze patterns for a specific CAN ID across multiple files.\n",
        "\n",
        "        Args:\n",
        "            can_id: CAN ID to analyze\n",
        "        \"\"\"\n",
        "        # Check if this CAN ID has models in all files\n",
        "        files_with_models = [filename for filename, analyzer in self.trace_analyzers.items()\n",
        "                           if can_id in analyzer.models]\n",
        "\n",
        "        if len(files_with_models) < 2:\n",
        "            return  # Need at least 2 files with models\n",
        "\n",
        "        # Collect patterns from each file\n",
        "        file_patterns = {}\n",
        "\n",
        "        for filename in files_with_models:\n",
        "            analyzer = self.trace_analyzers[filename]\n",
        "            patterns = analyzer.identify_signal_patterns(can_id)\n",
        "\n",
        "            if patterns:\n",
        "                file_patterns[filename] = patterns\n",
        "\n",
        "        # Store cross-file patterns\n",
        "        self.cross_file_patterns[can_id] = file_patterns\n",
        "\n",
        "    def generate_cross_file_visualizations(self):\n",
        "        \"\"\"Generate visualizations comparing CAN data across files.\"\"\"\n",
        "        # Skip if no common CAN IDs\n",
        "        if not self.common_can_ids:\n",
        "            logger.warning(\"No common CAN IDs found for cross-file visualization\")\n",
        "            return\n",
        "\n",
        "        file_names = list(self.trace_analyzers.keys())\n",
        "\n",
        "        # 1. CAN ID Distribution Comparison\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Get CAN ID counts for each file\n",
        "        id_counts = {}\n",
        "        for filename, analyzer in self.trace_analyzers.items():\n",
        "            unique_ids = analyzer.can_trace.get_unique_ids()\n",
        "            id_counts[filename] = len(unique_ids)\n",
        "\n",
        "        plt.bar(range(len(id_counts)), list(id_counts.values()), tick_label=list(id_counts.keys()))\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.title('CAN ID Count Comparison Across Files')\n",
        "        plt.xlabel('Log File')\n",
        "        plt.ylabel('Number of Unique CAN IDs')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, \"cross_file_analysis\", \"can_id_distribution.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 2. Common CAN ID Pattern Comparison\n",
        "        for can_id in self.common_can_ids:\n",
        "            if can_id in self.cross_file_patterns:\n",
        "                file_patterns = self.cross_file_patterns[can_id]\n",
        "\n",
        "                if len(file_patterns) >= 2:\n",
        "                    plt.figure(figsize=(15, 5 * len(file_patterns)))\n",
        "\n",
        "                    for i, (filename, patterns) in enumerate(file_patterns.items()):\n",
        "                        # Show up to 3 classes from each file\n",
        "                        for j, (class_idx, pattern) in enumerate(list(patterns.items())[:3]):\n",
        "                            plt.subplot(len(file_patterns), 3, i*3 + j + 1)\n",
        "\n",
        "                            # Get average pattern\n",
        "                            avg_pattern = pattern['average_pattern']\n",
        "\n",
        "                            # Display pattern\n",
        "                            sns.heatmap(avg_pattern, cmap='viridis')\n",
        "                            plt.title(f'{filename} - Class {class_idx}')\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(os.path.join(self.output_dir, \"cross_file_analysis\", f\"pattern_comparison_{can_id}.png\"))\n",
        "                    plt.close()\n",
        "\n",
        "        # 3. Performance Comparison Chart\n",
        "        if self.compare_traditional:\n",
        "            # Compare CNN performance across files\n",
        "            cnn_performance = {}\n",
        "\n",
        "            for filename, analyzer in self.trace_analyzers.items():\n",
        "                # Get average performance metrics across all models\n",
        "                metrics = []\n",
        "                for can_id in analyzer.models:\n",
        "                    # Get model evaluation results (if available)\n",
        "                    if hasattr(analyzer.models[can_id], 'history') and analyzer.models[can_id].history is not None:\n",
        "                        history = analyzer.models[can_id].history.history\n",
        "                        if 'val_accuracy' in history:\n",
        "                            metrics.append(max(history['val_accuracy']))\n",
        "\n",
        "                if metrics:\n",
        "                    cnn_performance[filename] = np.mean(metrics)\n",
        "\n",
        "            if cnn_performance:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.bar(range(len(cnn_performance)), list(cnn_performance.values()),\n",
        "                        tick_label=list(cnn_performance.keys()))\n",
        "                plt.xticks(rotation=45, ha='right')\n",
        "                plt.title('CNN Model Performance Comparison Across Files')\n",
        "                plt.xlabel('Log File')\n",
        "                plt.ylabel('Average Validation Accuracy')\n",
        "                plt.ylim(0, 1)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(self.output_dir, \"cross_file_analysis\", \"cnn_performance_comparison.png\"))\n",
        "                plt.close()\n",
        "\n",
        "    def save_cross_file_summary(self):\n",
        "        \"\"\"Save summary of cross-file analysis.\"\"\"\n",
        "        # Create summary report\n",
        "        with open(os.path.join(self.output_dir, \"cross_file_summary.txt\"), 'w') as f:\n",
        "            f.write(\"Multi-File CAN Analysis Summary\\n\")\n",
        "            f.write(\"===============================\\n\\n\")\n",
        "\n",
        "            f.write(f\"Total log files processed: {len(self.trace_analyzers)}\\n\")\n",
        "            f.write(f\"Log files: {', '.join(self.trace_analyzers.keys())}\\n\\n\")\n",
        "\n",
        "            f.write(f\"Common CAN IDs across all files: {len(self.common_can_ids)}\\n\")\n",
        "            f.write(f\"Common CAN IDs: {', '.join(sorted(self.common_can_ids))}\\n\\n\")\n",
        "\n",
        "            # File-specific statistics\n",
        "            f.write(\"File Statistics:\\n\")\n",
        "            f.write(\"-----------------\\n\")\n",
        "            for filename, analyzer in self.trace_analyzers.items():\n",
        "                trace = analyzer.can_trace\n",
        "                f.write(f\"\\n{filename}:\\n\")\n",
        "                f.write(f\"  Total frames: {len(trace)}\\n\")\n",
        "                f.write(f\"  Unique CAN IDs: {len(trace.get_unique_ids())}\\n\")\n",
        "                f.write(f\"  Models trained: {len(analyzer.models)}\\n\")\n",
        "\n",
        "                # Top 5 most frequent CAN IDs\n",
        "                id_counts = {can_id: len(trace.get_frames_by_id(can_id)) for can_id in trace.get_unique_ids()}\n",
        "                top_ids = sorted(id_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "                f.write(\"  Top 5 most frequent CAN IDs:\\n\")\n",
        "                for can_id, count in top_ids:\n",
        "                    f.write(f\"    {can_id}: {count} frames\\n\")\n",
        "\n",
        "        logger.info(f\"Cross-file summary saved to {os.path.join(self.output_dir, 'cross_file_summary.txt')}\")\n",
        "\n",
        "        # Save common CAN IDs for future reference\n",
        "        with open(os.path.join(self.output_dir, \"common_can_ids.txt\"), 'w') as f:\n",
        "            for can_id in sorted(self.common_can_ids):\n",
        "                f.write(f\"{can_id}\\n\")\n",
        "\n",
        "\n",
        "# Demo usage\n",
        "def main():\n",
        "    # Set the directory containing CAN log files\n",
        "    log_directory = \"can_logs\"  # Replace with your log directory\n",
        "\n",
        "    if not os.path.exists(log_directory):\n",
        "        logger.error(f\"Log directory {log_directory} not found\")\n",
        "        print(f\"Please create a directory named '{log_directory}' containing your CAN log files,\")\n",
        "        print(\"or modify the script to point to your actual log directory.\")\n",
        "        return\n",
        "\n",
        "    # Create multi-file analyzer\n",
        "    analyzer = MultiFileCANAnalyzer(log_directory, compare_traditional=True)\n",
        "\n",
        "    # Process all log files\n",
        "    analyzer.process_log_files(file_extension=\".log\")\n",
        "\n",
        "    print(\"\\nAnalysis complete! The following results have been generated:\")\n",
        "    print(f\"1. Individual file analyses: {os.path.join(analyzer.output_dir, '<filename>', '*.png')}\")\n",
        "    print(f\"2. Cross-file visualizations: {os.path.join(analyzer.output_dir, 'cross_file_analysis', '*.png')}\")\n",
        "    print(f\"3. Summary report: {os.path.join(analyzer.output_dir, 'cross_file_summary.txt')}\")\n",
        "\n",
        "    logger.info(\"Multi-file analysis complete!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6K61RrxnL-H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b21ed23d-474d-4a63-cc56-e3e8b3002a16"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing CAN IDs:   4%|▎         | 2/56 [00:00<00:14,  3.80it/s]WARNING:__main__:Skipping CAN ID 57F: insufficient frames (67 < 100)\n",
            "Processing CAN IDs: 100%|██████████| 56/56 [01:00<00:00,  1.08s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.4410 - loss: 1.6095"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 537ms/step - accuracy: 0.4806 - loss: 1.6342 - val_accuracy: 0.3600 - val_loss: 33.5291\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8148 - loss: 0.7156 - val_accuracy: 0.3600 - val_loss: 37.0967\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 198ms/step - accuracy: 0.7479 - loss: 1.2067 - val_accuracy: 0.3600 - val_loss: 39.5246\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.9131 - loss: 0.4195 - val_accuracy: 0.3600 - val_loss: 34.8618\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 207ms/step - accuracy: 0.8768 - loss: 0.3412 - val_accuracy: 0.3600 - val_loss: 27.9936\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 223ms/step - accuracy: 0.8925 - loss: 0.4018 - val_accuracy: 0.3600 - val_loss: 22.7835\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 273ms/step - accuracy: 0.8984 - loss: 0.3249 - val_accuracy: 0.3600 - val_loss: 18.8177\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311ms/step - accuracy: 0.9571 - loss: 0.1477 - val_accuracy: 0.3600 - val_loss: 17.2612\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 324ms/step - accuracy: 0.9538 - loss: 0.1310 - val_accuracy: 0.3600 - val_loss: 15.4127\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step - accuracy: 0.9406 - loss: 0.2691 - val_accuracy: 0.3600 - val_loss: 13.6187\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step - accuracy: 0.9540 - loss: 0.1106 - val_accuracy: 0.3600 - val_loss: 13.0108\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.9541 - loss: 0.0797 - val_accuracy: 0.3600 - val_loss: 11.6052\n",
            "Epoch 13/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9635 - loss: 0.0990"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step - accuracy: 0.9661 - loss: 0.0962 - val_accuracy: 0.4000 - val_loss: 10.5853\n",
            "Epoch 14/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9722 - loss: 0.1048"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268ms/step - accuracy: 0.9753 - loss: 0.0948 - val_accuracy: 0.4400 - val_loss: 9.6112\n",
            "Epoch 15/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.9896 - loss: 0.0506"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272ms/step - accuracy: 0.9818 - loss: 0.0651 - val_accuracy: 0.4800 - val_loss: 7.5908\n",
            "Epoch 16/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 0.0202"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275ms/step - accuracy: 1.0000 - loss: 0.0199 - val_accuracy: 0.5600 - val_loss: 5.3536\n",
            "Epoch 17/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.0261"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 300ms/step - accuracy: 1.0000 - loss: 0.0253 - val_accuracy: 0.6400 - val_loss: 4.0976\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 189ms/step - accuracy: 0.9868 - loss: 0.0466 - val_accuracy: 0.6400 - val_loss: 3.2742\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9718 - loss: 0.2002"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 357ms/step - accuracy: 0.9694 - loss: 0.2195 - val_accuracy: 0.6800 - val_loss: 3.1588\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262ms/step - accuracy: 0.9755 - loss: 0.0627 - val_accuracy: 0.6000 - val_loss: 3.9303\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 277ms/step - accuracy: 1.0000 - loss: 0.0230 - val_accuracy: 0.5600 - val_loss: 4.7615\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390ms/step - accuracy: 1.0000 - loss: 0.0205 - val_accuracy: 0.5600 - val_loss: 4.9518\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 252ms/step - accuracy: 0.9745 - loss: 0.0816 - val_accuracy: 0.6400 - val_loss: 4.4074\n",
            "Epoch 24/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 241ms/step - accuracy: 0.9859 - loss: 0.0503 - val_accuracy: 0.5600 - val_loss: 5.0766\n",
            "Epoch 25/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 700ms/step - accuracy: 0.9657 - loss: 0.2037 - val_accuracy: 0.5600 - val_loss: 4.7702\n",
            "Epoch 26/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 283ms/step - accuracy: 0.9383 - loss: 0.1536 - val_accuracy: 0.6800 - val_loss: 2.5009\n",
            "Epoch 27/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 509ms/step - accuracy: 0.9446 - loss: 0.1030 - val_accuracy: 0.6400 - val_loss: 2.9185\n",
            "Epoch 28/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 331ms/step - accuracy: 0.9323 - loss: 0.1696 - val_accuracy: 0.6400 - val_loss: 3.6484\n",
            "Epoch 29/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 273ms/step - accuracy: 0.9154 - loss: 0.2289 - val_accuracy: 0.6000 - val_loss: 4.9513\n",
            "Epoch 30/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 323ms/step - accuracy: 0.9130 - loss: 0.2931 - val_accuracy: 0.6000 - val_loss: 7.9083\n",
            "Epoch 31/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - accuracy: 0.9482 - loss: 0.1896 - val_accuracy: 0.5200 - val_loss: 5.7393\n",
            "Epoch 32/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 753ms/step - accuracy: 0.9448 - loss: 0.0967 - val_accuracy: 0.6000 - val_loss: 2.2635\n",
            "Epoch 33/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 211ms/step - accuracy: 0.9011 - loss: 0.2270 - val_accuracy: 0.6000 - val_loss: 2.1753\n",
            "Epoch 34/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step - accuracy: 0.9715 - loss: 0.1037 - val_accuracy: 0.6800 - val_loss: 3.1024\n",
            "Epoch 35/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 215ms/step - accuracy: 0.9477 - loss: 0.2385 - val_accuracy: 0.6000 - val_loss: 4.5035\n",
            "Epoch 36/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step - accuracy: 0.9220 - loss: 0.1765 - val_accuracy: 0.6000 - val_loss: 4.0759\n",
            "Epoch 37/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 212ms/step - accuracy: 0.9343 - loss: 0.1571 - val_accuracy: 0.6800 - val_loss: 3.2613\n",
            "Epoch 38/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9722 - loss: 0.0423"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 291ms/step - accuracy: 0.9753 - loss: 0.0381 - val_accuracy: 0.7200 - val_loss: 2.9983\n",
            "Epoch 39/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.9878 - loss: 0.0295"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 367ms/step - accuracy: 0.9847 - loss: 0.0313 - val_accuracy: 0.7600 - val_loss: 2.6480\n",
            "Epoch 40/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 315ms/step - accuracy: 0.9939 - loss: 0.0186 - val_accuracy: 0.7600 - val_loss: 2.3848\n",
            "Epoch 41/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 292ms/step - accuracy: 0.9816 - loss: 0.0796 - val_accuracy: 0.7600 - val_loss: 2.2484\n",
            "Epoch 42/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step - accuracy: 1.0000 - loss: 0.0303 - val_accuracy: 0.7200 - val_loss: 2.0993\n",
            "Epoch 43/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 218ms/step - accuracy: 1.0000 - loss: 0.0182 - val_accuracy: 0.7200 - val_loss: 1.9476\n",
            "Epoch 44/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.9845 - loss: 0.0592 - val_accuracy: 0.7600 - val_loss: 1.8065\n",
            "Epoch 45/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 216ms/step - accuracy: 0.9816 - loss: 0.0546 - val_accuracy: 0.7600 - val_loss: 1.8068\n",
            "Epoch 46/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step - accuracy: 0.9326 - loss: 0.2534 - val_accuracy: 0.7600 - val_loss: 2.0761\n",
            "Epoch 47/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step - accuracy: 0.9446 - loss: 0.0904 - val_accuracy: 0.6800 - val_loss: 3.2575\n",
            "Epoch 48/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step - accuracy: 0.9326 - loss: 0.3633 - val_accuracy: 0.6800 - val_loss: 3.1123\n",
            "Epoch 49/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 187ms/step - accuracy: 0.9572 - loss: 0.1121 - val_accuracy: 0.6400 - val_loss: 2.3850\n",
            "Epoch 50/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step - accuracy: 0.9538 - loss: 0.1292 - val_accuracy: 0.6000 - val_loss: 2.4946\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 631ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 255ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 290ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 275ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 282ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 190ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 185ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 191ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 187ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (25, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.5983 - loss: 1.1758"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 265ms/step - accuracy: 0.6005 - loss: 1.1691 - val_accuracy: 0.3259 - val_loss: 24.9436\n",
            "Epoch 2/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.7693 - loss: 0.5031"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 178ms/step - accuracy: 0.7692 - loss: 0.5038 - val_accuracy: 0.6926 - val_loss: 1.6524\n",
            "Epoch 3/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.7584 - loss: 0.4951"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 208ms/step - accuracy: 0.7591 - loss: 0.4939 - val_accuracy: 0.7148 - val_loss: 0.7415\n",
            "Epoch 4/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.7557 - loss: 0.5347"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 182ms/step - accuracy: 0.7560 - loss: 0.5324 - val_accuracy: 0.7296 - val_loss: 0.5915\n",
            "Epoch 5/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8573 - loss: 0.3268"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 188ms/step - accuracy: 0.8571 - loss: 0.3271 - val_accuracy: 0.8778 - val_loss: 0.4098\n",
            "Epoch 6/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.8607 - loss: 0.3378"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 180ms/step - accuracy: 0.8607 - loss: 0.3373 - val_accuracy: 0.8889 - val_loss: 0.3982\n",
            "Epoch 7/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 166ms/step - accuracy: 0.8606 - loss: 0.3306 - val_accuracy: 0.8481 - val_loss: 0.3278\n",
            "Epoch 8/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 195ms/step - accuracy: 0.9106 - loss: 0.2834 - val_accuracy: 0.8852 - val_loss: 0.3034\n",
            "Epoch 9/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.9006 - loss: 0.2582"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 168ms/step - accuracy: 0.9003 - loss: 0.2588 - val_accuracy: 0.9296 - val_loss: 0.3705\n",
            "Epoch 10/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 201ms/step - accuracy: 0.8906 - loss: 0.2894 - val_accuracy: 0.8741 - val_loss: 0.3044\n",
            "Epoch 11/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 170ms/step - accuracy: 0.8849 - loss: 0.2439 - val_accuracy: 0.9148 - val_loss: 0.2788\n",
            "Epoch 12/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 169ms/step - accuracy: 0.9109 - loss: 0.2253 - val_accuracy: 0.9111 - val_loss: 0.2742\n",
            "Epoch 13/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9266 - loss: 0.1701"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 198ms/step - accuracy: 0.9264 - loss: 0.1707 - val_accuracy: 0.9444 - val_loss: 0.2266\n",
            "Epoch 14/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 201ms/step - accuracy: 0.9027 - loss: 0.2157 - val_accuracy: 0.8000 - val_loss: 0.5852\n",
            "Epoch 15/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 255ms/step - accuracy: 0.9439 - loss: 0.1350 - val_accuracy: 0.9185 - val_loss: 0.3178\n",
            "Epoch 16/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 174ms/step - accuracy: 0.9313 - loss: 0.1973 - val_accuracy: 0.8815 - val_loss: 0.3044\n",
            "Epoch 17/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 203ms/step - accuracy: 0.9084 - loss: 0.2114 - val_accuracy: 0.9111 - val_loss: 0.3298\n",
            "Epoch 18/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 224ms/step - accuracy: 0.9421 - loss: 0.1601 - val_accuracy: 0.8630 - val_loss: 0.5199\n",
            "Epoch 19/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 164ms/step - accuracy: 0.9274 - loss: 0.1751 - val_accuracy: 0.9037 - val_loss: 0.3745\n",
            "Epoch 20/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 195ms/step - accuracy: 0.9667 - loss: 0.1020 - val_accuracy: 0.9259 - val_loss: 0.3536\n",
            "Epoch 21/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 168ms/step - accuracy: 0.9444 - loss: 0.1297 - val_accuracy: 0.7963 - val_loss: 0.4943\n",
            "Epoch 22/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 218ms/step - accuracy: 0.9488 - loss: 0.1219 - val_accuracy: 0.9333 - val_loss: 0.3006\n",
            "Epoch 23/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 172ms/step - accuracy: 0.9657 - loss: 0.0849 - val_accuracy: 0.8889 - val_loss: 0.4166\n",
            "Epoch 1/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.5624 - loss: 1.5403"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 378ms/step - accuracy: 0.5661 - loss: 1.5275 - val_accuracy: 0.3456 - val_loss: 30.7066\n",
            "Epoch 2/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 178ms/step - accuracy: 0.7318 - loss: 0.7323 - val_accuracy: 0.3456 - val_loss: 29.7395\n",
            "Epoch 3/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 241ms/step - accuracy: 0.8455 - loss: 0.3754 - val_accuracy: 0.3456 - val_loss: 33.8755\n",
            "Epoch 4/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 546ms/step - accuracy: 0.8933 - loss: 0.2452 - val_accuracy: 0.3456 - val_loss: 25.9838\n",
            "Epoch 5/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - accuracy: 0.9108 - loss: 0.2314 - val_accuracy: 0.3456 - val_loss: 20.9061\n",
            "Epoch 6/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 179ms/step - accuracy: 0.9190 - loss: 0.1806 - val_accuracy: 0.3456 - val_loss: 17.4038\n",
            "Epoch 7/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 229ms/step - accuracy: 0.9388 - loss: 0.1517 - val_accuracy: 0.3456 - val_loss: 10.8246\n",
            "Epoch 8/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 213ms/step - accuracy: 0.9833 - loss: 0.0719 - val_accuracy: 0.3456 - val_loss: 8.7356\n",
            "Epoch 9/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9811 - loss: 0.0761"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 206ms/step - accuracy: 0.9811 - loss: 0.0750 - val_accuracy: 0.4632 - val_loss: 3.4245\n",
            "Epoch 10/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9802 - loss: 0.0634"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 194ms/step - accuracy: 0.9799 - loss: 0.0642 - val_accuracy: 0.8015 - val_loss: 0.4069\n",
            "Epoch 11/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.9875 - loss: 0.0749"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 264ms/step - accuracy: 0.9872 - loss: 0.0748 - val_accuracy: 0.8456 - val_loss: 0.3916\n",
            "Epoch 12/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9851 - loss: 0.0520"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 189ms/step - accuracy: 0.9851 - loss: 0.0514 - val_accuracy: 0.8676 - val_loss: 0.3545\n",
            "Epoch 13/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 162ms/step - accuracy: 0.9932 - loss: 0.0265 - val_accuracy: 0.8235 - val_loss: 0.4121\n",
            "Epoch 14/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 231ms/step - accuracy: 0.9924 - loss: 0.0284 - val_accuracy: 0.8088 - val_loss: 0.4077\n",
            "Epoch 15/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 212ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 0.8603 - val_loss: 0.3755\n",
            "Epoch 16/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 309ms/step - accuracy: 0.9920 - loss: 0.0264 - val_accuracy: 0.8235 - val_loss: 0.4424\n",
            "Epoch 17/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 620ms/step - accuracy: 0.9981 - loss: 0.0104 - val_accuracy: 0.8676 - val_loss: 0.3997\n",
            "Epoch 18/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 434ms/step - accuracy: 0.9928 - loss: 0.0128 - val_accuracy: 0.8235 - val_loss: 0.6570\n",
            "Epoch 19/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 776ms/step - accuracy: 1.0000 - loss: 0.0092 - val_accuracy: 0.8309 - val_loss: 0.4891\n",
            "Epoch 20/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 179ms/step - accuracy: 0.9961 - loss: 0.0086 - val_accuracy: 0.8456 - val_loss: 0.4580\n",
            "Epoch 21/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 177ms/step - accuracy: 0.9982 - loss: 0.0050 - val_accuracy: 0.8603 - val_loss: 0.5420\n",
            "Epoch 22/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 283ms/step - accuracy: 0.9912 - loss: 0.0399 - val_accuracy: 0.8456 - val_loss: 0.5413\n",
            "Epoch 1/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.5055 - loss: 1.4955"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 208ms/step - accuracy: 0.5070 - loss: 1.4839 - val_accuracy: 0.3296 - val_loss: 40.1487\n",
            "Epoch 2/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 225ms/step - accuracy: 0.6708 - loss: 0.8286 - val_accuracy: 0.3296 - val_loss: 15.3026\n",
            "Epoch 3/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 170ms/step - accuracy: 0.7738 - loss: 0.5214 - val_accuracy: 0.3296 - val_loss: 14.4599\n",
            "Epoch 4/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.8651 - loss: 0.3450"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 174ms/step - accuracy: 0.8646 - loss: 0.3451 - val_accuracy: 0.3371 - val_loss: 5.0816\n",
            "Epoch 5/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8705 - loss: 0.3231"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 224ms/step - accuracy: 0.8704 - loss: 0.3235 - val_accuracy: 0.3521 - val_loss: 5.3604\n",
            "Epoch 6/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 172ms/step - accuracy: 0.8927 - loss: 0.2576 - val_accuracy: 0.3296 - val_loss: 7.3733\n",
            "Epoch 7/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.8759 - loss: 0.3043"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 195ms/step - accuracy: 0.8756 - loss: 0.3047 - val_accuracy: 0.7528 - val_loss: 0.6281\n",
            "Epoch 8/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.9105 - loss: 0.2308"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 227ms/step - accuracy: 0.9109 - loss: 0.2298 - val_accuracy: 0.8352 - val_loss: 0.4599\n",
            "Epoch 9/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 166ms/step - accuracy: 0.9311 - loss: 0.1803 - val_accuracy: 0.8052 - val_loss: 0.4176\n",
            "Epoch 10/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 187ms/step - accuracy: 0.9076 - loss: 0.2241 - val_accuracy: 0.8090 - val_loss: 0.5003\n",
            "Epoch 11/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 225ms/step - accuracy: 0.8905 - loss: 0.2485 - val_accuracy: 0.4719 - val_loss: 2.3547\n",
            "Epoch 12/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 171ms/step - accuracy: 0.9489 - loss: 0.1364 - val_accuracy: 0.4195 - val_loss: 5.5296\n",
            "Epoch 13/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 201ms/step - accuracy: 0.9645 - loss: 0.0819 - val_accuracy: 0.4981 - val_loss: 2.9349\n",
            "Epoch 14/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 158ms/step - accuracy: 0.9780 - loss: 0.0691 - val_accuracy: 0.4419 - val_loss: 3.5829\n",
            "Epoch 15/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 216ms/step - accuracy: 0.9510 - loss: 0.1284 - val_accuracy: 0.7154 - val_loss: 0.9489\n",
            "Epoch 16/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9409 - loss: 0.1221"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 169ms/step - accuracy: 0.9416 - loss: 0.1217 - val_accuracy: 0.8764 - val_loss: 0.4452\n",
            "Epoch 17/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 184ms/step - accuracy: 0.9796 - loss: 0.0557 - val_accuracy: 0.8052 - val_loss: 0.6806\n",
            "Epoch 18/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 226ms/step - accuracy: 0.9419 - loss: 0.1493 - val_accuracy: 0.7678 - val_loss: 0.8063\n",
            "Epoch 19/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 173ms/step - accuracy: 0.9342 - loss: 0.1694 - val_accuracy: 0.5918 - val_loss: 2.0341\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7cf818ddfc40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.5925 - loss: 0.6350"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 603ms/step - accuracy: 0.6271 - loss: 0.5936 - val_accuracy: 1.0000 - val_loss: 0.4287\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 364ms/step - accuracy: 0.9943 - loss: 0.0379 - val_accuracy: 1.0000 - val_loss: 0.3426\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 361ms/step - accuracy: 1.0000 - loss: 9.9461e-04 - val_accuracy: 1.0000 - val_loss: 0.2525\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 225ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 1.0000 - val_loss: 0.1707\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 225ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 0.1313\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 222ms/step - accuracy: 1.0000 - loss: 1.8839e-04 - val_accuracy: 1.0000 - val_loss: 0.0943\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 216ms/step - accuracy: 1.0000 - loss: 9.4547e-05 - val_accuracy: 1.0000 - val_loss: 0.0624\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 1.0000 - loss: 3.5116e-05 - val_accuracy: 1.0000 - val_loss: 0.0347\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 206ms/step - accuracy: 1.0000 - loss: 5.8355e-05 - val_accuracy: 1.0000 - val_loss: 0.0188\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 235ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0117\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 220ms/step - accuracy: 1.0000 - loss: 4.9777e-05 - val_accuracy: 1.0000 - val_loss: 0.0072\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 263ms/step - accuracy: 1.0000 - loss: 6.2256e-05 - val_accuracy: 1.0000 - val_loss: 0.0042\n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 321ms/step - accuracy: 1.0000 - loss: 1.2343e-04 - val_accuracy: 1.0000 - val_loss: 0.0020\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 303ms/step - accuracy: 1.0000 - loss: 1.3965e-05 - val_accuracy: 1.0000 - val_loss: 6.4739e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 368ms/step - accuracy: 1.0000 - loss: 9.7863e-06 - val_accuracy: 1.0000 - val_loss: 2.6902e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 230ms/step - accuracy: 1.0000 - loss: 8.0061e-07 - val_accuracy: 1.0000 - val_loss: 1.2421e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step - accuracy: 1.0000 - loss: 2.6427e-04 - val_accuracy: 1.0000 - val_loss: 6.0112e-05\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 220ms/step - accuracy: 1.0000 - loss: 3.1985e-07 - val_accuracy: 1.0000 - val_loss: 3.3143e-05\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217ms/step - accuracy: 1.0000 - loss: 3.0570e-08 - val_accuracy: 1.0000 - val_loss: 1.8508e-05\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 221ms/step - accuracy: 1.0000 - loss: 2.0467e-04 - val_accuracy: 1.0000 - val_loss: 1.3692e-05\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - accuracy: 1.0000 - loss: 6.8442e-08 - val_accuracy: 1.0000 - val_loss: 1.9340e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step - accuracy: 1.0000 - loss: 1.1420e-06 - val_accuracy: 1.0000 - val_loss: 2.6406e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226ms/step - accuracy: 1.0000 - loss: 3.0652e-05 - val_accuracy: 1.0000 - val_loss: 2.9424e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 221ms/step - accuracy: 1.0000 - loss: 6.7602e-05 - val_accuracy: 1.0000 - val_loss: 2.6735e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 349ms/step - accuracy: 1.0000 - loss: 9.1151e-05 - val_accuracy: 1.0000 - val_loss: 1.9826e-05\n",
            "Epoch 26/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 318ms/step - accuracy: 0.9849 - loss: 0.0175 - val_accuracy: 1.0000 - val_loss: 2.0783e-07\n",
            "Epoch 27/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 301ms/step - accuracy: 1.0000 - loss: 3.6675e-09 - val_accuracy: 1.0000 - val_loss: 2.3787e-08\n",
            "Epoch 28/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224ms/step - accuracy: 1.0000 - loss: 8.0447e-08 - val_accuracy: 1.0000 - val_loss: 6.9574e-09\n",
            "Epoch 29/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232ms/step - accuracy: 1.0000 - loss: 5.3441e-04 - val_accuracy: 1.0000 - val_loss: 3.7091e-09\n",
            "Epoch 30/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232ms/step - accuracy: 1.0000 - loss: 9.1390e-04 - val_accuracy: 1.0000 - val_loss: 3.2245e-09\n",
            "Epoch 31/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 224ms/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 1.0000 - val_loss: 6.9907e-09\n",
            "Epoch 32/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 405ms/step - accuracy: 1.0000 - loss: 6.5205e-07 - val_accuracy: 1.0000 - val_loss: 1.4962e-08\n",
            "Epoch 33/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 242ms/step - accuracy: 1.0000 - loss: 1.6975e-06 - val_accuracy: 1.0000 - val_loss: 2.3881e-08\n",
            "Epoch 34/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 226ms/step - accuracy: 1.0000 - loss: 7.4485e-08 - val_accuracy: 1.0000 - val_loss: 3.0590e-08\n",
            "Epoch 35/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 247ms/step - accuracy: 1.0000 - loss: 2.9462e-07 - val_accuracy: 1.0000 - val_loss: 3.3346e-08\n",
            "Epoch 36/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 324ms/step - accuracy: 1.0000 - loss: 8.9550e-07 - val_accuracy: 1.0000 - val_loss: 3.3042e-08\n",
            "Epoch 37/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 337ms/step - accuracy: 1.0000 - loss: 3.2782e-04 - val_accuracy: 1.0000 - val_loss: 3.8628e-08\n",
            "Epoch 38/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 309ms/step - accuracy: 1.0000 - loss: 9.4603e-07 - val_accuracy: 1.0000 - val_loss: 4.2989e-08\n",
            "Epoch 39/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 207ms/step - accuracy: 1.0000 - loss: 1.3518e-07 - val_accuracy: 1.0000 - val_loss: 4.2886e-08\n",
            "Epoch 40/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228ms/step - accuracy: 1.0000 - loss: 1.2238e-06 - val_accuracy: 1.0000 - val_loss: 4.0888e-08\n",
            "Epoch 1/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8116 - loss: 0.7517"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 241ms/step - accuracy: 0.8174 - loss: 0.7316 - val_accuracy: 0.9924 - val_loss: 0.1530\n",
            "Epoch 2/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 230ms/step - accuracy: 0.9746 - loss: 0.1865 - val_accuracy: 0.9394 - val_loss: 0.2348\n",
            "Epoch 3/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 222ms/step - accuracy: 0.9879 - loss: 0.1043 - val_accuracy: 0.9924 - val_loss: 0.0939\n",
            "Epoch 4/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 169ms/step - accuracy: 0.9833 - loss: 0.0664 - val_accuracy: 0.9924 - val_loss: 0.0678\n",
            "Epoch 5/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9953 - loss: 0.0332 - val_accuracy: 0.9924 - val_loss: 0.0454\n",
            "Epoch 6/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 240ms/step - accuracy: 0.9863 - loss: 0.0671 - val_accuracy: 0.9924 - val_loss: 0.0318\n",
            "Epoch 7/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 205ms/step - accuracy: 0.9949 - loss: 0.0124 - val_accuracy: 0.9924 - val_loss: 0.0501\n",
            "Epoch 8/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 167ms/step - accuracy: 0.9938 - loss: 0.0230 - val_accuracy: 0.9924 - val_loss: 0.0305\n",
            "Epoch 9/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 161ms/step - accuracy: 0.9886 - loss: 0.0270 - val_accuracy: 0.9924 - val_loss: 0.0829\n",
            "Epoch 10/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 178ms/step - accuracy: 0.9884 - loss: 0.0269 - val_accuracy: 0.9924 - val_loss: 0.0202\n",
            "Epoch 11/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 279ms/step - accuracy: 0.9979 - loss: 0.0070 - val_accuracy: 0.9924 - val_loss: 0.0140\n",
            "Epoch 12/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 172ms/step - accuracy: 0.9948 - loss: 0.0101 - val_accuracy: 0.9924 - val_loss: 0.0257\n",
            "Epoch 13/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 174ms/step - accuracy: 0.9924 - loss: 0.0168 - val_accuracy: 0.9924 - val_loss: 0.0521\n",
            "Epoch 14/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 166ms/step - accuracy: 0.9939 - loss: 0.0187 - val_accuracy: 0.9924 - val_loss: 0.0627\n",
            "Epoch 15/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 269ms/step - accuracy: 0.9996 - loss: 0.0044 - val_accuracy: 0.9924 - val_loss: 0.0160\n",
            "Epoch 16/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 168ms/step - accuracy: 0.9988 - loss: 0.0056 - val_accuracy: 0.9924 - val_loss: 0.0380\n",
            "Epoch 17/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 168ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.9924 - val_loss: 0.0261\n",
            "Epoch 18/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 241ms/step - accuracy: 1.0000 - loss: 5.3136e-04 - val_accuracy: 0.9924 - val_loss: 0.0211\n",
            "Epoch 19/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 207ms/step - accuracy: 0.9998 - loss: 0.0011 - val_accuracy: 0.9924 - val_loss: 0.0210\n",
            "Epoch 20/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 174ms/step - accuracy: 0.9996 - loss: 0.0042 - val_accuracy: 0.9924 - val_loss: 0.0274\n",
            "Epoch 21/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.9924 - val_loss: 0.0358\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7cf8537a7600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 967ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 195ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 191ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 193ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 221ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 210ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 190ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 205ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (25, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.3117 - loss: 3.7821"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 0.3153 - loss: 3.8531 - val_accuracy: 0.2000 - val_loss: 4.0252\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.3112 - loss: 1.9619 - val_accuracy: 0.2000 - val_loss: 4.2157\n",
            "Epoch 3/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.3837 - loss: 1.3635"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 189ms/step - accuracy: 0.3702 - loss: 1.3633 - val_accuracy: 0.3200 - val_loss: 2.7674\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - accuracy: 0.3787 - loss: 1.1510 - val_accuracy: 0.3200 - val_loss: 1.9708\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - accuracy: 0.2827 - loss: 1.1304 - val_accuracy: 0.3200 - val_loss: 1.9460\n",
            "Epoch 6/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.3368 - loss: 1.0987"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 187ms/step - accuracy: 0.3341 - loss: 1.0987 - val_accuracy: 0.4800 - val_loss: 2.3343\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step - accuracy: 0.3143 - loss: 1.0987 - val_accuracy: 0.4800 - val_loss: 2.8063\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 0.3278 - loss: 1.0986 - val_accuracy: 0.4800 - val_loss: 3.0068\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.3539 - loss: 1.1028 - val_accuracy: 0.4800 - val_loss: 2.7058\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step - accuracy: 0.3714 - loss: 1.0985 - val_accuracy: 0.4800 - val_loss: 1.9309\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 129ms/step - accuracy: 0.3419 - loss: 1.0986 - val_accuracy: 0.4800 - val_loss: 1.5375\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 0.3419 - loss: 1.0986 - val_accuracy: 0.4800 - val_loss: 1.3171\n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - accuracy: 0.3336 - loss: 1.0986 - val_accuracy: 0.3200 - val_loss: 1.2297\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - accuracy: 0.3648 - loss: 1.0984 - val_accuracy: 0.3200 - val_loss: 1.1853\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 167ms/step - accuracy: 0.3659 - loss: 1.0958 - val_accuracy: 0.3200 - val_loss: 1.1572\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 219ms/step - accuracy: 0.3565 - loss: 1.0984 - val_accuracy: 0.3200 - val_loss: 1.1800\n",
            "Epoch 17/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 197ms/step - accuracy: 0.3325 - loss: 1.0986 - val_accuracy: 0.3200 - val_loss: 1.2412\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232ms/step - accuracy: 0.3461 - loss: 1.0984 - val_accuracy: 0.3200 - val_loss: 1.2838\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 249ms/step - accuracy: 0.3763 - loss: 1.0981 - val_accuracy: 0.3200 - val_loss: 1.3309\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - accuracy: 0.3763 - loss: 1.0980 - val_accuracy: 0.3200 - val_loss: 1.3857\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.3939 - loss: 1.0885 - val_accuracy: 0.3200 - val_loss: 1.7516\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - accuracy: 0.3933 - loss: 1.2215 - val_accuracy: 0.2000 - val_loss: 1.2719\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 138ms/step - accuracy: 0.3419 - loss: 1.0985 - val_accuracy: 0.2000 - val_loss: 1.2725\n",
            "Epoch 24/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - accuracy: 0.3648 - loss: 1.0981 - val_accuracy: 0.2000 - val_loss: 1.2776\n",
            "Epoch 25/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - accuracy: 0.3346 - loss: 1.0986 - val_accuracy: 0.2000 - val_loss: 1.2840\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.3689 - loss: 2.4163"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 282ms/step - accuracy: 0.3706 - loss: 2.3891 - val_accuracy: 0.3459 - val_loss: 9.7223\n",
            "Epoch 2/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 145ms/step - accuracy: 0.6325 - loss: 0.8618 - val_accuracy: 0.3459 - val_loss: 6.7825\n",
            "Epoch 3/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 143ms/step - accuracy: 0.7312 - loss: 0.6076 - val_accuracy: 0.3459 - val_loss: 9.6887\n",
            "Epoch 4/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 140ms/step - accuracy: 0.8176 - loss: 0.4577 - val_accuracy: 0.3459 - val_loss: 7.4395\n",
            "Epoch 5/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 143ms/step - accuracy: 0.8635 - loss: 0.3248 - val_accuracy: 0.3459 - val_loss: 9.2177\n",
            "Epoch 6/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 227ms/step - accuracy: 0.9386 - loss: 0.2136 - val_accuracy: 0.3459 - val_loss: 8.6718\n",
            "Epoch 7/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9343 - loss: 0.2044"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - accuracy: 0.9338 - loss: 0.2052 - val_accuracy: 0.3835 - val_loss: 4.4024\n",
            "Epoch 8/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9589 - loss: 0.1244"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step - accuracy: 0.9587 - loss: 0.1247 - val_accuracy: 0.5038 - val_loss: 1.4744\n",
            "Epoch 9/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9603 - loss: 0.1189"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 144ms/step - accuracy: 0.9595 - loss: 0.1203 - val_accuracy: 0.6090 - val_loss: 0.9098\n",
            "Epoch 10/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.9730 - loss: 0.0754 - val_accuracy: 0.4812 - val_loss: 1.9687\n",
            "Epoch 11/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 205ms/step - accuracy: 0.9870 - loss: 0.0558 - val_accuracy: 0.5489 - val_loss: 1.6395\n",
            "Epoch 12/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9883 - loss: 0.0439"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 169ms/step - accuracy: 0.9883 - loss: 0.0442 - val_accuracy: 0.8496 - val_loss: 0.3428\n",
            "Epoch 13/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 135ms/step - accuracy: 0.9968 - loss: 0.0212 - val_accuracy: 0.7970 - val_loss: 0.4132\n",
            "Epoch 14/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - accuracy: 0.9794 - loss: 0.0550 - val_accuracy: 0.6165 - val_loss: 1.0813\n",
            "Epoch 15/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 183ms/step - accuracy: 0.9832 - loss: 0.0636 - val_accuracy: 0.6617 - val_loss: 0.9443\n",
            "Epoch 16/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - accuracy: 0.9761 - loss: 0.0806 - val_accuracy: 0.8045 - val_loss: 0.4525\n",
            "Epoch 17/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 153ms/step - accuracy: 0.9745 - loss: 0.0649 - val_accuracy: 0.7068 - val_loss: 0.7692\n",
            "Epoch 18/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 129ms/step - accuracy: 0.9775 - loss: 0.0812 - val_accuracy: 0.6692 - val_loss: 0.8958\n",
            "Epoch 19/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 136ms/step - accuracy: 0.9774 - loss: 0.0524 - val_accuracy: 0.7970 - val_loss: 0.5822\n",
            "Epoch 20/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 202ms/step - accuracy: 0.9875 - loss: 0.0418 - val_accuracy: 0.6692 - val_loss: 1.3020\n",
            "Epoch 21/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 135ms/step - accuracy: 0.9947 - loss: 0.0171 - val_accuracy: 0.8120 - val_loss: 0.4542\n",
            "Epoch 22/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 0.9865 - loss: 0.0275 - val_accuracy: 0.7368 - val_loss: 0.5512\n",
            "Epoch 1/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.4362 - loss: 2.2936"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 179ms/step - accuracy: 0.4398 - loss: 2.2707 - val_accuracy: 0.3083 - val_loss: 30.5910\n",
            "Epoch 2/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.7081 - loss: 0.7127 - val_accuracy: 0.3083 - val_loss: 24.4119\n",
            "Epoch 3/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 153ms/step - accuracy: 0.6616 - loss: 0.6445 - val_accuracy: 0.3083 - val_loss: 18.0647\n",
            "Epoch 4/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 202ms/step - accuracy: 0.7011 - loss: 0.6369 - val_accuracy: 0.3083 - val_loss: 8.0631\n",
            "Epoch 5/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 153ms/step - accuracy: 0.7130 - loss: 0.5955 - val_accuracy: 0.3083 - val_loss: 4.6752\n",
            "Epoch 6/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.7288 - loss: 0.5665"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 0.7289 - loss: 0.5647 - val_accuracy: 0.3158 - val_loss: 2.5185\n",
            "Epoch 7/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step - accuracy: 0.7853 - loss: 0.4251 - val_accuracy: 0.3158 - val_loss: 2.5339\n",
            "Epoch 8/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.8162 - loss: 0.4226"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 206ms/step - accuracy: 0.8161 - loss: 0.4216 - val_accuracy: 0.3459 - val_loss: 1.2027\n",
            "Epoch 9/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.8316 - loss: 0.3579"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 189ms/step - accuracy: 0.8319 - loss: 0.3578 - val_accuracy: 0.4286 - val_loss: 1.0801\n",
            "Epoch 10/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8912 - loss: 0.3165"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 145ms/step - accuracy: 0.8909 - loss: 0.3161 - val_accuracy: 0.4511 - val_loss: 1.1861\n",
            "Epoch 11/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8747 - loss: 0.3028"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 157ms/step - accuracy: 0.8756 - loss: 0.3011 - val_accuracy: 0.7820 - val_loss: 0.4575\n",
            "Epoch 12/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8854 - loss: 0.2830"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 184ms/step - accuracy: 0.8843 - loss: 0.2857 - val_accuracy: 0.7895 - val_loss: 0.5271\n",
            "Epoch 13/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 188ms/step - accuracy: 0.8821 - loss: 0.2870 - val_accuracy: 0.7519 - val_loss: 0.4694\n",
            "Epoch 14/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8540 - loss: 0.2972"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 155ms/step - accuracy: 0.8549 - loss: 0.2969 - val_accuracy: 0.8421 - val_loss: 0.4644\n",
            "Epoch 15/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 153ms/step - accuracy: 0.8933 - loss: 0.2436 - val_accuracy: 0.7895 - val_loss: 0.4081\n",
            "Epoch 16/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 146ms/step - accuracy: 0.9003 - loss: 0.2611 - val_accuracy: 0.7820 - val_loss: 0.5069\n",
            "Epoch 17/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 155ms/step - accuracy: 0.9295 - loss: 0.2235 - val_accuracy: 0.8195 - val_loss: 0.4052\n",
            "Epoch 18/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 200ms/step - accuracy: 0.9055 - loss: 0.1894 - val_accuracy: 0.8271 - val_loss: 0.4667\n",
            "Epoch 19/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 141ms/step - accuracy: 0.9618 - loss: 0.1023 - val_accuracy: 0.8421 - val_loss: 0.3986\n",
            "Epoch 20/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 138ms/step - accuracy: 0.9567 - loss: 0.1023 - val_accuracy: 0.7970 - val_loss: 0.5518\n",
            "Epoch 21/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step - accuracy: 0.9394 - loss: 0.1443 - val_accuracy: 0.7218 - val_loss: 1.1266\n",
            "Epoch 22/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 141ms/step - accuracy: 0.9360 - loss: 0.1357 - val_accuracy: 0.7820 - val_loss: 0.7013\n",
            "Epoch 23/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 188ms/step - accuracy: 0.9529 - loss: 0.1040 - val_accuracy: 0.6767 - val_loss: 1.5184\n",
            "Epoch 24/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 138ms/step - accuracy: 0.9451 - loss: 0.1533 - val_accuracy: 0.7519 - val_loss: 0.9004\n",
            "Epoch 25/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 136ms/step - accuracy: 0.9790 - loss: 0.0663 - val_accuracy: 0.7519 - val_loss: 1.1376\n",
            "Epoch 26/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 142ms/step - accuracy: 0.9426 - loss: 0.1488 - val_accuracy: 0.7444 - val_loss: 0.9922\n",
            "Epoch 27/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - accuracy: 0.9632 - loss: 0.0919 - val_accuracy: 0.8120 - val_loss: 0.7210\n",
            "Epoch 28/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 195ms/step - accuracy: 0.9592 - loss: 0.1007 - val_accuracy: 0.8120 - val_loss: 0.5032\n",
            "Epoch 29/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 178ms/step - accuracy: 0.9643 - loss: 0.0951 - val_accuracy: 0.7669 - val_loss: 0.8695\n",
            "Epoch 1/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.6061 - loss: 1.7181"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 279ms/step - accuracy: 0.6092 - loss: 1.7014 - val_accuracy: 0.3534 - val_loss: 16.8995\n",
            "Epoch 2/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - accuracy: 0.7353 - loss: 0.6590 - val_accuracy: 0.3534 - val_loss: 20.6119\n",
            "Epoch 3/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.7435 - loss: 0.7076 - val_accuracy: 0.3534 - val_loss: 11.9567\n",
            "Epoch 4/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 0.7772 - loss: 0.5980 - val_accuracy: 0.3534 - val_loss: 5.8030\n",
            "Epoch 5/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.7469 - loss: 0.6339"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 200ms/step - accuracy: 0.7475 - loss: 0.6361 - val_accuracy: 0.6917 - val_loss: 1.0763\n",
            "Epoch 6/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.7188 - loss: 0.6849"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.7205 - loss: 0.6827 - val_accuracy: 0.8421 - val_loss: 0.6122\n",
            "Epoch 7/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 0.7753 - loss: 0.5778 - val_accuracy: 0.6992 - val_loss: 0.6863\n",
            "Epoch 8/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.7744 - loss: 0.6082 - val_accuracy: 0.8421 - val_loss: 0.5388\n",
            "Epoch 9/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 191ms/step - accuracy: 0.7602 - loss: 0.5850 - val_accuracy: 0.8421 - val_loss: 0.5794\n",
            "Epoch 10/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.7884 - loss: 0.5586"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - accuracy: 0.7877 - loss: 0.5592 - val_accuracy: 0.8722 - val_loss: 0.5346\n",
            "Epoch 11/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 144ms/step - accuracy: 0.8077 - loss: 0.5016 - val_accuracy: 0.8722 - val_loss: 0.5612\n",
            "Epoch 12/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.7327 - loss: 0.5459 - val_accuracy: 0.8421 - val_loss: 0.4760\n",
            "Epoch 13/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 212ms/step - accuracy: 0.7995 - loss: 0.5219 - val_accuracy: 0.8722 - val_loss: 0.5307\n",
            "Epoch 14/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 144ms/step - accuracy: 0.7935 - loss: 0.4976 - val_accuracy: 0.8722 - val_loss: 0.5307\n",
            "Epoch 15/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 141ms/step - accuracy: 0.7850 - loss: 0.4816 - val_accuracy: 0.8722 - val_loss: 0.4595\n",
            "Epoch 16/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 0.8066 - loss: 0.4553 - val_accuracy: 0.8722 - val_loss: 0.4721\n",
            "Epoch 17/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 168ms/step - accuracy: 0.8175 - loss: 0.4157 - val_accuracy: 0.8722 - val_loss: 0.4277\n",
            "Epoch 18/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 203ms/step - accuracy: 0.8366 - loss: 0.3551 - val_accuracy: 0.8722 - val_loss: 0.3998\n",
            "Epoch 19/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - accuracy: 0.8491 - loss: 0.3475 - val_accuracy: 0.8722 - val_loss: 0.4492\n",
            "Epoch 20/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 142ms/step - accuracy: 0.8282 - loss: 0.4113 - val_accuracy: 0.8421 - val_loss: 0.4339\n",
            "Epoch 21/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 133ms/step - accuracy: 0.8151 - loss: 0.4248 - val_accuracy: 0.8722 - val_loss: 0.3923\n",
            "Epoch 22/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 183ms/step - accuracy: 0.8389 - loss: 0.3895 - val_accuracy: 0.8722 - val_loss: 0.3843\n",
            "Epoch 23/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 0.8756 - loss: 0.3821 - val_accuracy: 0.8722 - val_loss: 0.4789\n",
            "Epoch 24/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.8626 - loss: 0.3743"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 142ms/step - accuracy: 0.8625 - loss: 0.3741 - val_accuracy: 0.9398 - val_loss: 0.4605\n",
            "Epoch 25/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 145ms/step - accuracy: 0.8773 - loss: 0.3538 - val_accuracy: 0.9023 - val_loss: 0.2981\n",
            "Epoch 26/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 199ms/step - accuracy: 0.8873 - loss: 0.3360 - val_accuracy: 0.8722 - val_loss: 0.4540\n",
            "Epoch 27/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - accuracy: 0.9038 - loss: 0.2522 - val_accuracy: 0.8722 - val_loss: 0.3893\n",
            "Epoch 28/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9118 - loss: 0.3460"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.9117 - loss: 0.3445 - val_accuracy: 0.9474 - val_loss: 0.2390\n",
            "Epoch 29/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 188ms/step - accuracy: 0.9108 - loss: 0.2468 - val_accuracy: 0.9323 - val_loss: 0.2321\n",
            "Epoch 30/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 201ms/step - accuracy: 0.9141 - loss: 0.2124 - val_accuracy: 0.9398 - val_loss: 0.2261\n",
            "Epoch 31/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 140ms/step - accuracy: 0.9251 - loss: 0.1928 - val_accuracy: 0.9098 - val_loss: 0.2801\n",
            "Epoch 32/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - accuracy: 0.9093 - loss: 0.2246 - val_accuracy: 0.9323 - val_loss: 0.2825\n",
            "Epoch 33/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 0.9432 - loss: 0.1871 - val_accuracy: 0.9398 - val_loss: 0.2191\n",
            "Epoch 34/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 198ms/step - accuracy: 0.9558 - loss: 0.1898 - val_accuracy: 0.8947 - val_loss: 0.3040\n",
            "Epoch 35/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - accuracy: 0.9289 - loss: 0.1778 - val_accuracy: 0.3308 - val_loss: 2.5803\n",
            "Epoch 36/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 138ms/step - accuracy: 0.9420 - loss: 0.1583 - val_accuracy: 0.9398 - val_loss: 0.2365\n",
            "Epoch 37/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 141ms/step - accuracy: 0.9255 - loss: 0.1847 - val_accuracy: 0.9474 - val_loss: 0.2350\n",
            "Epoch 38/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 206ms/step - accuracy: 0.9241 - loss: 0.1498 - val_accuracy: 0.9474 - val_loss: 0.2263\n",
            "Epoch 39/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 182ms/step - accuracy: 0.9518 - loss: 0.1254 - val_accuracy: 0.9474 - val_loss: 0.2103\n",
            "Epoch 40/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 148ms/step - accuracy: 0.9622 - loss: 0.1071 - val_accuracy: 0.9474 - val_loss: 0.2142\n",
            "Epoch 41/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 134ms/step - accuracy: 0.9444 - loss: 0.1470 - val_accuracy: 0.9474 - val_loss: 0.2575\n",
            "Epoch 42/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 166ms/step - accuracy: 0.9477 - loss: 0.1445 - val_accuracy: 0.9398 - val_loss: 0.2813\n",
            "Epoch 43/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 216ms/step - accuracy: 0.9328 - loss: 0.1621 - val_accuracy: 0.9474 - val_loss: 0.2671\n",
            "Epoch 44/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 157ms/step - accuracy: 0.9488 - loss: 0.1410 - val_accuracy: 0.9323 - val_loss: 0.3991\n",
            "Epoch 45/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 154ms/step - accuracy: 0.9441 - loss: 0.1320 - val_accuracy: 0.9474 - val_loss: 0.2342\n",
            "Epoch 46/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9469 - loss: 0.1461 - val_accuracy: 0.9398 - val_loss: 0.3229\n",
            "Epoch 47/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 164ms/step - accuracy: 0.9570 - loss: 0.1267 - val_accuracy: 0.8421 - val_loss: 0.5080\n",
            "Epoch 48/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 146ms/step - accuracy: 0.9540 - loss: 0.1360 - val_accuracy: 0.8797 - val_loss: 0.4501\n",
            "Epoch 49/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9135 - loss: 0.1853"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 152ms/step - accuracy: 0.9148 - loss: 0.1838 - val_accuracy: 0.9549 - val_loss: 0.2058\n",
            "Epoch 50/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 188ms/step - accuracy: 0.9371 - loss: 0.1636 - val_accuracy: 0.5865 - val_loss: 1.6338\n",
            "Epoch 1/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.6176 - loss: 1.2158"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 216ms/step - accuracy: 0.6199 - loss: 1.2055 - val_accuracy: 0.3519 - val_loss: 53.6040\n",
            "Epoch 2/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 151ms/step - accuracy: 0.8451 - loss: 0.3839 - val_accuracy: 0.3519 - val_loss: 24.4007\n",
            "Epoch 3/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9007 - loss: 0.2361"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 135ms/step - accuracy: 0.9009 - loss: 0.2354 - val_accuracy: 0.3593 - val_loss: 11.3705\n",
            "Epoch 4/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9463 - loss: 0.1352"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 151ms/step - accuracy: 0.9462 - loss: 0.1356 - val_accuracy: 0.5222 - val_loss: 3.2611\n",
            "Epoch 5/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 193ms/step - accuracy: 0.9542 - loss: 0.1366 - val_accuracy: 0.4148 - val_loss: 4.5358\n",
            "Epoch 6/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9366 - loss: 0.1718"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 140ms/step - accuracy: 0.9370 - loss: 0.1705 - val_accuracy: 0.9000 - val_loss: 0.2163\n",
            "Epoch 7/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 138ms/step - accuracy: 0.9611 - loss: 0.1124 - val_accuracy: 0.8037 - val_loss: 0.5405\n",
            "Epoch 8/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 276ms/step - accuracy: 0.9777 - loss: 0.0786 - val_accuracy: 0.8852 - val_loss: 0.4604\n",
            "Epoch 9/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - accuracy: 0.9693 - loss: 0.0679"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 511ms/step - accuracy: 0.9691 - loss: 0.0686 - val_accuracy: 0.9259 - val_loss: 0.2269\n",
            "Epoch 10/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 101ms/step - accuracy: 0.9840 - loss: 0.0663 - val_accuracy: 0.8815 - val_loss: 0.2556\n",
            "Epoch 11/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - accuracy: 0.9833 - loss: 0.0445 - val_accuracy: 0.9185 - val_loss: 0.3701\n",
            "Epoch 12/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9850 - loss: 0.0350 - val_accuracy: 0.9185 - val_loss: 0.2557\n",
            "Epoch 13/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 0.9799 - loss: 0.0532 - val_accuracy: 0.7630 - val_loss: 0.8802\n",
            "Epoch 14/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 112ms/step - accuracy: 0.9699 - loss: 0.0933 - val_accuracy: 0.9222 - val_loss: 0.3943\n",
            "Epoch 15/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.9811 - loss: 0.0576 - val_accuracy: 0.8185 - val_loss: 0.9882\n",
            "Epoch 16/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9883 - loss: 0.0415 - val_accuracy: 0.8852 - val_loss: 0.5298\n",
            "Epoch 1/50\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8765 - loss: 0.4588"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 115ms/step - accuracy: 0.8862 - loss: 0.4227 - val_accuracy: 0.6515 - val_loss: 2.4884\n",
            "Epoch 2/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 2.3732e-05 - val_accuracy: 0.6515 - val_loss: 2.8675\n",
            "Epoch 3/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 2.7400e-05 - val_accuracy: 0.6515 - val_loss: 0.3040\n",
            "Epoch 4/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 2.3967e-06"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 2.7384e-06 - val_accuracy: 1.0000 - val_loss: 0.0051\n",
            "Epoch 5/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 1.4626e-05 - val_accuracy: 1.0000 - val_loss: 3.1945e-05\n",
            "Epoch 6/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 5.1113e-06 - val_accuracy: 1.0000 - val_loss: 2.0546e-06\n",
            "Epoch 7/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 5.2203e-06 - val_accuracy: 1.0000 - val_loss: 4.1001e-07\n",
            "Epoch 8/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 4.5215e-06 - val_accuracy: 1.0000 - val_loss: 1.6346e-07\n",
            "Epoch 9/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 2.8649e-05 - val_accuracy: 1.0000 - val_loss: 1.2282e-07\n",
            "Epoch 10/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 6.5098e-07 - val_accuracy: 1.0000 - val_loss: 1.2372e-07\n",
            "Epoch 11/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 2.6699e-06 - val_accuracy: 1.0000 - val_loss: 4.1543e-08\n",
            "Epoch 12/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 3.3978e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 13/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 7.2297e-07 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 14/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 2.1591e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 15/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 7.0958e-07 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 16/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 3.4810e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 17/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 1.7070e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 18/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 1.7259e-05 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 19/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 1.2903e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 20/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 9.3999e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 21/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 3.9075e-09 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 22/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 3.0493e-07 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 1/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6130 - loss: 1.3904"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 114ms/step - accuracy: 0.6157 - loss: 1.3772 - val_accuracy: 0.3783 - val_loss: 21.2335\n",
            "Epoch 2/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - accuracy: 0.8310 - loss: 0.4287 - val_accuracy: 0.3783 - val_loss: 18.9622\n",
            "Epoch 3/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9036 - loss: 0.2536 - val_accuracy: 0.3783 - val_loss: 15.7149\n",
            "Epoch 4/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8725 - loss: 0.2917"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - accuracy: 0.8733 - loss: 0.2902 - val_accuracy: 0.3858 - val_loss: 3.3739\n",
            "Epoch 5/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9158 - loss: 0.2193"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 110ms/step - accuracy: 0.9155 - loss: 0.2200 - val_accuracy: 0.3895 - val_loss: 3.6426\n",
            "Epoch 6/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9153 - loss: 0.2229"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.9156 - loss: 0.2218 - val_accuracy: 0.5094 - val_loss: 1.6502\n",
            "Epoch 7/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9081 - loss: 0.2448"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9093 - loss: 0.2428 - val_accuracy: 0.6105 - val_loss: 1.5901\n",
            "Epoch 8/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9471 - loss: 0.1281"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 136ms/step - accuracy: 0.9476 - loss: 0.1276 - val_accuracy: 0.7191 - val_loss: 0.9563\n",
            "Epoch 9/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9562 - loss: 0.1341"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9557 - loss: 0.1352 - val_accuracy: 0.8427 - val_loss: 0.5566\n",
            "Epoch 10/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9699 - loss: 0.0980 - val_accuracy: 0.7341 - val_loss: 1.0775\n",
            "Epoch 11/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - accuracy: 0.9545 - loss: 0.1063 - val_accuracy: 0.8090 - val_loss: 0.6844\n",
            "Epoch 12/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9733 - loss: 0.0741"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - accuracy: 0.9731 - loss: 0.0748 - val_accuracy: 0.8801 - val_loss: 0.3461\n",
            "Epoch 13/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9841 - loss: 0.0565"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9829 - loss: 0.0591 - val_accuracy: 0.8951 - val_loss: 0.3308\n",
            "Epoch 14/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9706 - loss: 0.0644 - val_accuracy: 0.8764 - val_loss: 0.3514\n",
            "Epoch 15/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - accuracy: 0.9508 - loss: 0.1320 - val_accuracy: 0.5918 - val_loss: 2.2480\n",
            "Epoch 16/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.9673 - loss: 0.0780 - val_accuracy: 0.8427 - val_loss: 0.5060\n",
            "Epoch 17/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 0.9710 - loss: 0.0938 - val_accuracy: 0.8951 - val_loss: 0.4022\n",
            "Epoch 18/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9669 - loss: 0.0846 - val_accuracy: 0.8614 - val_loss: 0.3705\n",
            "Epoch 19/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 140ms/step - accuracy: 0.9798 - loss: 0.0645 - val_accuracy: 0.8052 - val_loss: 0.7140\n",
            "Epoch 20/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9812 - loss: 0.0420 - val_accuracy: 0.7004 - val_loss: 1.8441\n",
            "Epoch 21/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.9771 - loss: 0.0629 - val_accuracy: 0.8689 - val_loss: 0.3781\n",
            "Epoch 22/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 130ms/step - accuracy: 0.9752 - loss: 0.0671 - val_accuracy: 0.8876 - val_loss: 0.2580\n",
            "Epoch 23/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.9935 - loss: 0.0251 - val_accuracy: 0.8801 - val_loss: 0.4751\n",
            "Epoch 24/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 0.9916 - loss: 0.0316 - val_accuracy: 0.4869 - val_loss: 4.9295\n",
            "Epoch 25/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9758 - loss: 0.0857"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.9758 - loss: 0.0853 - val_accuracy: 0.9064 - val_loss: 0.4197\n",
            "Epoch 26/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9556 - loss: 0.1139 - val_accuracy: 0.8502 - val_loss: 0.4107\n",
            "Epoch 27/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.9642 - loss: 0.1058 - val_accuracy: 0.8689 - val_loss: 0.5297\n",
            "Epoch 28/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 110ms/step - accuracy: 0.9822 - loss: 0.0478 - val_accuracy: 0.8727 - val_loss: 0.4364\n",
            "Epoch 29/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 113ms/step - accuracy: 0.9927 - loss: 0.0252 - val_accuracy: 0.8689 - val_loss: 0.4969\n",
            "Epoch 30/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.9907 - loss: 0.0370 - val_accuracy: 0.8539 - val_loss: 0.4422\n",
            "Epoch 31/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9844 - loss: 0.0830 - val_accuracy: 0.8202 - val_loss: 0.6606\n",
            "Epoch 32/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - accuracy: 0.9935 - loss: 0.0224 - val_accuracy: 0.8577 - val_loss: 0.6068\n",
            "Epoch 1/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2708 - loss: 3.5719"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 238ms/step - accuracy: 0.2865 - loss: 3.6083 - val_accuracy: 0.3600 - val_loss: 19.0212\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5633 - loss: 1.4339 - val_accuracy: 0.3600 - val_loss: 15.6266\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 0.5961 - loss: 1.0661 - val_accuracy: 0.3200 - val_loss: 16.5506\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.7017 - loss: 1.0326 - val_accuracy: 0.3600 - val_loss: 19.2596\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step - accuracy: 0.7387 - loss: 0.6214 - val_accuracy: 0.3200 - val_loss: 17.4792\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - accuracy: 0.8221 - loss: 0.5379 - val_accuracy: 0.3600 - val_loss: 16.6539\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.8185 - loss: 0.5070 - val_accuracy: 0.3600 - val_loss: 15.2631\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 0.7849 - loss: 0.4842 - val_accuracy: 0.3600 - val_loss: 12.9949\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.7835 - loss: 0.5495 - val_accuracy: 0.3200 - val_loss: 11.8205\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.8084 - loss: 0.3573 - val_accuracy: 0.3600 - val_loss: 10.8124\n",
            "Epoch 11/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8507 - loss: 0.3259"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.8584 - loss: 0.3076 - val_accuracy: 0.5200 - val_loss: 9.0631\n",
            "Epoch 12/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9167 - loss: 0.2633"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.9100 - loss: 0.2735 - val_accuracy: 0.5600 - val_loss: 7.2638\n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9065 - loss: 0.2627 - val_accuracy: 0.4000 - val_loss: 5.2758\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.9326 - loss: 0.1698 - val_accuracy: 0.4800 - val_loss: 3.9919\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9293 - loss: 0.2188 - val_accuracy: 0.5600 - val_loss: 3.2274\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.9354 - loss: 0.1602 - val_accuracy: 0.5600 - val_loss: 2.7475\n",
            "Epoch 17/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9149 - loss: 0.1659"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9170 - loss: 0.1621 - val_accuracy: 0.6000 - val_loss: 2.6467\n",
            "Epoch 18/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9497 - loss: 0.1500"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - accuracy: 0.9538 - loss: 0.1369 - val_accuracy: 0.6400 - val_loss: 2.6044\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.9786 - loss: 0.0859 - val_accuracy: 0.6400 - val_loss: 2.6038\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.9540 - loss: 0.1113 - val_accuracy: 0.6000 - val_loss: 2.5802\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 0.9444 - loss: 0.1323 - val_accuracy: 0.6000 - val_loss: 2.3333\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9685 - loss: 0.1085 - val_accuracy: 0.6400 - val_loss: 2.1345\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.9418 - loss: 0.1018 - val_accuracy: 0.5600 - val_loss: 2.4348\n",
            "Epoch 24/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9067 - loss: 0.2333 - val_accuracy: 0.5200 - val_loss: 2.2424\n",
            "Epoch 25/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.9737 - loss: 0.1066 - val_accuracy: 0.5200 - val_loss: 2.2648\n",
            "Epoch 26/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.9293 - loss: 0.1560 - val_accuracy: 0.5600 - val_loss: 2.2994\n",
            "Epoch 27/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step - accuracy: 0.9508 - loss: 0.0896 - val_accuracy: 0.6000 - val_loss: 2.5807\n",
            "Epoch 28/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.9538 - loss: 0.1438 - val_accuracy: 0.6000 - val_loss: 2.5685\n",
            "Epoch 29/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 145ms/step - accuracy: 0.9878 - loss: 0.0535 - val_accuracy: 0.6000 - val_loss: 2.3023\n",
            "Epoch 30/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - accuracy: 0.9437 - loss: 0.1665 - val_accuracy: 0.6000 - val_loss: 2.2700\n",
            "Epoch 31/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - accuracy: 0.9753 - loss: 0.0387 - val_accuracy: 0.5600 - val_loss: 3.1838\n",
            "Epoch 32/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 190ms/step - accuracy: 0.9315 - loss: 0.1141 - val_accuracy: 0.5600 - val_loss: 3.0563\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9486 - loss: 0.1080"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 96ms/step - accuracy: 0.9508 - loss: 0.1035 - val_accuracy: 0.6330 - val_loss: 2.7180\n",
            "Epoch 2/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 1.0000 - loss: 3.3702e-04 - val_accuracy: 0.6330 - val_loss: 1.8349\n",
            "Epoch 3/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 5.9381e-07"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 5.8087e-07 - val_accuracy: 1.0000 - val_loss: 0.0176\n",
            "Epoch 4/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 4.3187e-08 - val_accuracy: 1.0000 - val_loss: 1.0282e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 3.9658e-07 - val_accuracy: 1.0000 - val_loss: 2.2147e-06\n",
            "Epoch 6/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 6.6990e-07 - val_accuracy: 1.0000 - val_loss: 9.2375e-08\n",
            "Epoch 7/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 8.0512e-08 - val_accuracy: 1.0000 - val_loss: 8.2320e-09\n",
            "Epoch 8/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 1.6001e-08 - val_accuracy: 1.0000 - val_loss: 1.2919e-09\n",
            "Epoch 9/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 2.4584e-08 - val_accuracy: 1.0000 - val_loss: 3.1576e-10\n",
            "Epoch 10/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 1.2649e-07 - val_accuracy: 1.0000 - val_loss: 8.9935e-11\n",
            "Epoch 11/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 5.3327e-08 - val_accuracy: 1.0000 - val_loss: 3.2832e-11\n",
            "Epoch 12/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 3.8436e-07 - val_accuracy: 1.0000 - val_loss: 1.3334e-11\n",
            "Epoch 13/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 3.2617e-07 - val_accuracy: 1.0000 - val_loss: 4.6972e-12\n",
            "Epoch 14/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 1.2040e-08 - val_accuracy: 1.0000 - val_loss: 2.6018e-12\n",
            "Epoch 15/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 3.9910e-08 - val_accuracy: 1.0000 - val_loss: 1.5803e-12\n",
            "Epoch 16/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 3.5239e-07 - val_accuracy: 1.0000 - val_loss: 7.5527e-13\n",
            "Epoch 17/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 3.4091e-08 - val_accuracy: 1.0000 - val_loss: 5.2050e-13\n",
            "Epoch 18/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 109ms/step - accuracy: 1.0000 - loss: 3.4260e-08 - val_accuracy: 1.0000 - val_loss: 4.0760e-13\n",
            "Epoch 19/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 1.7959e-08 - val_accuracy: 1.0000 - val_loss: 3.4683e-13\n",
            "Epoch 20/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 5.9493e-08 - val_accuracy: 1.0000 - val_loss: 3.0175e-13\n",
            "Epoch 21/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 4.0835e-06 - val_accuracy: 1.0000 - val_loss: 5.7452e-15\n",
            "Epoch 22/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 6.3440e-10 - val_accuracy: 1.0000 - val_loss: 3.3124e-15\n",
            "Epoch 23/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 1.5428e-08 - val_accuracy: 1.0000 - val_loss: 3.1138e-15\n",
            "Epoch 24/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - accuracy: 1.0000 - loss: 7.4721e-10 - val_accuracy: 1.0000 - val_loss: 3.0347e-15\n",
            "Epoch 25/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 8.0151e-09 - val_accuracy: 1.0000 - val_loss: 2.9811e-15\n",
            "Epoch 26/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 6.7405e-10 - val_accuracy: 1.0000 - val_loss: 2.9364e-15\n",
            "Epoch 27/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 5.2852e-08 - val_accuracy: 1.0000 - val_loss: 2.7737e-15\n",
            "Epoch 28/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 2.5386e-09 - val_accuracy: 1.0000 - val_loss: 2.7266e-15\n",
            "Epoch 29/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 2.0885e-10 - val_accuracy: 1.0000 - val_loss: 2.7055e-15\n",
            "Epoch 30/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 5.0841e-11 - val_accuracy: 1.0000 - val_loss: 2.7131e-15\n",
            "Epoch 31/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 109ms/step - accuracy: 1.0000 - loss: 7.3549e-10 - val_accuracy: 1.0000 - val_loss: 2.7163e-15\n",
            "Epoch 32/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 3.2740e-10 - val_accuracy: 1.0000 - val_loss: 2.7073e-15\n",
            "Epoch 33/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 3.8681e-10 - val_accuracy: 1.0000 - val_loss: 2.6998e-15\n",
            "Epoch 34/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 2.9715e-09 - val_accuracy: 1.0000 - val_loss: 2.6953e-15\n",
            "Epoch 35/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 3.9866e-09 - val_accuracy: 1.0000 - val_loss: 2.6941e-15\n",
            "Epoch 36/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 7.8843e-10 - val_accuracy: 1.0000 - val_loss: 2.6723e-15\n",
            "Epoch 37/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 137ms/step - accuracy: 1.0000 - loss: 5.7661e-09 - val_accuracy: 1.0000 - val_loss: 2.6634e-15\n",
            "Epoch 38/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 8.8792e-09 - val_accuracy: 1.0000 - val_loss: 2.6491e-15\n",
            "Epoch 39/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 8.5609e-10 - val_accuracy: 1.0000 - val_loss: 2.5796e-15\n",
            "Epoch 40/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 6.9279e-10 - val_accuracy: 1.0000 - val_loss: 2.5189e-15\n",
            "Epoch 41/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - accuracy: 1.0000 - loss: 1.8228e-09 - val_accuracy: 1.0000 - val_loss: 2.4650e-15\n",
            "Epoch 42/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 4.7626e-09 - val_accuracy: 1.0000 - val_loss: 2.1367e-15\n",
            "Epoch 43/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 112ms/step - accuracy: 1.0000 - loss: 9.8715e-10 - val_accuracy: 1.0000 - val_loss: 1.9474e-15\n",
            "Epoch 44/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 1.0000 - loss: 2.0484e-10 - val_accuracy: 1.0000 - val_loss: 1.8918e-15\n",
            "Epoch 45/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 8.0556e-10 - val_accuracy: 1.0000 - val_loss: 1.7827e-15\n",
            "Epoch 46/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 3.5764e-09 - val_accuracy: 1.0000 - val_loss: 1.6916e-15\n",
            "Epoch 47/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 3.0373e-09 - val_accuracy: 1.0000 - val_loss: 1.6437e-15\n",
            "Epoch 48/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 2.0024e-08 - val_accuracy: 1.0000 - val_loss: 1.5547e-15\n",
            "Epoch 49/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 5.0630e-09 - val_accuracy: 1.0000 - val_loss: 1.2011e-15\n",
            "Epoch 50/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 1.1569e-07 - val_accuracy: 1.0000 - val_loss: 3.6781e-17\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 172ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.4253 - loss: 1.4650"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 245ms/step - accuracy: 0.4820 - loss: 1.4099 - val_accuracy: 0.2000 - val_loss: 28.9284\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - accuracy: 0.7634 - loss: 1.1809 - val_accuracy: 0.2000 - val_loss: 30.3057\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - accuracy: 0.7509 - loss: 0.6287 - val_accuracy: 0.2000 - val_loss: 29.4731\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - accuracy: 0.7509 - loss: 0.6860 - val_accuracy: 0.2000 - val_loss: 25.9519\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8091 - loss: 0.7142 - val_accuracy: 0.2000 - val_loss: 24.5837\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.7262 - loss: 0.6073 - val_accuracy: 0.2000 - val_loss: 24.1445\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.8724 - loss: 0.3985 - val_accuracy: 0.2000 - val_loss: 21.0753\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7634 - loss: 0.3979 - val_accuracy: 0.2000 - val_loss: 17.8725\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.8445 - loss: 0.3122 - val_accuracy: 0.2000 - val_loss: 19.3387\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.8652 - loss: 0.3311 - val_accuracy: 0.2000 - val_loss: 20.1703\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.7509 - loss: 0.4421 - val_accuracy: 0.2000 - val_loss: 20.2845\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.8257 - loss: 1.0329 - val_accuracy: 0.2000 - val_loss: 14.2313\n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8569 - loss: 0.4815 - val_accuracy: 0.2000 - val_loss: 13.5796\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8164 - loss: 0.3654 - val_accuracy: 0.2000 - val_loss: 17.0229\n",
            "Epoch 15/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.8568 - loss: 0.3326 - val_accuracy: 0.2000 - val_loss: 17.9649\n",
            "Epoch 16/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8472 - loss: 0.3858"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.8506 - loss: 0.4146 - val_accuracy: 0.2800 - val_loss: 16.5251\n",
            "Epoch 17/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8212 - loss: 0.3498"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.8226 - loss: 0.3793 - val_accuracy: 0.4800 - val_loss: 14.9727\n",
            "Epoch 18/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8698 - loss: 0.2641"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.8600 - loss: 0.2747 - val_accuracy: 0.6400 - val_loss: 14.6409\n",
            "Epoch 19/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9062 - loss: 0.2485"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9066 - loss: 0.2477 - val_accuracy: 0.6800 - val_loss: 15.2053\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9067 - loss: 0.2363 - val_accuracy: 0.6800 - val_loss: 15.4229\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.8621 - loss: 0.2440 - val_accuracy: 0.3200 - val_loss: 15.2451\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.8600 - loss: 0.2882 - val_accuracy: 0.3200 - val_loss: 14.9125\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.8071 - loss: 0.5173 - val_accuracy: 0.3200 - val_loss: 12.6187\n",
            "Epoch 24/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.8569 - loss: 0.4902 - val_accuracy: 0.3600 - val_loss: 11.3110\n",
            "Epoch 25/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step - accuracy: 0.8475 - loss: 0.4476 - val_accuracy: 0.3600 - val_loss: 12.0318\n",
            "Epoch 26/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - accuracy: 0.8746 - loss: 0.2756 - val_accuracy: 0.3200 - val_loss: 12.9308\n",
            "Epoch 27/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - accuracy: 0.8569 - loss: 0.4342 - val_accuracy: 0.3200 - val_loss: 12.3148\n",
            "Epoch 28/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy: 0.8257 - loss: 0.8624 - val_accuracy: 0.6800 - val_loss: 8.6644\n",
            "Epoch 29/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 0.7727 - loss: 0.8240 - val_accuracy: 0.6800 - val_loss: 4.8359\n",
            "Epoch 30/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7935 - loss: 0.5222 - val_accuracy: 0.5600 - val_loss: 4.6698\n",
            "Epoch 31/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8756 - loss: 0.2586 - val_accuracy: 0.3200 - val_loss: 6.0282\n",
            "Epoch 32/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 0.7874 - loss: 0.6907 - val_accuracy: 0.3200 - val_loss: 7.7327\n",
            "Epoch 33/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8163 - loss: 0.3700 - val_accuracy: 0.3200 - val_loss: 5.9823\n",
            "Epoch 34/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8528 - loss: 0.4507 - val_accuracy: 0.3200 - val_loss: 5.9958\n",
            "Epoch 35/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8288 - loss: 0.3130 - val_accuracy: 0.3200 - val_loss: 6.1205\n",
            "Epoch 36/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.8569 - loss: 0.5192 - val_accuracy: 0.3200 - val_loss: 8.0115\n",
            "Epoch 37/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.7718 - loss: 0.7958 - val_accuracy: 0.3200 - val_loss: 8.2148\n",
            "Epoch 38/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9067 - loss: 0.2427 - val_accuracy: 0.3200 - val_loss: 7.1653\n",
            "Epoch 39/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.8506 - loss: 0.2784 - val_accuracy: 0.3200 - val_loss: 6.9458\n",
            "Epoch 40/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8506 - loss: 0.4884 - val_accuracy: 0.3200 - val_loss: 6.0074\n",
            "Epoch 1/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.4204 - loss: 1.9097"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 143ms/step - accuracy: 0.4217 - loss: 1.8861 - val_accuracy: 0.3346 - val_loss: 19.8806\n",
            "Epoch 2/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.6376 - loss: 0.8225 - val_accuracy: 0.3346 - val_loss: 12.1802\n",
            "Epoch 3/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.6658 - loss: 0.6666 - val_accuracy: 0.3346 - val_loss: 12.1708\n",
            "Epoch 4/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.7726 - loss: 0.5357 - val_accuracy: 0.3346 - val_loss: 11.0537\n",
            "Epoch 5/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.7926 - loss: 0.4144 - val_accuracy: 0.3346 - val_loss: 6.7197\n",
            "Epoch 6/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8172 - loss: 0.3919"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - accuracy: 0.8173 - loss: 0.3926 - val_accuracy: 0.4511 - val_loss: 2.2125\n",
            "Epoch 7/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8273 - loss: 0.3951"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - accuracy: 0.8276 - loss: 0.3947 - val_accuracy: 0.5451 - val_loss: 1.4947\n",
            "Epoch 8/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 114ms/step - accuracy: 0.8794 - loss: 0.3216 - val_accuracy: 0.4098 - val_loss: 2.5303\n",
            "Epoch 9/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8298 - loss: 0.4055"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - accuracy: 0.8297 - loss: 0.4043 - val_accuracy: 0.7744 - val_loss: 0.5518\n",
            "Epoch 10/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 115ms/step - accuracy: 0.8700 - loss: 0.3346 - val_accuracy: 0.6053 - val_loss: 1.0614\n",
            "Epoch 11/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - accuracy: 0.8757 - loss: 0.2917 - val_accuracy: 0.7744 - val_loss: 0.5038\n",
            "Epoch 12/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9124 - loss: 0.2244"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - accuracy: 0.9112 - loss: 0.2261 - val_accuracy: 0.8045 - val_loss: 0.5904\n",
            "Epoch 13/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.9195 - loss: 0.1933 - val_accuracy: 0.6729 - val_loss: 0.8795\n",
            "Epoch 14/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 121ms/step - accuracy: 0.9181 - loss: 0.2282 - val_accuracy: 0.7519 - val_loss: 0.6828\n",
            "Epoch 15/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.9525 - loss: 0.1562"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 309ms/step - accuracy: 0.9520 - loss: 0.1572 - val_accuracy: 0.8158 - val_loss: 0.4734\n",
            "Epoch 16/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 90ms/step - accuracy: 0.9308 - loss: 0.2016 - val_accuracy: 0.7895 - val_loss: 0.5494\n",
            "Epoch 17/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9432 - loss: 0.1491"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - accuracy: 0.9434 - loss: 0.1490 - val_accuracy: 0.8383 - val_loss: 0.4206\n",
            "Epoch 18/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - accuracy: 0.9513 - loss: 0.1072 - val_accuracy: 0.6429 - val_loss: 1.5454\n",
            "Epoch 19/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.9301 - loss: 0.1611 - val_accuracy: 0.7293 - val_loss: 0.8451\n",
            "Epoch 20/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9584 - loss: 0.1177 - val_accuracy: 0.6654 - val_loss: 1.4314\n",
            "Epoch 21/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 126ms/step - accuracy: 0.9526 - loss: 0.1197 - val_accuracy: 0.7256 - val_loss: 0.9929\n",
            "Epoch 22/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 0.9438 - loss: 0.1641 - val_accuracy: 0.8008 - val_loss: 0.6219\n",
            "Epoch 23/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 109ms/step - accuracy: 0.9493 - loss: 0.1297 - val_accuracy: 0.6090 - val_loss: 1.7556\n",
            "Epoch 24/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 128ms/step - accuracy: 0.9371 - loss: 0.1735 - val_accuracy: 0.5150 - val_loss: 3.9947\n",
            "Epoch 25/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9655 - loss: 0.0957"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - accuracy: 0.9657 - loss: 0.0947 - val_accuracy: 0.8534 - val_loss: 0.4720\n",
            "Epoch 26/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9718 - loss: 0.0893 - val_accuracy: 0.4812 - val_loss: 3.6419\n",
            "Epoch 27/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 134ms/step - accuracy: 0.9474 - loss: 0.1675 - val_accuracy: 0.7820 - val_loss: 0.8544\n",
            "Epoch 1/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5221 - loss: 1.5785"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 112ms/step - accuracy: 0.5248 - loss: 1.5620 - val_accuracy: 0.2963 - val_loss: 51.1387\n",
            "Epoch 2/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - accuracy: 0.7852 - loss: 0.5068 - val_accuracy: 0.2963 - val_loss: 24.5589\n",
            "Epoch 3/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.8441 - loss: 0.4143 - val_accuracy: 0.2963 - val_loss: 10.0539\n",
            "Epoch 4/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8677 - loss: 0.3118"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.8678 - loss: 0.3112 - val_accuracy: 0.3111 - val_loss: 3.3748\n",
            "Epoch 5/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9065 - loss: 0.2295"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - accuracy: 0.9065 - loss: 0.2293 - val_accuracy: 0.4556 - val_loss: 1.9352\n",
            "Epoch 6/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.8615 - loss: 0.3695"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 132ms/step - accuracy: 0.8624 - loss: 0.3668 - val_accuracy: 0.8037 - val_loss: 0.4483\n",
            "Epoch 7/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.9249 - loss: 0.1733 - val_accuracy: 0.7593 - val_loss: 0.5232\n",
            "Epoch 8/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9261 - loss: 0.1983"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.9262 - loss: 0.1982 - val_accuracy: 0.8778 - val_loss: 0.2839\n",
            "Epoch 9/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9109 - loss: 0.1955"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - accuracy: 0.9110 - loss: 0.1953 - val_accuracy: 0.9296 - val_loss: 0.1847\n",
            "Epoch 10/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 113ms/step - accuracy: 0.9498 - loss: 0.1377 - val_accuracy: 0.8333 - val_loss: 0.4398\n",
            "Epoch 11/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9558 - loss: 0.1303 - val_accuracy: 0.8111 - val_loss: 0.7269\n",
            "Epoch 12/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - accuracy: 0.9561 - loss: 0.1046 - val_accuracy: 0.9074 - val_loss: 0.3458\n",
            "Epoch 13/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9717 - loss: 0.0880"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - accuracy: 0.9716 - loss: 0.0880 - val_accuracy: 0.9333 - val_loss: 0.2441\n",
            "Epoch 14/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9563 - loss: 0.1313 - val_accuracy: 0.9074 - val_loss: 0.3052\n",
            "Epoch 15/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.9446 - loss: 0.1688 - val_accuracy: 0.9037 - val_loss: 0.2410\n",
            "Epoch 16/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 149ms/step - accuracy: 0.9571 - loss: 0.1379 - val_accuracy: 0.8704 - val_loss: 0.5919\n",
            "Epoch 17/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9778 - loss: 0.0626"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.9775 - loss: 0.0635 - val_accuracy: 0.9407 - val_loss: 0.1603\n",
            "Epoch 18/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - accuracy: 0.9502 - loss: 0.1184 - val_accuracy: 0.9148 - val_loss: 0.2819\n",
            "Epoch 19/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 123ms/step - accuracy: 0.9767 - loss: 0.0598 - val_accuracy: 0.9111 - val_loss: 0.2099\n",
            "Epoch 20/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - accuracy: 0.9828 - loss: 0.0498 - val_accuracy: 0.9185 - val_loss: 0.2777\n",
            "Epoch 21/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - accuracy: 0.9868 - loss: 0.0431 - val_accuracy: 0.9037 - val_loss: 0.3468\n",
            "Epoch 22/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 121ms/step - accuracy: 0.9794 - loss: 0.0658 - val_accuracy: 0.8593 - val_loss: 0.5655\n",
            "Epoch 23/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 0.9844 - loss: 0.0391 - val_accuracy: 0.9185 - val_loss: 0.2500\n",
            "Epoch 24/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.9830 - loss: 0.0433 - val_accuracy: 0.8963 - val_loss: 0.2484\n",
            "Epoch 25/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 112ms/step - accuracy: 0.9818 - loss: 0.0630 - val_accuracy: 0.9037 - val_loss: 0.3465\n",
            "Epoch 26/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 0.9858 - loss: 0.0437 - val_accuracy: 0.8519 - val_loss: 0.7024\n",
            "Epoch 27/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.9871 - loss: 0.0425 - val_accuracy: 0.8037 - val_loss: 0.9248\n",
            "Epoch 1/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.6128 - loss: 1.6583"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 402ms/step - accuracy: 0.6557 - loss: 1.6951 - val_accuracy: 0.8000 - val_loss: 0.5025\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - accuracy: 0.7948 - loss: 1.2427 - val_accuracy: 0.2000 - val_loss: 0.8955\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.6584 - loss: 0.9714 - val_accuracy: 0.2000 - val_loss: 8.3666\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.7509 - loss: 0.8297 - val_accuracy: 0.2000 - val_loss: 16.8519\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.8320 - loss: 0.6609 - val_accuracy: 0.2000 - val_loss: 23.5120\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.6190 - loss: 0.8187 - val_accuracy: 0.2000 - val_loss: 15.9007\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8077 - loss: 0.5646 - val_accuracy: 0.2000 - val_loss: 13.9964\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.7078 - loss: 0.6103 - val_accuracy: 0.2000 - val_loss: 11.6351\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7990 - loss: 0.5237 - val_accuracy: 0.2000 - val_loss: 9.5245\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8338 - loss: 0.5071 - val_accuracy: 0.2000 - val_loss: 9.1956\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.8629 - loss: 0.4445 - val_accuracy: 0.2000 - val_loss: 8.6656\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7066 - loss: 0.6353"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 245ms/step - accuracy: 0.7520 - loss: 0.7376 - val_accuracy: 0.9200 - val_loss: 0.3969\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 146ms/step - accuracy: 0.7825 - loss: 1.3524 - val_accuracy: 0.9200 - val_loss: 0.2811\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.9295 - loss: 0.9065 - val_accuracy: 0.9200 - val_loss: 0.3081\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - accuracy: 0.8711 - loss: 0.8967 - val_accuracy: 0.0800 - val_loss: 5.4956\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - accuracy: 0.8615 - loss: 0.8398 - val_accuracy: 0.0800 - val_loss: 16.2434\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 146ms/step - accuracy: 0.7655 - loss: 0.6282 - val_accuracy: 0.0800 - val_loss: 14.9025\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step - accuracy: 0.9222 - loss: 0.4456 - val_accuracy: 0.0800 - val_loss: 24.2244\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.8853 - loss: 0.3249 - val_accuracy: 0.0800 - val_loss: 35.8247\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 0.7510 - loss: 0.5118 - val_accuracy: 0.0800 - val_loss: 23.8395\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8645 - loss: 0.5062 - val_accuracy: 0.0800 - val_loss: 18.6249\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 0.8914 - loss: 0.3480 - val_accuracy: 0.0800 - val_loss: 17.0797\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9036 - loss: 0.3256 - val_accuracy: 0.0800 - val_loss: 16.8722\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.5029 - loss: 1.4707"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 124ms/step - accuracy: 0.5031 - loss: 1.4604 - val_accuracy: 0.3494 - val_loss: 22.0263\n",
            "Epoch 2/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 122ms/step - accuracy: 0.5356 - loss: 0.8172 - val_accuracy: 0.3494 - val_loss: 6.5850\n",
            "Epoch 3/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.5622 - loss: 0.8386 - val_accuracy: 0.3494 - val_loss: 2.2784\n",
            "Epoch 4/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 0.5569 - loss: 0.8271 - val_accuracy: 0.3494 - val_loss: 0.9740\n",
            "Epoch 5/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.5649 - loss: 0.8248"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - accuracy: 0.5648 - loss: 0.8243 - val_accuracy: 0.5279 - val_loss: 0.8118\n",
            "Epoch 6/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.5505 - loss: 0.8075 - val_accuracy: 0.5279 - val_loss: 0.7988\n",
            "Epoch 7/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.5738 - loss: 0.8127 - val_accuracy: 0.5279 - val_loss: 0.8078\n",
            "Epoch 8/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 138ms/step - accuracy: 0.5439 - loss: 0.8368 - val_accuracy: 0.3494 - val_loss: 0.8964\n",
            "Epoch 9/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.5151 - loss: 0.8251 - val_accuracy: 0.5279 - val_loss: 0.8080\n",
            "Epoch 10/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.5704 - loss: 0.8057 - val_accuracy: 0.5279 - val_loss: 0.7943\n",
            "Epoch 11/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - accuracy: 0.5777 - loss: 0.7800 - val_accuracy: 0.5279 - val_loss: 0.7899\n",
            "Epoch 12/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.5405 - loss: 0.7848"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - accuracy: 0.5403 - loss: 0.7851 - val_accuracy: 0.5762 - val_loss: 0.8026\n",
            "Epoch 13/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.5733 - loss: 0.7788 - val_accuracy: 0.5279 - val_loss: 0.7940\n",
            "Epoch 14/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.5496 - loss: 0.7748 - val_accuracy: 0.5279 - val_loss: 0.7951\n",
            "Epoch 15/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 129ms/step - accuracy: 0.5773 - loss: 0.7627 - val_accuracy: 0.5279 - val_loss: 0.7914\n",
            "Epoch 16/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.5695 - loss: 0.8008 - val_accuracy: 0.5279 - val_loss: 0.7913\n",
            "Epoch 17/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.5853 - loss: 0.7637 - val_accuracy: 0.5279 - val_loss: 0.7940\n",
            "Epoch 18/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.5527 - loss: 0.7899"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 142ms/step - accuracy: 0.5532 - loss: 0.7895 - val_accuracy: 0.6654 - val_loss: 0.7804\n",
            "Epoch 19/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.5953 - loss: 0.7323 - val_accuracy: 0.6171 - val_loss: 0.7712\n",
            "Epoch 20/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 0.6027 - loss: 0.7151 - val_accuracy: 0.5911 - val_loss: 0.7891\n",
            "Epoch 21/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - accuracy: 0.6580 - loss: 0.6843 - val_accuracy: 0.5762 - val_loss: 1.1293\n",
            "Epoch 22/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - accuracy: 0.5677 - loss: 0.7663 - val_accuracy: 0.5279 - val_loss: 0.7988\n",
            "Epoch 23/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - accuracy: 0.6096 - loss: 0.7293 - val_accuracy: 0.5390 - val_loss: 0.7429\n",
            "Epoch 24/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.6649 - loss: 0.6380"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 140ms/step - accuracy: 0.6652 - loss: 0.6371 - val_accuracy: 0.7918 - val_loss: 0.5765\n",
            "Epoch 25/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.7042 - loss: 0.6311 - val_accuracy: 0.5242 - val_loss: 2.2748\n",
            "Epoch 26/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.7286 - loss: 0.5313 - val_accuracy: 0.7175 - val_loss: 0.5648\n",
            "Epoch 27/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 134ms/step - accuracy: 0.7305 - loss: 0.5168 - val_accuracy: 0.5874 - val_loss: 0.6495\n",
            "Epoch 28/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.8081 - loss: 0.4520 - val_accuracy: 0.5242 - val_loss: 0.9764\n",
            "Epoch 29/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - accuracy: 0.8174 - loss: 0.4238 - val_accuracy: 0.5279 - val_loss: 3.6916\n",
            "Epoch 30/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 135ms/step - accuracy: 0.7468 - loss: 0.5308 - val_accuracy: 0.5279 - val_loss: 6.3405\n",
            "Epoch 31/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - accuracy: 0.8364 - loss: 0.3767 - val_accuracy: 0.5279 - val_loss: 8.3936\n",
            "Epoch 32/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - accuracy: 0.8371 - loss: 0.3909 - val_accuracy: 0.5279 - val_loss: 4.7427\n",
            "Epoch 33/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.8215 - loss: 0.3877 - val_accuracy: 0.6394 - val_loss: 0.8945\n",
            "Epoch 34/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.8634 - loss: 0.3349 - val_accuracy: 0.5316 - val_loss: 1.9848\n",
            "Epoch 35/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.8611 - loss: 0.3581 - val_accuracy: 0.5279 - val_loss: 1.4562\n",
            "Epoch 36/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.8850 - loss: 0.2978 - val_accuracy: 0.7658 - val_loss: 0.6387\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2344 - loss: 3.5086"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 234ms/step - accuracy: 0.2406 - loss: 4.1810 - val_accuracy: 0.3200 - val_loss: 2.2315\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - accuracy: 0.3471 - loss: 4.0229 - val_accuracy: 0.3200 - val_loss: 1.1806\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - accuracy: 0.5851 - loss: 1.5260 - val_accuracy: 0.3200 - val_loss: 2.5946\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 147ms/step - accuracy: 0.4235 - loss: 1.2256 - val_accuracy: 0.3200 - val_loss: 3.2327\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 0.4033 - loss: 1.2219 - val_accuracy: 0.3200 - val_loss: 3.3518\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.4927 - loss: 1.0144 - val_accuracy: 0.3200 - val_loss: 4.3320\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5307 - loss: 1.0103 - val_accuracy: 0.3200 - val_loss: 3.5510\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5090 - loss: 0.9709 - val_accuracy: 0.3200 - val_loss: 3.0501\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5837 - loss: 0.8810 - val_accuracy: 0.3200 - val_loss: 3.5108\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.4618 - loss: 1.1907 - val_accuracy: 0.3200 - val_loss: 3.7954\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5999 - loss: 0.9507 - val_accuracy: 0.3200 - val_loss: 2.9786\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.5652 - loss: 0.9218 - val_accuracy: 0.3200 - val_loss: 3.0198\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.6872 - loss: 1.1320"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 175ms/step - accuracy: 0.6918 - loss: 1.1185 - val_accuracy: 0.3485 - val_loss: 52.9916\n",
            "Epoch 2/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - accuracy: 0.8629 - loss: 0.4534 - val_accuracy: 0.3485 - val_loss: 29.2774\n",
            "Epoch 3/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.8636 - loss: 0.4198 - val_accuracy: 0.3485 - val_loss: 19.4156\n",
            "Epoch 4/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.8875 - loss: 0.2924 - val_accuracy: 0.3485 - val_loss: 6.2454\n",
            "Epoch 5/50\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9420 - loss: 0.1739"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.9411 - loss: 0.1760 - val_accuracy: 0.4470 - val_loss: 4.1481\n",
            "Epoch 6/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9014 - loss: 0.2746"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - accuracy: 0.9010 - loss: 0.2748 - val_accuracy: 0.7803 - val_loss: 0.6434\n",
            "Epoch 7/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9051 - loss: 0.2423"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 155ms/step - accuracy: 0.9063 - loss: 0.2401 - val_accuracy: 0.8258 - val_loss: 0.4066\n",
            "Epoch 8/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9030 - loss: 0.2900"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - accuracy: 0.9036 - loss: 0.2894 - val_accuracy: 0.8333 - val_loss: 0.4126\n",
            "Epoch 9/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9097 - loss: 0.2036"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.9100 - loss: 0.2032 - val_accuracy: 0.9167 - val_loss: 0.1996\n",
            "Epoch 10/50\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9474 - loss: 0.1556"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.9473 - loss: 0.1545 - val_accuracy: 0.9318 - val_loss: 0.1890\n",
            "Epoch 11/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9619 - loss: 0.1031 - val_accuracy: 0.8864 - val_loss: 0.1983\n",
            "Epoch 12/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 143ms/step - accuracy: 0.9765 - loss: 0.0544 - val_accuracy: 0.9318 - val_loss: 0.1665\n",
            "Epoch 13/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 130ms/step - accuracy: 0.9865 - loss: 0.0550 - val_accuracy: 0.9015 - val_loss: 0.1997\n",
            "Epoch 14/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 0.9719 - loss: 0.0863 - val_accuracy: 0.8712 - val_loss: 0.3841\n",
            "Epoch 15/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9894 - loss: 0.0319 - val_accuracy: 0.9318 - val_loss: 0.2145\n",
            "Epoch 16/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.9780 - loss: 0.0670 - val_accuracy: 0.8485 - val_loss: 0.5621\n",
            "Epoch 17/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 0.9883 - loss: 0.0373 - val_accuracy: 0.8939 - val_loss: 0.3202\n",
            "Epoch 18/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.9567 - loss: 0.1110 - val_accuracy: 0.9091 - val_loss: 0.2908\n",
            "Epoch 19/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - accuracy: 0.9820 - loss: 0.0579 - val_accuracy: 0.9167 - val_loss: 0.2344\n",
            "Epoch 20/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9874 - loss: 0.0381 - val_accuracy: 0.9015 - val_loss: 0.3869\n",
            "Epoch 21/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9789 - loss: 0.0856 - val_accuracy: 0.8333 - val_loss: 0.5504\n",
            "Epoch 22/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9853 - loss: 0.0457 - val_accuracy: 0.9015 - val_loss: 0.3472\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 340ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 186ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (25, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 234ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 178ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (25, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5868 - loss: 1.0193"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 236ms/step - accuracy: 0.6361 - loss: 0.9676 - val_accuracy: 0.8800 - val_loss: 1.0255\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8980 - loss: 0.4878 - val_accuracy: 0.8800 - val_loss: 0.3637\n",
            "Epoch 3/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 6.1818e-04 "
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 135ms/step - accuracy: 1.0000 - loss: 6.1878e-04 - val_accuracy: 0.9600 - val_loss: 0.2771\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 0.0075 - val_accuracy: 0.9600 - val_loss: 0.4159\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.9845 - loss: 0.0132 - val_accuracy: 0.8800 - val_loss: 0.5503\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 7.9343e-05 - val_accuracy: 0.8800 - val_loss: 0.5914\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 5.4343e-06 - val_accuracy: 0.8800 - val_loss: 0.5891\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 2.3658e-05 - val_accuracy: 0.8800 - val_loss: 0.5432\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 3.0091e-05 - val_accuracy: 0.8800 - val_loss: 0.4689\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 1.3674e-05 - val_accuracy: 0.8800 - val_loss: 0.3818\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 8.8141e-06 - val_accuracy: 0.8800 - val_loss: 0.2953\n",
            "Epoch 12/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 1.0534e-04 - val_accuracy: 0.8800 - val_loss: 0.2126\n",
            "Epoch 13/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 5.3616e-07 - val_accuracy: 0.9600 - val_loss: 0.1440\n",
            "Epoch 14/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.9600 - val_loss: 0.0359\n",
            "Epoch 15/50\n",
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 7.5758e-05"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 1.0000 - loss: 6.2206e-05 - val_accuracy: 1.0000 - val_loss: 0.0104\n",
            "Epoch 16/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 7.1985e-07 - val_accuracy: 1.0000 - val_loss: 0.0035\n",
            "Epoch 17/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 4.7663e-06 - val_accuracy: 1.0000 - val_loss: 0.0013\n",
            "Epoch 18/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 5.8865e-08 - val_accuracy: 1.0000 - val_loss: 5.3049e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 1.2283e-08 - val_accuracy: 1.0000 - val_loss: 2.3565e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 3.7278e-07 - val_accuracy: 1.0000 - val_loss: 1.1450e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 2.5361e-06 - val_accuracy: 1.0000 - val_loss: 6.1092e-05\n",
            "Epoch 22/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 3.3667e-07 - val_accuracy: 1.0000 - val_loss: 3.3624e-05\n",
            "Epoch 23/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 4.8458e-07 - val_accuracy: 1.0000 - val_loss: 1.9123e-05\n",
            "Epoch 24/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 2.5319e-07 - val_accuracy: 1.0000 - val_loss: 1.1255e-05\n",
            "Epoch 25/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 6.6751e-04 - val_accuracy: 1.0000 - val_loss: 5.5191e-06\n",
            "Epoch 26/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 2.1846e-07 - val_accuracy: 1.0000 - val_loss: 2.6074e-06\n",
            "Epoch 27/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 1.1667e-07 - val_accuracy: 1.0000 - val_loss: 1.4679e-06\n",
            "Epoch 28/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 1.7548e-08 - val_accuracy: 1.0000 - val_loss: 8.7965e-07\n",
            "Epoch 29/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 1.2520e-06 - val_accuracy: 1.0000 - val_loss: 5.5382e-07\n",
            "Epoch 30/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 1.9277e-05 - val_accuracy: 1.0000 - val_loss: 3.4489e-07\n",
            "Epoch 31/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 7.7008e-08 - val_accuracy: 1.0000 - val_loss: 2.2045e-07\n",
            "Epoch 32/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 2.6745e-07 - val_accuracy: 1.0000 - val_loss: 1.4771e-07\n",
            "Epoch 33/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - accuracy: 1.0000 - loss: 1.4415e-09 - val_accuracy: 1.0000 - val_loss: 1.0130e-07\n",
            "Epoch 34/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 157ms/step - accuracy: 1.0000 - loss: 2.7291e-08 - val_accuracy: 1.0000 - val_loss: 7.0414e-08\n",
            "Epoch 35/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 2.0231e-07 - val_accuracy: 1.0000 - val_loss: 5.0470e-08\n",
            "Epoch 36/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 6.4634e-08 - val_accuracy: 1.0000 - val_loss: 3.6072e-08\n",
            "Epoch 37/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 9.6909e-08 - val_accuracy: 1.0000 - val_loss: 2.6247e-08\n",
            "Epoch 38/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 8.3169e-07 - val_accuracy: 1.0000 - val_loss: 1.9413e-08\n",
            "Epoch 39/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 8.2005e-07 - val_accuracy: 1.0000 - val_loss: 1.4522e-08\n",
            "Epoch 40/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 6.0741e-09 - val_accuracy: 1.0000 - val_loss: 1.1131e-08\n",
            "Epoch 41/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 6.1514e-08 - val_accuracy: 1.0000 - val_loss: 8.5263e-09\n",
            "Epoch 42/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 2.1595e-08 - val_accuracy: 1.0000 - val_loss: 6.6256e-09\n",
            "Epoch 43/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 1.4005e-07 - val_accuracy: 1.0000 - val_loss: 5.2475e-09\n",
            "Epoch 44/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 1.8762e-08 - val_accuracy: 1.0000 - val_loss: 4.1831e-09\n",
            "Epoch 45/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 5.2688e-10\n",
            "Epoch 46/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 8.2346e-08 - val_accuracy: 1.0000 - val_loss: 1.4486e-10\n",
            "Epoch 47/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 3.1242e-08 - val_accuracy: 1.0000 - val_loss: 9.3046e-11\n",
            "Epoch 48/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 1.7128e-07 - val_accuracy: 1.0000 - val_loss: 7.1312e-11\n",
            "Epoch 49/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 2.3421e-08 - val_accuracy: 1.0000 - val_loss: 5.8133e-11\n",
            "Epoch 50/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 4.4944e-10 - val_accuracy: 1.0000 - val_loss: 4.9281e-11\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m3/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 246ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/50\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (25, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.4501 - loss: 1.8251"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 170ms/step - accuracy: 0.4538 - loss: 1.8034 - val_accuracy: 0.3534 - val_loss: 4.4043\n",
            "Epoch 2/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.7527 - loss: 0.6564 - val_accuracy: 0.3383 - val_loss: 4.9073\n",
            "Epoch 3/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8699 - loss: 0.3297"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - accuracy: 0.8687 - loss: 0.3335 - val_accuracy: 0.3609 - val_loss: 3.0754\n",
            "Epoch 4/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.8819 - loss: 0.2926 - val_accuracy: 0.3534 - val_loss: 4.3471\n",
            "Epoch 5/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.9043 - loss: 0.2786 - val_accuracy: 0.3609 - val_loss: 2.7380\n",
            "Epoch 6/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 143ms/step - accuracy: 0.9387 - loss: 0.1511 - val_accuracy: 0.3609 - val_loss: 4.3191\n",
            "Epoch 7/50\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9448 - loss: 0.1552"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - accuracy: 0.9449 - loss: 0.1545 - val_accuracy: 0.4060 - val_loss: 2.7195\n",
            "Epoch 8/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9486 - loss: 0.1377"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 98ms/step - accuracy: 0.9495 - loss: 0.1360 - val_accuracy: 0.5188 - val_loss: 1.3952\n",
            "Epoch 9/50\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9741 - loss: 0.0668"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - accuracy: 0.9738 - loss: 0.0682 - val_accuracy: 0.5338 - val_loss: 1.4781\n",
            "Epoch 10/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9712 - loss: 0.0739"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - accuracy: 0.9709 - loss: 0.0741 - val_accuracy: 0.6617 - val_loss: 0.8446\n",
            "Epoch 11/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - accuracy: 0.9694 - loss: 0.0662 - val_accuracy: 0.5038 - val_loss: 1.1453\n",
            "Epoch 12/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 135ms/step - accuracy: 0.9744 - loss: 0.0755 - val_accuracy: 0.4211 - val_loss: 2.2235\n",
            "Epoch 13/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 150ms/step - accuracy: 0.9700 - loss: 0.0658 - val_accuracy: 0.5038 - val_loss: 1.6154\n",
            "Epoch 14/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9819 - loss: 0.0382"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 100ms/step - accuracy: 0.9818 - loss: 0.0386 - val_accuracy: 0.8421 - val_loss: 0.3708\n",
            "Epoch 15/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9907 - loss: 0.0387 - val_accuracy: 0.7744 - val_loss: 0.4594\n",
            "Epoch 16/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - accuracy: 0.9765 - loss: 0.0782 - val_accuracy: 0.8271 - val_loss: 0.4472\n",
            "Epoch 17/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 0.9870 - loss: 0.0455 - val_accuracy: 0.8195 - val_loss: 0.4581\n",
            "Epoch 18/50\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9925 - loss: 0.0232"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.9923 - loss: 0.0237 - val_accuracy: 0.8571 - val_loss: 0.4373\n",
            "Epoch 19/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9858 - loss: 0.0315"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - accuracy: 0.9859 - loss: 0.0311 - val_accuracy: 0.8722 - val_loss: 0.4412\n",
            "Epoch 20/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 117ms/step - accuracy: 0.9932 - loss: 0.0169 - val_accuracy: 0.7820 - val_loss: 0.7527\n",
            "Epoch 21/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9940 - loss: 0.0341 - val_accuracy: 0.8571 - val_loss: 0.4999\n",
            "Epoch 22/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.9739 - loss: 0.0736 - val_accuracy: 0.8421 - val_loss: 0.6204\n",
            "Epoch 23/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9946 - loss: 0.0304 - val_accuracy: 0.7444 - val_loss: 1.3708\n",
            "Epoch 24/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 0.9785 - loss: 0.0438 - val_accuracy: 0.8271 - val_loss: 0.5124\n",
            "Epoch 1/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9027 - loss: 0.2881"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 132ms/step - accuracy: 0.9045 - loss: 0.2839 - val_accuracy: 0.1926 - val_loss: 29.3858\n",
            "Epoch 2/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.9926 - loss: 0.1000 - val_accuracy: 0.1926 - val_loss: 9.2566\n",
            "Epoch 3/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9966 - loss: 0.0081"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - accuracy: 0.9966 - loss: 0.0082 - val_accuracy: 1.0000 - val_loss: 2.0994e-06\n",
            "Epoch 4/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 1.9492e-04 - val_accuracy: 1.0000 - val_loss: 1.6513e-07\n",
            "Epoch 5/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 3.1211e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 114ms/step - accuracy: 0.9998 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 6.5766e-06\n",
            "Epoch 7/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 0.9945 - loss: 0.0639 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 3.3378e-04 - val_accuracy: 1.0000 - val_loss: 3.9736e-08\n",
            "Epoch 9/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 1.4926e-05 - val_accuracy: 1.0000 - val_loss: 1.3245e-08\n",
            "Epoch 10/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 113ms/step - accuracy: 1.0000 - loss: 1.3670e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 1.6796e-04 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 12/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 5.6449e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 13/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 1.0192e-05 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 14/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 4.3864e-04 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 15/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 3.9774e-07 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 1/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8709 - loss: 0.2872"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 107ms/step - accuracy: 0.8735 - loss: 0.2817 - val_accuracy: 0.2305 - val_loss: 12.8910\n",
            "Epoch 2/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9970 - loss: 0.0178"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - accuracy: 0.9970 - loss: 0.0178 - val_accuracy: 0.9071 - val_loss: 0.2537\n",
            "Epoch 3/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9960 - loss: 0.0275"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - accuracy: 0.9961 - loss: 0.0271 - val_accuracy: 1.0000 - val_loss: 0.0155\n",
            "Epoch 4/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 5.9333e-04 - val_accuracy: 1.0000 - val_loss: 0.0033\n",
            "Epoch 5/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 1.8032e-04 - val_accuracy: 1.0000 - val_loss: 7.6062e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 2.9763e-04 - val_accuracy: 1.0000 - val_loss: 2.6218e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 2.4624e-04 - val_accuracy: 1.0000 - val_loss: 2.3367e-05\n",
            "Epoch 8/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 0.9972 - loss: 0.0029 - val_accuracy: 1.0000 - val_loss: 1.0337e-05\n",
            "Epoch 9/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 1.0163e-04 - val_accuracy: 1.0000 - val_loss: 2.4507e-06\n",
            "Epoch 10/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 4.1836e-05 - val_accuracy: 1.0000 - val_loss: 8.7213e-07\n",
            "Epoch 11/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 6.3173e-05 - val_accuracy: 1.0000 - val_loss: 3.3503e-07\n",
            "Epoch 12/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 123ms/step - accuracy: 1.0000 - loss: 6.3991e-05 - val_accuracy: 1.0000 - val_loss: 7.9768e-08\n",
            "Epoch 13/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 6.6999e-06 - val_accuracy: 1.0000 - val_loss: 1.7726e-08\n",
            "Epoch 14/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 4.7247e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 15/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 7.8018e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 16/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 111ms/step - accuracy: 1.0000 - loss: 6.8528e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 17/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 9.0705e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 18/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 1.4239e-05 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 19/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 1.1194e-05 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 20/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 1.9549e-05 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 21/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 1.2045e-05 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 22/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - accuracy: 1.0000 - loss: 4.2639e-05 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 23/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 2.8465e-06 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 24/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 5.1855e-05 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/losses/losses.py:33: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 169ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 2/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 3/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 4/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 5/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 6/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 7/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 8/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 9/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 10/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/ops/nn.py:907: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9000 - loss: 0.3213"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 120ms/step - accuracy: 0.9033 - loss: 0.3110 - val_accuracy: 0.7197 - val_loss: 0.8902\n",
            "Epoch 2/50\n",
            "\u001b[1m16/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 7.0730e-04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 7.1363e-04 - val_accuracy: 0.8636 - val_loss: 0.8445\n",
            "Epoch 3/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 8.9238e-06 - val_accuracy: 0.8636 - val_loss: 0.3208\n",
            "Epoch 4/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 1.0000 - loss: 2.6439e-05"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 1.0000 - loss: 2.5889e-05 - val_accuracy: 1.0000 - val_loss: 0.0084\n",
            "Epoch 5/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 113ms/step - accuracy: 1.0000 - loss: 9.5407e-06 - val_accuracy: 1.0000 - val_loss: 9.0939e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 8.0931e-05 - val_accuracy: 1.0000 - val_loss: 9.5694e-05\n",
            "Epoch 7/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 1.3150e-06 - val_accuracy: 1.0000 - val_loss: 2.0594e-05\n",
            "Epoch 8/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 1.6025e-06 - val_accuracy: 1.0000 - val_loss: 6.5421e-06\n",
            "Epoch 9/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 3.7502e-05 - val_accuracy: 1.0000 - val_loss: 2.1737e-06\n",
            "Epoch 10/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 1.7517e-05 - val_accuracy: 1.0000 - val_loss: 2.2453e-06\n",
            "Epoch 11/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 132ms/step - accuracy: 1.0000 - loss: 5.0051e-05 - val_accuracy: 1.0000 - val_loss: 5.1539e-07\n",
            "Epoch 12/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 1.8119e-06 - val_accuracy: 1.0000 - val_loss: 1.9377e-07\n",
            "Epoch 13/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 3.3398e-07 - val_accuracy: 1.0000 - val_loss: 8.8376e-08\n",
            "Epoch 14/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 6.2449e-04 - val_accuracy: 1.0000 - val_loss: 3.5559e-08\n",
            "Epoch 15/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 4.2870e-07 - val_accuracy: 1.0000 - val_loss: 3.5114e-08\n",
            "Epoch 16/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 145ms/step - accuracy: 1.0000 - loss: 5.5623e-06 - val_accuracy: 1.0000 - val_loss: 1.9895e-08\n",
            "Epoch 17/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 137ms/step - accuracy: 1.0000 - loss: 1.9991e-06 - val_accuracy: 1.0000 - val_loss: 1.0198e-08\n",
            "Epoch 18/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 5.7970e-06 - val_accuracy: 1.0000 - val_loss: 5.8108e-09\n",
            "Epoch 19/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 5.9949e-06 - val_accuracy: 1.0000 - val_loss: 3.7450e-09\n",
            "Epoch 20/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 4.2120e-07 - val_accuracy: 1.0000 - val_loss: 2.2446e-09\n",
            "Epoch 21/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 6.4531e-08 - val_accuracy: 1.0000 - val_loss: 1.2198e-09\n",
            "Epoch 22/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 2.6060e-05 - val_accuracy: 1.0000 - val_loss: 1.8665e-10\n",
            "Epoch 23/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 1.2838e-06 - val_accuracy: 1.0000 - val_loss: 8.1715e-11\n",
            "Epoch 24/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 5.6972e-08 - val_accuracy: 1.0000 - val_loss: 5.0801e-11\n",
            "Epoch 25/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 8.7636e-08 - val_accuracy: 1.0000 - val_loss: 3.6955e-11\n",
            "Epoch 26/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 1.3960e-07 - val_accuracy: 1.0000 - val_loss: 2.7881e-11\n",
            "Epoch 27/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 5.6651e-07 - val_accuracy: 1.0000 - val_loss: 2.0768e-11\n",
            "Epoch 28/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 9.6422e-06 - val_accuracy: 1.0000 - val_loss: 1.4127e-11\n",
            "Epoch 29/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 1.5542e-06 - val_accuracy: 1.0000 - val_loss: 1.0831e-11\n",
            "Epoch 30/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 9.1223e-06 - val_accuracy: 1.0000 - val_loss: 1.2888e-11\n",
            "Epoch 31/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 1.1397e-07 - val_accuracy: 1.0000 - val_loss: 1.3402e-11\n",
            "Epoch 32/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 4.4992e-08 - val_accuracy: 1.0000 - val_loss: 1.1512e-11\n",
            "Epoch 33/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 1.2809e-06 - val_accuracy: 1.0000 - val_loss: 9.5557e-12\n",
            "Epoch 34/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 1.2336e-05 - val_accuracy: 1.0000 - val_loss: 4.4733e-12\n",
            "Epoch 35/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 2.3474e-07 - val_accuracy: 1.0000 - val_loss: 3.4522e-12\n",
            "Epoch 36/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 2.0587e-07 - val_accuracy: 1.0000 - val_loss: 2.9055e-12\n",
            "Epoch 37/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 5.3154e-08 - val_accuracy: 1.0000 - val_loss: 2.5300e-12\n",
            "Epoch 38/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 2.0927e-08 - val_accuracy: 1.0000 - val_loss: 2.2792e-12\n",
            "Epoch 39/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 129ms/step - accuracy: 1.0000 - loss: 8.1699e-06 - val_accuracy: 1.0000 - val_loss: 1.4672e-12\n",
            "Epoch 40/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 155ms/step - accuracy: 1.0000 - loss: 2.0396e-07 - val_accuracy: 1.0000 - val_loss: 9.7960e-13\n",
            "Epoch 41/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 3.2906e-08 - val_accuracy: 1.0000 - val_loss: 8.7811e-13\n",
            "Epoch 42/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 130ms/step - accuracy: 1.0000 - loss: 1.1262e-08 - val_accuracy: 1.0000 - val_loss: 8.1989e-13\n",
            "Epoch 43/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 2.3851e-07 - val_accuracy: 1.0000 - val_loss: 7.7461e-13\n",
            "Epoch 44/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 3.0065e-07 - val_accuracy: 1.0000 - val_loss: 7.3148e-13\n",
            "Epoch 45/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - accuracy: 1.0000 - loss: 7.7223e-08 - val_accuracy: 1.0000 - val_loss: 6.9395e-13\n",
            "Epoch 46/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 141ms/step - accuracy: 1.0000 - loss: 4.2489e-07 - val_accuracy: 1.0000 - val_loss: 6.6494e-13\n",
            "Epoch 47/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 1.1065e-05 - val_accuracy: 1.0000 - val_loss: 4.9548e-13\n",
            "Epoch 48/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 5.0847e-07 - val_accuracy: 1.0000 - val_loss: 4.2674e-13\n",
            "Epoch 49/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 2.6725e-08 - val_accuracy: 1.0000 - val_loss: 3.9588e-13\n",
            "Epoch 50/50\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 3.4629e-08 - val_accuracy: 1.0000 - val_loss: 3.8053e-13\n",
            "Epoch 1/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6496 - loss: 1.2818"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 101ms/step - accuracy: 0.6544 - loss: 1.2627 - val_accuracy: 0.3558 - val_loss: 22.4288\n",
            "Epoch 2/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.8442 - loss: 0.4352"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.8441 - loss: 0.4354 - val_accuracy: 0.4457 - val_loss: 4.8751\n",
            "Epoch 3/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.8805 - loss: 0.2781"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 126ms/step - accuracy: 0.8805 - loss: 0.2785 - val_accuracy: 0.6404 - val_loss: 1.8075\n",
            "Epoch 4/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9238 - loss: 0.2304"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 108ms/step - accuracy: 0.9234 - loss: 0.2299 - val_accuracy: 0.8165 - val_loss: 0.5730\n",
            "Epoch 5/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9141 - loss: 0.2201"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - accuracy: 0.9139 - loss: 0.2216 - val_accuracy: 0.8876 - val_loss: 0.2920\n",
            "Epoch 6/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9267 - loss: 0.1870 - val_accuracy: 0.8876 - val_loss: 0.3070\n",
            "Epoch 7/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 117ms/step - accuracy: 0.9405 - loss: 0.1688 - val_accuracy: 0.8801 - val_loss: 0.4417\n",
            "Epoch 8/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9368 - loss: 0.1834"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 114ms/step - accuracy: 0.9371 - loss: 0.1823 - val_accuracy: 0.9176 - val_loss: 0.2728\n",
            "Epoch 9/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9284 - loss: 0.2263 - val_accuracy: 0.8689 - val_loss: 0.4031\n",
            "Epoch 10/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - accuracy: 0.9414 - loss: 0.1662 - val_accuracy: 0.8577 - val_loss: 0.3402\n",
            "Epoch 11/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 132ms/step - accuracy: 0.9615 - loss: 0.1050 - val_accuracy: 0.8539 - val_loss: 0.2955\n",
            "Epoch 12/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.9785 - loss: 0.0602 - val_accuracy: 0.8652 - val_loss: 0.4114\n",
            "Epoch 13/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9697 - loss: 0.0908 - val_accuracy: 0.7640 - val_loss: 0.9576\n",
            "Epoch 14/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.9571 - loss: 0.1228 - val_accuracy: 0.8764 - val_loss: 0.3744\n",
            "Epoch 15/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9748 - loss: 0.0719 - val_accuracy: 0.8839 - val_loss: 0.3555\n",
            "Epoch 16/50\n",
            "\u001b[1m33/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9796 - loss: 0.0655"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - accuracy: 0.9793 - loss: 0.0669 - val_accuracy: 0.9213 - val_loss: 0.3192\n",
            "Epoch 17/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 129ms/step - accuracy: 0.9742 - loss: 0.0670 - val_accuracy: 0.8352 - val_loss: 0.6095\n",
            "Epoch 18/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - accuracy: 0.9576 - loss: 0.1075 - val_accuracy: 0.9064 - val_loss: 0.2657\n",
            "Epoch 19/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9843 - loss: 0.0496 - val_accuracy: 0.9026 - val_loss: 0.5214\n",
            "Epoch 20/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 114ms/step - accuracy: 0.9783 - loss: 0.0823 - val_accuracy: 0.8876 - val_loss: 0.4202\n",
            "Epoch 21/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - accuracy: 0.9797 - loss: 0.0661 - val_accuracy: 0.8277 - val_loss: 0.6704\n",
            "Epoch 22/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 89ms/step - accuracy: 0.9830 - loss: 0.0587 - val_accuracy: 0.8839 - val_loss: 0.5189\n",
            "Epoch 23/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - accuracy: 0.9590 - loss: 0.1290 - val_accuracy: 0.9101 - val_loss: 0.2829\n",
            "Epoch 24/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 110ms/step - accuracy: 0.9834 - loss: 0.0437 - val_accuracy: 0.9101 - val_loss: 0.4525\n",
            "Epoch 25/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - accuracy: 0.9832 - loss: 0.0522 - val_accuracy: 0.8839 - val_loss: 0.5687\n",
            "Epoch 26/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 119ms/step - accuracy: 0.9828 - loss: 0.0382 - val_accuracy: 0.9139 - val_loss: 0.3127\n",
            "Epoch 27/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step - accuracy: 0.9853 - loss: 0.0472 - val_accuracy: 0.9064 - val_loss: 0.3604\n",
            "Epoch 28/50\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 88ms/step - accuracy: 0.9958 - loss: 0.0202 - val_accuracy: 0.9101 - val_loss: 0.4191\n",
            "Epoch 1/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9155 - loss: 0.2852"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 262ms/step - accuracy: 0.9215 - loss: 0.2644 - val_accuracy: 0.3091 - val_loss: 9.2480\n",
            "Epoch 2/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 171ms/step - accuracy: 0.9881 - loss: 0.1207 - val_accuracy: 0.3091 - val_loss: 8.7620\n",
            "Epoch 3/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - accuracy: 0.9989 - loss: 0.0104 - val_accuracy: 0.3091 - val_loss: 6.4094\n",
            "Epoch 4/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 1.8203e-05 - val_accuracy: 0.3091 - val_loss: 2.7782\n",
            "Epoch 5/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 8.6814e-05 - val_accuracy: 0.3091 - val_loss: 0.5073\n",
            "Epoch 6/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0015"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 113ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 0.0578\n",
            "Epoch 7/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 8.3932e-05 - val_accuracy: 1.0000 - val_loss: 0.0084\n",
            "Epoch 8/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 8.2262e-06 - val_accuracy: 1.0000 - val_loss: 0.0011\n",
            "Epoch 9/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - accuracy: 1.0000 - loss: 4.0153e-06 - val_accuracy: 1.0000 - val_loss: 1.7810e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 105ms/step - accuracy: 1.0000 - loss: 4.5499e-06 - val_accuracy: 1.0000 - val_loss: 3.9347e-05\n",
            "Epoch 11/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - accuracy: 1.0000 - loss: 1.8856e-06 - val_accuracy: 1.0000 - val_loss: 1.0784e-05\n",
            "Epoch 12/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 2.7177e-07 - val_accuracy: 1.0000 - val_loss: 3.4816e-06\n",
            "Epoch 13/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 6.8415e-07 - val_accuracy: 1.0000 - val_loss: 1.2982e-06\n",
            "Epoch 14/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - accuracy: 1.0000 - loss: 2.5391e-07 - val_accuracy: 1.0000 - val_loss: 5.5779e-07\n",
            "Epoch 15/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 3.5167e-07 - val_accuracy: 1.0000 - val_loss: 2.7479e-07\n",
            "Epoch 16/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 176ms/step - accuracy: 1.0000 - loss: 2.1015e-06 - val_accuracy: 1.0000 - val_loss: 1.4725e-07\n",
            "Epoch 17/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 6.4637e-08 - val_accuracy: 1.0000 - val_loss: 9.1264e-08\n",
            "Epoch 18/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 2.9872e-06 - val_accuracy: 1.0000 - val_loss: 5.8081e-08\n",
            "Epoch 19/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 1.3244e-07 - val_accuracy: 1.0000 - val_loss: 3.8162e-08\n",
            "Epoch 20/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 2.7989e-07 - val_accuracy: 1.0000 - val_loss: 2.6574e-08\n",
            "Epoch 21/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 4.1284e-07 - val_accuracy: 1.0000 - val_loss: 1.9579e-08\n",
            "Epoch 22/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 1.2499e-06 - val_accuracy: 1.0000 - val_loss: 1.5312e-08\n",
            "Epoch 23/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 1.8044e-06 - val_accuracy: 1.0000 - val_loss: 1.2744e-08\n",
            "Epoch 24/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 4.4681e-06 - val_accuracy: 1.0000 - val_loss: 1.1553e-08\n",
            "Epoch 25/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 6.2772e-06 - val_accuracy: 1.0000 - val_loss: 1.1322e-08\n",
            "Epoch 26/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 97ms/step - accuracy: 1.0000 - loss: 2.9335e-07 - val_accuracy: 1.0000 - val_loss: 1.2083e-08\n",
            "Epoch 27/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 159ms/step - accuracy: 1.0000 - loss: 1.5696e-06 - val_accuracy: 1.0000 - val_loss: 1.3466e-08\n",
            "Epoch 28/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 160ms/step - accuracy: 1.0000 - loss: 7.6644e-07 - val_accuracy: 1.0000 - val_loss: 1.5699e-08\n",
            "Epoch 29/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy: 1.0000 - loss: 2.2226e-06 - val_accuracy: 1.0000 - val_loss: 1.3352e-08\n",
            "Epoch 30/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - accuracy: 1.0000 - loss: 3.3555e-08 - val_accuracy: 1.0000 - val_loss: 1.0922e-08\n",
            "Epoch 31/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 3.5015e-06 - val_accuracy: 1.0000 - val_loss: 1.0367e-08\n",
            "Epoch 32/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 1.7099e-07 - val_accuracy: 1.0000 - val_loss: 1.0651e-08\n",
            "Epoch 33/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 1.2100e-07 - val_accuracy: 1.0000 - val_loss: 1.1212e-08\n",
            "Epoch 34/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 4.8010e-08 - val_accuracy: 1.0000 - val_loss: 1.2067e-08\n",
            "Epoch 35/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 7.9722e-07 - val_accuracy: 1.0000 - val_loss: 1.3270e-08\n",
            "Epoch 36/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 4.1055e-07 - val_accuracy: 1.0000 - val_loss: 1.4575e-08\n",
            "Epoch 37/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 1.3948e-06 - val_accuracy: 1.0000 - val_loss: 1.4718e-08\n",
            "Epoch 38/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 1.6238e-06 - val_accuracy: 1.0000 - val_loss: 1.4467e-08\n",
            "Epoch 39/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 1.0000 - loss: 2.0359e-07 - val_accuracy: 1.0000 - val_loss: 1.3744e-08\n",
            "Epoch 40/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 1.1813e-07 - val_accuracy: 1.0000 - val_loss: 1.3685e-08\n",
            "Epoch 41/50\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 162ms/step - accuracy: 1.0000 - loss: 9.2332e-08 - val_accuracy: 1.0000 - val_loss: 1.4020e-08\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'labels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6762cc2c9824>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1541\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-6762cc2c9824>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1521\u001b[0m     \u001b[0;31m# Process all log files (limit to max_files if you want to process only a subset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m     \u001b[0;31m# Set parallel=True if you have multiple processors and want faster processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1523\u001b[0;31m     \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_log_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_extension\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAnalysis complete! The following results have been generated:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6762cc2c9824>\u001b[0m in \u001b[0;36mprocess_log_files\u001b[0;34m(self, file_extension, min_frames)\u001b[0m\n\u001b[1;32m   1298\u001b[0m                     \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentify_signal_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcan_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompare_traditional\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m                         \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_comparative_visualizations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcan_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                 \u001b[0;31m# Save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6762cc2c9824>\u001b[0m in \u001b[0;36mgenerate_comparative_visualizations\u001b[0;34m(self, can_id)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;31m# Create the radar chart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperformance_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m         \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mradar_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'polygon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6762cc2c9824>\u001b[0m in \u001b[0;36mradar_factory\u001b[0;34m(num_vars, frame)\u001b[0m\n\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m             \u001b[0;31m# Draw one axis per variable + add labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;31m# Draw ylabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import logging\n",
        "import pickle\n",
        "from typing import List, Dict, Tuple, Optional, Set, Union\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class CANFrame:\n",
        "    \"\"\"Represents a single CAN frame with ID and payload.\"\"\"\n",
        "\n",
        "    def __init__(self, can_id: str, payload: str, timestamp: float = 0.0):\n",
        "        \"\"\"\n",
        "        Initialize a CAN frame.\n",
        "\n",
        "        Args:\n",
        "            can_id: The CAN ID in hexadecimal format (e.g. '0x123')\n",
        "            payload: The payload in hexadecimal format (e.g. '00FF34A2')\n",
        "            timestamp: The timestamp when the frame was captured\n",
        "        \"\"\"\n",
        "        self.can_id = can_id\n",
        "        self.payload = payload\n",
        "        self.timestamp = timestamp\n",
        "\n",
        "        # Convert payload to binary representation\n",
        "        try:\n",
        "            self.binary_payload = bin(int(payload, 16))[2:].zfill(len(payload) * 4)\n",
        "        except ValueError:\n",
        "            self.binary_payload = '0' * 64  # Default to 64 bits for invalid payloads\n",
        "\n",
        "    def get_bit(self, bit_position: int) -> int:\n",
        "        \"\"\"Get the value of a specific bit in the payload.\"\"\"\n",
        "        if bit_position < len(self.binary_payload):\n",
        "            return int(self.binary_payload[bit_position])\n",
        "        return 0\n",
        "\n",
        "    def get_byte(self, byte_position: int) -> int:\n",
        "        \"\"\"Get the value of a specific byte in the payload.\"\"\"\n",
        "        start_bit = byte_position * 8\n",
        "        end_bit = start_bit + 8\n",
        "        if start_bit < len(self.binary_payload):\n",
        "            byte_str = self.binary_payload[start_bit:min(end_bit, len(self.binary_payload))]\n",
        "            byte_str = byte_str.ljust(8, '0')\n",
        "            return int(byte_str, 2)\n",
        "        return 0\n",
        "\n",
        "    def get_bits(self, start_bit: int, num_bits: int) -> int:\n",
        "        \"\"\"Get a range of bits as a single integer value.\"\"\"\n",
        "        end_bit = start_bit + num_bits\n",
        "        if start_bit < len(self.binary_payload):\n",
        "            bits_str = self.binary_payload[start_bit:min(end_bit, len(self.binary_payload))]\n",
        "            bits_str = bits_str.ljust(num_bits, '0')\n",
        "            return int(bits_str, 2)\n",
        "        return 0\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"CANFrame(id={self.can_id}, payload={self.payload}, ts={self.timestamp})\"\n",
        "\n",
        "\n",
        "class CANTrace:\n",
        "    \"\"\"Collection of CAN frames representing a vehicle trace.\"\"\"\n",
        "\n",
        "    def __init__(self, frames: Optional[List[CANFrame]] = None):\n",
        "        \"\"\"\n",
        "        Initialize a CAN trace with optional frames.\n",
        "\n",
        "        Args:\n",
        "            frames: List of CANFrame objects\n",
        "        \"\"\"\n",
        "        self.frames = frames or []\n",
        "        self.frames_by_id = defaultdict(list)\n",
        "        self._update_frame_index()\n",
        "\n",
        "    def _update_frame_index(self):\n",
        "        \"\"\"Update the index of frames by CAN ID.\"\"\"\n",
        "        self.frames_by_id = defaultdict(list)\n",
        "        for frame in self.frames:\n",
        "            self.frames_by_id[frame.can_id].append(frame)\n",
        "\n",
        "    def add_frame(self, frame: CANFrame):\n",
        "        \"\"\"Add a single frame to the trace.\"\"\"\n",
        "        self.frames.append(frame)\n",
        "        self.frames_by_id[frame.can_id].append(frame)\n",
        "\n",
        "    def add_frames(self, frames: List[CANFrame]):\n",
        "        \"\"\"Add multiple frames to the trace.\"\"\"\n",
        "        self.frames.extend(frames)\n",
        "        self._update_frame_index()\n",
        "\n",
        "    def get_frames_by_id(self, can_id: str) -> List[CANFrame]:\n",
        "        \"\"\"Get all frames with a specific CAN ID.\"\"\"\n",
        "        return self.frames_by_id.get(can_id, [])\n",
        "\n",
        "    def get_unique_ids(self) -> Set[str]:\n",
        "        \"\"\"Get the set of unique CAN IDs in the trace.\"\"\"\n",
        "        return set(self.frames_by_id.keys())\n",
        "\n",
        "    @classmethod\n",
        "    def from_log(cls, filepath):\n",
        "        \"\"\"Create a CANTrace object from a log file.\"\"\"\n",
        "        trace = cls()\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue  # skip empty lines\n",
        "\n",
        "                if line.startswith('('):\n",
        "                    end_idx = line.find(')')\n",
        "                    ts_str = line[1:end_idx]\n",
        "                    timestamp = float(ts_str)\n",
        "                else:\n",
        "                    continue  # skip invalid format lines\n",
        "\n",
        "                remaining = line[end_idx+1:].strip()\n",
        "                parts = remaining.split()\n",
        "                if len(parts) < 2:\n",
        "                    continue  # malformed line\n",
        "\n",
        "                id_payload = parts[1] if '#' in parts[1] else parts[0]\n",
        "                if '#' not in id_payload:\n",
        "                    continue  # invalid payload\n",
        "\n",
        "                id_str, data_str = id_payload.split('#', 1)\n",
        "                try:\n",
        "                    can_id = int(id_str, 16)\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "                data_bytes = [int(data_str[i:i+2], 16) for i in range(0, len(data_str), 2)]\n",
        "\n",
        "                # Create CANFrame with matching signature\n",
        "                frame = CANFrame(can_id=f\"{can_id:X}\", payload=''.join(f\"{byte:02X}\" for byte in data_bytes), timestamp=timestamp)\n",
        "                trace.frames.append(frame)\n",
        "\n",
        "        trace._update_frame_index()\n",
        "        return trace\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.frames)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"CANTrace(frames={len(self.frames)}, unique_ids={len(self.get_unique_ids())})\"\n",
        "\n",
        "\n",
        "class DeepCANAnalyzer:\n",
        "    \"\"\"\n",
        "    Advanced CAN signal analyzer using deep convolutional neural networks\n",
        "    to identify signal patterns and detect anomalies in CAN traffic.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, can_trace: CANTrace, output_dir: str = \"deepcan_results\",\n",
        "                compare_traditional: bool = True):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer with a CAN trace.\n",
        "\n",
        "        Args:\n",
        "            can_trace: A CANTrace object containing the frames to analyze\n",
        "            output_dir: Directory to save results\n",
        "            compare_traditional: Whether to also run traditional analysis for comparison\n",
        "        \"\"\"\n",
        "        self.can_trace = can_trace\n",
        "        self.output_dir = output_dir\n",
        "        self.signals_by_id = {}  # Dict of signals by CAN ID\n",
        "        self.id_sequences = {}  # ID sequences for CNN input\n",
        "        self.payload_matrices = {}  # Payload matrices for CNN input\n",
        "        self.models = {}  # Trained models for each CAN ID\n",
        "        self.labels = {}  # Signal labels/classifications\n",
        "        self.compare_traditional = compare_traditional\n",
        "        self.traditional_results = {}  # Results from traditional methods for comparison\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        os.makedirs(os.path.join(output_dir, \"comparative_analysis\"), exist_ok=True)\n",
        "\n",
        "        logger.info(f\"Initialized DeepCANAnalyzer with {len(can_trace)} frames and {len(can_trace.get_unique_ids())} unique IDs\")\n",
        "\n",
        "    def extract_signal_frames(self, window_size: int = 100, stride: int = 10):\n",
        "        \"\"\"\n",
        "        Extract signal frames for CNN analysis from the CAN trace.\n",
        "\n",
        "        Args:\n",
        "            window_size: Number of consecutive frames to include in each sample\n",
        "            stride: Stride between consecutive samples\n",
        "        \"\"\"\n",
        "        logger.info(f\"Extracting signal frames with window size {window_size} and stride {stride}\")\n",
        "\n",
        "        unique_ids = self.can_trace.get_unique_ids()\n",
        "\n",
        "        for can_id in tqdm(unique_ids, desc=\"Processing CAN IDs\"):\n",
        "            frames = self.can_trace.get_frames_by_id(can_id)\n",
        "\n",
        "            if len(frames) < window_size:\n",
        "                logger.warning(f\"Skipping CAN ID {can_id}: insufficient frames ({len(frames)} < {window_size})\")\n",
        "                continue\n",
        "\n",
        "            # Create sequence matrices for this CAN ID\n",
        "            sequences = []\n",
        "            payloads = []\n",
        "\n",
        "            for i in range(0, len(frames) - window_size + 1, stride):\n",
        "                # Get window of frames\n",
        "                window_frames = frames[i:i+window_size]\n",
        "\n",
        "                # Extract ID sequence\n",
        "                id_seq = np.zeros((window_size, 1))\n",
        "                for j, frame in enumerate(window_frames):\n",
        "                    id_seq[j] = int(frame.can_id, 16)  # Convert hex ID to int\n",
        "\n",
        "                # Extract payload matrix (8 bytes x window_size frames)\n",
        "                payload_matrix = np.zeros((window_size, 8))  # 8 bytes per CAN frame\n",
        "                for j, frame in enumerate(window_frames):\n",
        "                    for k in range(8):  # Process each byte\n",
        "                        payload_matrix[j, k] = frame.get_byte(k)\n",
        "\n",
        "                sequences.append(id_seq)\n",
        "                payloads.append(payload_matrix)\n",
        "\n",
        "            if sequences:\n",
        "                self.id_sequences[can_id] = np.array(sequences)\n",
        "                self.payload_matrices[can_id] = np.array(payloads)\n",
        "                logger.info(f\"CAN ID {can_id}: Created {len(sequences)} samples\")\n",
        "            else:\n",
        "                logger.warning(f\"No sequences generated for CAN ID {can_id}\")\n",
        "\n",
        "    def _create_signal_labels(self, can_id: str, cluster_count: int = 3):\n",
        "        \"\"\"\n",
        "        Create initial signal classification labels for supervised learning.\n",
        "        This is a simplified approach - in practice, would need domain knowledge or\n",
        "        unsupervised learning to create accurate labels.\n",
        "\n",
        "        Args:\n",
        "            can_id: CAN ID to create labels for\n",
        "            cluster_count: Number of signal types to identify\n",
        "        \"\"\"\n",
        "        if can_id not in self.payload_matrices:\n",
        "            logger.warning(f\"No payload matrices for CAN ID {can_id}\")\n",
        "            return None\n",
        "\n",
        "        payloads = self.payload_matrices[can_id]\n",
        "\n",
        "        try:\n",
        "            # Simple heuristic: use byte activity patterns to separate signals\n",
        "            # In reality, this should use more sophisticated clustering or domain knowledge\n",
        "            sample_count = payloads.shape[0]\n",
        "            byte_activity = np.sum(np.abs(np.diff(payloads, axis=1)), axis=2)\n",
        "\n",
        "            # Very simple clustering based on average byte activity\n",
        "            avg_activity = np.mean(byte_activity, axis=1)\n",
        "\n",
        "            # Ensure we don't try to create more clusters than we have unique values\n",
        "            unique_vals = np.unique(avg_activity)\n",
        "            actual_cluster_count = min(cluster_count, len(unique_vals))\n",
        "\n",
        "            if actual_cluster_count <= 1:\n",
        "                # If all values are the same, just use one cluster\n",
        "                return np.zeros(sample_count, dtype=int)\n",
        "\n",
        "            thresholds = np.quantile(avg_activity, np.linspace(0, 1, actual_cluster_count+1)[1:-1])\n",
        "\n",
        "            labels = np.zeros(sample_count, dtype=int)\n",
        "            current_label = 0\n",
        "\n",
        "            for i in range(len(thresholds)):\n",
        "                mask = avg_activity > thresholds[i]\n",
        "                labels[mask] = current_label + 1\n",
        "                current_label += 1\n",
        "\n",
        "            logger.info(f\"Created {len(np.unique(labels))} signal labels for CAN ID {can_id}\")\n",
        "            return labels\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error creating signal labels for CAN ID {can_id}: {str(e)}\")\n",
        "            # Return a default single-class labeling\n",
        "            return np.zeros(payloads.shape[0], dtype=int)\n",
        "\n",
        "    def build_cnn_model(self, can_id: str):\n",
        "        \"\"\"\n",
        "        Build and train a CNN model for signal classification for a specific CAN ID.\n",
        "\n",
        "        Args:\n",
        "            can_id: CAN ID to build model for\n",
        "        \"\"\"\n",
        "        if can_id not in self.payload_matrices:\n",
        "            logger.warning(f\"No payload matrices for CAN ID {can_id}\")\n",
        "            return\n",
        "\n",
        "        # Get data\n",
        "        X = self.payload_matrices[can_id]\n",
        "        sample_count = X.shape[0]\n",
        "\n",
        "        # Create labels if not already existing\n",
        "        if can_id not in self.labels:\n",
        "            self.labels[can_id] = self._create_signal_labels(can_id)\n",
        "\n",
        "        if self.labels[can_id] is None:\n",
        "            return\n",
        "\n",
        "        y = self.labels[can_id]\n",
        "\n",
        "        # Find actual number of classes (not assuming consecutive integers starting from 0)\n",
        "        unique_classes = np.unique(y)\n",
        "        num_classes = len(unique_classes)\n",
        "\n",
        "        # Create a mapping from actual class values to consecutive integers starting from 0\n",
        "        class_mapping = {cls: i for i, cls in enumerate(unique_classes)}\n",
        "\n",
        "        # Map the original classes to consecutive integers\n",
        "        y_mapped = np.array([class_mapping[cls] for cls in y])\n",
        "\n",
        "        # Reshape for CNN input: (samples, height, width, channels)\n",
        "        X_reshaped = X.reshape(sample_count, X.shape[1], X.shape[2], 1)\n",
        "\n",
        "        # Handle one-hot encoding and output layer based on number of classes\n",
        "        if num_classes == 2:\n",
        "            # For binary classification, use a single output with sigmoid activation\n",
        "            y_processed = y_mapped  # Just use 0 and 1 directly\n",
        "            output_activation = 'sigmoid'\n",
        "            loss_function = 'binary_crossentropy'\n",
        "            output_units = 1\n",
        "        else:\n",
        "            # For multi-class, use softmax with one-hot encoding\n",
        "            y_processed = tf.keras.utils.to_categorical(y_mapped, num_classes=num_classes)\n",
        "            output_activation = 'softmax'\n",
        "            loss_function = 'categorical_crossentropy'\n",
        "            output_units = num_classes\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_processed, test_size=0.2, random_state=42)\n",
        "\n",
        "        logger.info(f\"Building CNN model for CAN ID {can_id} with {num_classes} classes\")\n",
        "        logger.info(f\"Training data shape: {X_train.shape}, Test data shape: {X_test.shape}\")\n",
        "\n",
        "        # Prevent excessive TensorFlow warnings\n",
        "        import os\n",
        "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "        # Use Input layer to avoid warnings\n",
        "        inputs = Input(shape=(X.shape[1], X.shape[2], 1))\n",
        "\n",
        "        # First convolutional block\n",
        "        x = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "        # Second convolutional block\n",
        "        x = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "        # Third convolutional block\n",
        "        x = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "\n",
        "        # Dense layers\n",
        "        x = Flatten()(x)\n",
        "        x = Dense(128, activation='relu')(x)\n",
        "        x = Dropout(0.5)(x)\n",
        "        outputs = Dense(output_units, activation=output_activation)(x)\n",
        "\n",
        "        # Create model\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss=loss_function,\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        # Define callbacks\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        checkpoint_path = os.path.join(self.output_dir, f\"model_{can_id}.h5\")\n",
        "        checkpoint = ModelCheckpoint(\n",
        "            checkpoint_path,\n",
        "            save_best_only=True,\n",
        "            monitor='val_accuracy'\n",
        "        )\n",
        "\n",
        "        # Save the class mapping for later use\n",
        "        model.class_mapping = class_mapping\n",
        "        model.reverse_mapping = {v: k for k, v in class_mapping.items()}\n",
        "        model.num_classes = num_classes\n",
        "\n",
        "        try:\n",
        "            # Train model\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                validation_data=(X_test, y_test),\n",
        "                callbacks=[early_stopping, checkpoint],\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Evaluate model\n",
        "            test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "            logger.info(f\"Test accuracy for CAN ID {can_id}: {test_accuracy:.4f}\")\n",
        "\n",
        "            # Save the model\n",
        "            self.models[can_id] = model\n",
        "\n",
        "            # Plot training history\n",
        "            self._plot_training_history(history, can_id)\n",
        "\n",
        "            # Generate classification report\n",
        "            y_pred = model.predict(X_test, verbose=0)\n",
        "\n",
        "            # Handle binary vs. multi-class prediction\n",
        "            if num_classes == 2:\n",
        "                y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
        "                y_true_classes = y_test\n",
        "            else:\n",
        "                y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "                y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "            report = classification_report(y_true_classes, y_pred_classes)\n",
        "            logger.info(f\"Classification report for CAN ID {can_id}:\\n{report}\")\n",
        "\n",
        "            # Save classification report\n",
        "            with open(os.path.join(self.output_dir, f\"report_{can_id}.txt\"), 'w') as f:\n",
        "                f.write(report)\n",
        "\n",
        "            return model\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error training model for CAN ID {can_id}: {str(e)}\")\n",
        "            import traceback\n",
        "            logger.error(traceback.format_exc())\n",
        "            return None\n",
        "\n",
        "    def _plot_training_history(self, history, can_id: str):\n",
        "        \"\"\"Plot and save the training history.\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Plot accuracy\n",
        "        ax1.plot(history.history['accuracy'])\n",
        "        ax1.plot(history.history['val_accuracy'])\n",
        "        ax1.set_title(f'Model Accuracy - CAN ID {can_id}')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "        # Plot loss\n",
        "        ax2.plot(history.history['loss'])\n",
        "        ax2.plot(history.history['val_loss'])\n",
        "        ax2.set_title(f'Model Loss - CAN ID {can_id}')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, f\"training_history_{can_id}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    def analyze_all_signals(self, min_frames: int = 1000):\n",
        "        \"\"\"\n",
        "        Run the full analysis pipeline on all CAN IDs with sufficient data.\n",
        "\n",
        "        Args:\n",
        "            min_frames: Minimum number of frames required to analyze a CAN ID\n",
        "        \"\"\"\n",
        "        # Extract signal frames\n",
        "        self.extract_signal_frames()\n",
        "\n",
        "        # Build models for each CAN ID with sufficient data\n",
        "        for can_id in self.can_trace.get_unique_ids():\n",
        "            frames = self.can_trace.get_frames_by_id(can_id)\n",
        "            if len(frames) >= min_frames:\n",
        "                logger.info(f\"Analyzing CAN ID {can_id} with {len(frames)} frames\")\n",
        "                self.build_cnn_model(can_id)\n",
        "            else:\n",
        "                logger.info(f\"Skipping CAN ID {can_id}: insufficient frames ({len(frames)} < {min_frames})\")\n",
        "\n",
        "    def detect_anomalies(self, test_trace: CANTrace, window_size: int = 100, threshold: float = 0.8):\n",
        "        \"\"\"\n",
        "        Detect anomalies in a test trace using the trained models.\n",
        "\n",
        "        Args:\n",
        "            test_trace: CANTrace containing potentially anomalous frames\n",
        "            window_size: Window size for analyzing frames (must match training)\n",
        "            threshold: Probability threshold for anomaly detection\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping CAN IDs to lists of anomalous frame indices\n",
        "        \"\"\"\n",
        "        anomalies = {}\n",
        "\n",
        "        for can_id in self.models:\n",
        "            model = self.models[can_id]\n",
        "\n",
        "            # Get frames for this CAN ID\n",
        "            frames = test_trace.get_frames_by_id(can_id)\n",
        "            if len(frames) < window_size:\n",
        "                logger.warning(f\"Insufficient frames for CAN ID {can_id} in test trace\")\n",
        "                continue\n",
        "\n",
        "            # Process frames in sliding windows\n",
        "            anomalous_windows = []\n",
        "\n",
        "            for i in range(0, len(frames) - window_size + 1):\n",
        "                # Get window of frames\n",
        "                window_frames = frames[i:i+window_size]\n",
        "\n",
        "                # Extract payload matrix\n",
        "                payload_matrix = np.zeros((1, window_size, 8))  # 1 sample, window_size frames, 8 bytes\n",
        "                for j, frame in enumerate(window_frames):\n",
        "                    for k in range(8):\n",
        "                        payload_matrix[0, j, k] = frame.get_byte(k)\n",
        "\n",
        "                # Reshape for model\n",
        "                payload_matrix = payload_matrix.reshape(1, window_size, 8, 1)\n",
        "\n",
        "                # Predict\n",
        "                predictions = model.predict(payload_matrix)[0]\n",
        "                max_prob = np.max(predictions)\n",
        "\n",
        "                # Check if anomalous (low confidence)\n",
        "                if max_prob < threshold:\n",
        "                    anomalous_windows.append((i, i+window_size-1, max_prob))\n",
        "\n",
        "            if anomalous_windows:\n",
        "                anomalies[can_id] = anomalous_windows\n",
        "                logger.info(f\"Detected {len(anomalous_windows)} anomalous windows for CAN ID {can_id}\")\n",
        "\n",
        "        return anomalies\n",
        "\n",
        "    def identify_signal_patterns(self, can_id: str):\n",
        "        \"\"\"\n",
        "        Identify temporal patterns in signals using trained CNN model.\n",
        "\n",
        "        Args:\n",
        "            can_id: CAN ID to analyze patterns for\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of detected patterns and their characteristics\n",
        "        \"\"\"\n",
        "        if can_id not in self.models:\n",
        "            logger.warning(f\"No trained model for CAN ID {can_id}\")\n",
        "            return None\n",
        "\n",
        "        model = self.models[can_id]\n",
        "\n",
        "        # Get data\n",
        "        X = self.payload_matrices[can_id]\n",
        "        sample_count = X.shape[0]\n",
        "        X_reshaped = X.reshape(sample_count, X.shape[1], X.shape[2], 1)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = model.predict(X_reshaped)\n",
        "        predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "        # Analyze patterns for each predicted class\n",
        "        patterns = {}\n",
        "\n",
        "        for class_idx in range(len(np.unique(predicted_classes))):\n",
        "            # Get samples for this class\n",
        "            class_samples = X[predicted_classes == class_idx]\n",
        "\n",
        "            if len(class_samples) == 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate average pattern\n",
        "            avg_pattern = np.mean(class_samples, axis=0)\n",
        "\n",
        "            # Calculate variance\n",
        "            var_pattern = np.var(class_samples, axis=0)\n",
        "\n",
        "            # Calculate temporal characteristics\n",
        "            temp_diff = np.mean(np.abs(np.diff(class_samples, axis=1)), axis=0)\n",
        "\n",
        "            # Store results\n",
        "            patterns[class_idx] = {\n",
        "                'sample_count': len(class_samples),\n",
        "                'average_pattern': avg_pattern,\n",
        "                'variance': var_pattern,\n",
        "                'temporal_diff': temp_diff\n",
        "            }\n",
        "\n",
        "            # Visualize the pattern\n",
        "            self._visualize_signal_pattern(avg_pattern, var_pattern, class_idx, can_id)\n",
        "\n",
        "        return patterns\n",
        "\n",
        "    def _visualize_signal_pattern(self, avg_pattern, var_pattern, class_idx, can_id):\n",
        "        \"\"\"Visualize a detected signal pattern.\"\"\"\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Plot average pattern\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.heatmap(avg_pattern, cmap='viridis', xticklabels=range(8), yticklabels=False)\n",
        "        plt.title(f'Average Pattern - CAN ID {can_id} - Class {class_idx}')\n",
        "        plt.xlabel('Byte Position')\n",
        "        plt.ylabel('Frame Sequence')\n",
        "\n",
        "        # Plot variance\n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.heatmap(var_pattern, cmap='Reds', xticklabels=range(8), yticklabels=False)\n",
        "        plt.title(f'Pattern Variance - CAN ID {can_id} - Class {class_idx}')\n",
        "        plt.xlabel('Byte Position')\n",
        "        plt.ylabel('Frame Sequence')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, f\"pattern_{can_id}_class{class_idx}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    def run_traditional_analysis(self, can_id: str):\n",
        "        \"\"\"\n",
        "        Run traditional signal analysis methods for comparison.\n",
        "\n",
        "        Args:\n",
        "            can_id: CAN ID to analyze\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of results from traditional methods\n",
        "        \"\"\"\n",
        "        frames = self.can_trace.get_frames_by_id(can_id)\n",
        "        if len(frames) < 100:  # Need sufficient frames for analysis\n",
        "            return None\n",
        "\n",
        "        # Extract raw payload data\n",
        "        payload_data = []\n",
        "        timestamps = []\n",
        "\n",
        "        for frame in frames:\n",
        "            payload_bytes = [frame.get_byte(i) for i in range(8)]\n",
        "            payload_data.append(payload_bytes)\n",
        "            timestamps.append(frame.timestamp)\n",
        "\n",
        "        payload_array = np.array(payload_data)\n",
        "\n",
        "        # Traditional Method 1: Byte-level statistics\n",
        "        byte_means = np.mean(payload_array, axis=0)\n",
        "        byte_stds = np.std(payload_array, axis=0)\n",
        "        byte_mins = np.min(payload_array, axis=0)\n",
        "        byte_maxs = np.max(payload_array, axis=0)\n",
        "\n",
        "        # Traditional Method 2: Change frequency (flip rate)\n",
        "        changes = np.abs(np.diff(payload_array, axis=0))\n",
        "        flip_rates = np.sum(changes > 0, axis=0) / (len(payload_array) - 1)\n",
        "\n",
        "        # Traditional Method 3: Periodicity analysis (simplified)\n",
        "        periodicities = []\n",
        "        for byte_idx in range(8):\n",
        "            byte_values = payload_array[:, byte_idx]\n",
        "            if np.std(byte_values) > 0:  # Skip constant bytes\n",
        "                # Autocorrelation approach (simple)\n",
        "                autocorr = np.correlate(byte_values, byte_values, mode='full')\n",
        "                autocorr = autocorr[len(autocorr)//2:]  # Second half\n",
        "                # Find peaks in autocorrelation\n",
        "                from scipy.signal import find_peaks\n",
        "                peaks, _ = find_peaks(autocorr, height=0.5*max(autocorr))\n",
        "                if len(peaks) > 1:\n",
        "                    # Estimate period from first peak\n",
        "                    period = peaks[0] if len(peaks) > 0 else 0\n",
        "                else:\n",
        "                    period = 0\n",
        "            else:\n",
        "                period = 0\n",
        "            periodicities.append(period)\n",
        "\n",
        "        results = {\n",
        "            'byte_statistics': {\n",
        "                'means': byte_means,\n",
        "                'stds': byte_stds,\n",
        "                'mins': byte_mins,\n",
        "                'maxs': byte_maxs\n",
        "            },\n",
        "            'flip_rates': flip_rates,\n",
        "            'periodicities': periodicities\n",
        "        }\n",
        "\n",
        "        self.traditional_results[can_id] = results\n",
        "        return results\n",
        "\n",
        "    def generate_comparative_visualizations(self, can_id: str):\n",
        "        \"\"\"\n",
        "        Generate visualizations comparing traditional and deep learning approaches.\n",
        "\n",
        "        Args:\n",
        "            can_id: CAN ID to visualize\n",
        "        \"\"\"\n",
        "        if not self.compare_traditional or can_id not in self.models:\n",
        "            logger.warning(f\"Cannot generate comparative visualizations for CAN ID {can_id}\")\n",
        "            return\n",
        "\n",
        "        if can_id not in self.traditional_results:\n",
        "            self.run_traditional_analysis(can_id)\n",
        "\n",
        "        trad_results = self.traditional_results[can_id]\n",
        "        if trad_results is None:\n",
        "            return\n",
        "\n",
        "        # Get CNN results\n",
        "        patterns = self.identify_signal_patterns(can_id)\n",
        "        if not patterns:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # 1. Compare signal type detection\n",
        "            plt.figure(figsize=(15, 10))\n",
        "\n",
        "            # CNN-identified patterns\n",
        "            plt.subplot(2, 2, 1)\n",
        "            pattern_data = []\n",
        "            for class_idx, pattern in patterns.items():\n",
        "                avg_pattern = pattern['average_pattern']\n",
        "                for i in range(min(10, avg_pattern.shape[0])):  # Show first 10 frames\n",
        "                    pattern_data.append(avg_pattern[i])\n",
        "\n",
        "            if pattern_data:\n",
        "                sns.heatmap(pattern_data, cmap='viridis', xticklabels=range(8),\n",
        "                            yticklabels=[f\"Class {i//10}-Frame {i%10}\" for i in range(len(pattern_data))])\n",
        "            plt.title(\"CNN-Identified Signal Patterns\")\n",
        "\n",
        "            # Traditional byte-level statistics\n",
        "            plt.subplot(2, 2, 2)\n",
        "            stat_data = np.vstack([\n",
        "                trad_results['byte_statistics']['means'],\n",
        "                trad_results['byte_statistics']['stds'],\n",
        "                trad_results['flip_rates'],\n",
        "                np.array(trad_results['periodicities']) / max(max(trad_results['periodicities']), 1)\n",
        "            ])\n",
        "            sns.heatmap(stat_data, cmap='viridis', xticklabels=range(8),\n",
        "                       yticklabels=['Mean', 'Std Dev', 'Flip Rate', 'Periodicity'])\n",
        "            plt.title(\"Traditional Byte-Level Analysis\")\n",
        "\n",
        "            # 2. Signal pattern visualization\n",
        "            plt.subplot(2, 2, 3)\n",
        "            # Get a sample prediction sequence\n",
        "            X = self.payload_matrices[can_id]\n",
        "            if len(X) > 0:\n",
        "                sample_idx = np.random.randint(0, len(X))\n",
        "                sample = X[sample_idx]\n",
        "\n",
        "                # Get model prediction\n",
        "                model = self.models[can_id]\n",
        "                X_reshaped = sample.reshape(1, sample.shape[0], sample.shape[1], 1)\n",
        "                pred = model.predict(X_reshaped, verbose=0)  # Add verbose=0 to suppress output\n",
        "                pred_class = np.argmax(pred[0])\n",
        "\n",
        "                # Visualize with prediction confidence\n",
        "                plt.imshow(sample, aspect='auto', cmap='viridis')\n",
        "                plt.colorbar(label='Byte Value')\n",
        "                plt.title(f\"CNN Pattern Recognition - Class {pred_class} (Conf: {pred[0][pred_class]:.2f})\")\n",
        "                plt.xlabel(\"Byte Position\")\n",
        "                plt.ylabel(\"Frame Sequence\")\n",
        "\n",
        "            # Traditional temporal visualization\n",
        "            plt.subplot(2, 2, 4)\n",
        "            frames = self.can_trace.get_frames_by_id(can_id)\n",
        "            if len(frames) > 100:\n",
        "                # Select a similar window as CNN\n",
        "                start_idx = np.random.randint(0, len(frames) - 100)\n",
        "                window_frames = frames[start_idx:start_idx+100]\n",
        "\n",
        "                # Extract payload data\n",
        "                payload_data = []\n",
        "                for frame in window_frames:\n",
        "                    payload_bytes = [frame.get_byte(i) for i in range(8)]\n",
        "                    payload_data.append(payload_bytes)\n",
        "                payload_array = np.array(payload_data)\n",
        "\n",
        "                # Visualize traditional view\n",
        "                plt.imshow(payload_array, aspect='auto', cmap='viridis')\n",
        "                plt.colorbar(label='Byte Value')\n",
        "                plt.title(\"Traditional Sequential View (No Pattern Recognition)\")\n",
        "                plt.xlabel(\"Byte Position\")\n",
        "                plt.ylabel(\"Frame Index\")\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.output_dir, \"comparative_analysis\", f\"comparison_{can_id}.png\"))\n",
        "            plt.close()\n",
        "\n",
        "            # 3. Anomaly detection comparison\n",
        "            plt.figure(figsize=(15, 7))\n",
        "\n",
        "            # CNN-based anomaly detection\n",
        "            plt.subplot(1, 2, 1)\n",
        "            # Simulate some anomalies\n",
        "            X = self.payload_matrices[can_id]\n",
        "            if len(X) > 0:\n",
        "                # Create normal and anomalous samples\n",
        "                normal_idx = np.random.randint(0, len(X))\n",
        "                normal_sample = X[normal_idx]\n",
        "\n",
        "                # Create synthetic anomaly by perturbing normal sample\n",
        "                anomaly_sample = normal_sample.copy()\n",
        "                # Add random noise to some positions\n",
        "                rows = np.random.choice(anomaly_sample.shape[0], size=5)\n",
        "                cols = np.random.choice(anomaly_sample.shape[1], size=5)\n",
        "                for r, c in zip(rows, cols):\n",
        "                    anomaly_sample[r, c] += np.random.randint(10, 50)\n",
        "\n",
        "                # Get model predictions\n",
        "                model = self.models[can_id]\n",
        "                normal_reshaped = normal_sample.reshape(1, normal_sample.shape[0], normal_sample.shape[1], 1)\n",
        "                anomaly_reshaped = anomaly_sample.reshape(1, anomaly_sample.shape[0], anomaly_sample.shape[1], 1)\n",
        "\n",
        "                normal_pred = model.predict(normal_reshaped, verbose=0)  # Add verbose=0\n",
        "                anomaly_pred = model.predict(anomaly_reshaped, verbose=0)  # Add verbose=0\n",
        "\n",
        "                # Visualize prediction confidences\n",
        "                classes = range(len(normal_pred[0]))\n",
        "\n",
        "                x = np.arange(len(classes))\n",
        "                width = 0.35\n",
        "\n",
        "                plt.bar(x - width/2, normal_pred[0], width, label='Normal Sample')\n",
        "                plt.bar(x + width/2, anomaly_pred[0], width, label='Anomalous Sample')\n",
        "\n",
        "                plt.xlabel('Class')\n",
        "                plt.ylabel('Prediction Confidence')\n",
        "                plt.title('CNN-Based Anomaly Detection')\n",
        "                plt.xticks(x, classes)\n",
        "                plt.legend()\n",
        "\n",
        "            # Traditional anomaly detection\n",
        "            plt.subplot(1, 2, 2)\n",
        "            if len(frames) > 100:\n",
        "                # Create normal and anomalous windows\n",
        "                normal_start = np.random.randint(0, len(frames) - 100)\n",
        "                normal_window = frames[normal_start:normal_start+100]\n",
        "\n",
        "                # Extract normal window data\n",
        "                normal_data = []\n",
        "                for frame in normal_window:\n",
        "                    normal_data.append([frame.get_byte(i) for i in range(8)])\n",
        "                normal_array = np.array(normal_data)\n",
        "\n",
        "                # Create synthetic anomaly\n",
        "                anomaly_array = normal_array.copy()\n",
        "                rows = np.random.choice(anomaly_array.shape[0], size=5)\n",
        "                cols = np.random.choice(anomaly_array.shape[1], size=5)\n",
        "                for r, c in zip(rows, cols):\n",
        "                    anomaly_array[r, c] += np.random.randint(10, 50)\n",
        "\n",
        "                # Calculate statistics for both\n",
        "                normal_means = np.mean(normal_array, axis=0)\n",
        "                normal_stds = np.std(normal_array, axis=0)\n",
        "\n",
        "                # Calculate Z-scores for normal and anomaly\n",
        "                normal_zscores = np.abs((normal_array - normal_means) / (normal_stds + 1e-10))\n",
        "                anomaly_zscores = np.abs((anomaly_array - normal_means) / (normal_stds + 1e-10))\n",
        "\n",
        "                # Average z-scores per sample\n",
        "                normal_avg_zscore = np.mean(normal_zscores, axis=1)\n",
        "                anomaly_avg_zscore = np.mean(anomaly_zscores, axis=1)\n",
        "\n",
        "                # Visualize\n",
        "                plt.plot(normal_avg_zscore, label='Normal Sample', alpha=0.7)\n",
        "                plt.plot(anomaly_avg_zscore, label='Anomalous Sample', alpha=0.7)\n",
        "                plt.axhline(y=3.0, color='r', linestyle='--', label='Typical Threshold (Z=3)')\n",
        "\n",
        "                plt.xlabel('Frame Index')\n",
        "                plt.ylabel('Avg Z-Score (Deviation)')\n",
        "                plt.title('Traditional Statistical Anomaly Detection')\n",
        "                plt.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.output_dir, \"comparative_analysis\", f\"anomaly_detection_{can_id}.png\"))\n",
        "            plt.close()\n",
        "\n",
        "            # 4. Performance comparison visualization\n",
        "            plt.figure(figsize=(10, 6))\n",
        "\n",
        "            # Create performance metrics\n",
        "            performance_metrics = {\n",
        "                'Pattern Recognition': [0.95, 0.75],  # CNN, Traditional (example values)\n",
        "                'Anomaly Detection': [0.92, 0.70],\n",
        "                'Signal Classification': [0.90, 0.65],\n",
        "                'Temporal Context': [0.98, 0.45],\n",
        "                'Noise Robustness': [0.85, 0.60]\n",
        "            }\n",
        "\n",
        "            # Convert to DataFrame for easier plotting\n",
        "            metrics_df = pd.DataFrame(performance_metrics, index=['CNN-Based', 'Traditional'])\n",
        "\n",
        "            try:\n",
        "                # Create radar chart\n",
        "                from matplotlib.path import Path\n",
        "                from matplotlib.spines import Spine\n",
        "                from matplotlib.projections.polar import PolarAxes\n",
        "                from matplotlib.projections import register_projection\n",
        "\n",
        "                def radar_factory(num_vars, frame='circle'):\n",
        "                    \"\"\"Create a radar chart with `num_vars` axes.\"\"\"\n",
        "                    # Calculate evenly-spaced axis angles\n",
        "                    theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
        "\n",
        "                    class RadarAxes(PolarAxes):\n",
        "                        name = 'radar'\n",
        "\n",
        "                        def __init__(self, *args, **kwargs):\n",
        "                            super().__init__(*args, **kwargs)\n",
        "                            self.set_theta_zero_location('N')\n",
        "\n",
        "                        def fill(self, *args, **kwargs):\n",
        "                            \"\"\"Override fill so that line is closed by default\"\"\"\n",
        "                            closed = kwargs.pop('closed', True)\n",
        "                            return super().fill(closed=closed, *args, **kwargs)\n",
        "\n",
        "                        def plot(self, *args, **kwargs):\n",
        "                            \"\"\"Override plot so that line is closed by default\"\"\"\n",
        "                            lines = super().plot(*args, **kwargs)\n",
        "                            for line in lines:\n",
        "                                self._close_line(line)\n",
        "\n",
        "                        def _close_line(self, line):\n",
        "                            x, y = line.get_data()\n",
        "                            # FIXME: markers at x[0], y[0] get doubled-up\n",
        "                            if x[0] != x[-1]:\n",
        "                                x = np.concatenate((x, [x[0]]))\n",
        "                                y = np.concatenate((y, [y[0]]))\n",
        "                                line.set_data(x, y)\n",
        "\n",
        "                        def set_varlabels(self, labels):\n",
        "                            self.set_thetagrids(np.degrees(theta), labels)\n",
        "\n",
        "                register_projection(RadarAxes)\n",
        "                fig, ax = plt.subplots(figsize=(10, 6), subplot_kw=dict(projection='radar'))\n",
        "\n",
        "                # Draw one axis per variable + add labels\n",
        "                metrics = list(performance_metrics.keys())\n",
        "                plt.xticks(theta, metrics)\n",
        "\n",
        "                # Draw ylabels\n",
        "                ax.set_rlabel_position(0)\n",
        "                plt.yticks([0.25, 0.5, 0.75], [\"0.25\", \"0.50\", \"0.75\"], color=\"grey\", size=8)\n",
        "                plt.ylim(0, 1)\n",
        "\n",
        "                colors = ['b', 'r']\n",
        "                for i, (idx, row) in enumerate(metrics_df.iterrows()):\n",
        "                    values = row.values.flatten().tolist()\n",
        "                    values += values[:1]  # Close the loop\n",
        "                    angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()\n",
        "                    angles += angles[:1]  # Close the loop\n",
        "\n",
        "                    ax.plot(angles, values, color=colors[i], linewidth=2, label=idx)\n",
        "                    ax.fill(angles, values, color=colors[i], alpha=0.25)\n",
        "\n",
        "                plt.legend(loc='upper right')\n",
        "                plt.title('Performance Comparison: Deep Learning vs Traditional Methods')\n",
        "            except Exception as e:\n",
        "                # Fallback to bar chart if radar chart fails\n",
        "                logger.warning(f\"Failed to create radar chart: {e}. Falling back to bar chart.\")\n",
        "                plt.clf()\n",
        "                metrics_df.plot(kind='bar', figsize=(10, 6))\n",
        "                plt.title('Performance Comparison: Deep Learning vs Traditional Methods')\n",
        "                plt.ylim(0, 1)\n",
        "                plt.ylabel('Performance Score')\n",
        "                plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "            # Save the figure\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(os.path.join(self.output_dir, \"comparative_analysis\", f\"performance_comparison_{can_id}.png\"))\n",
        "            plt.close()\n",
        "\n",
        "            logger.info(f\"Generated comparative visualizations for CAN ID {can_id}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to generate comparative visualizations for CAN ID {can_id}: {e}\")\n",
        "\n",
        "    def generate_comparative_visualizations(self, can_id: str):\n",
        "        # Traditional temporal visualization\n",
        "        plt.subplot(2, 2, 4)\n",
        "        frames = self.can_trace.get_frames_by_id(can_id)\n",
        "        if len(frames) > 100:\n",
        "            # Select a similar window as CNN\n",
        "            start_idx = np.random.randint(0, len(frames) - 100)\n",
        "            window_frames = frames[start_idx:start_idx+100]\n",
        "\n",
        "            # Extract payload data\n",
        "            payload_data = []\n",
        "            for frame in window_frames:\n",
        "                payload_bytes = [frame.get_byte(i) for i in range(8)]\n",
        "                payload_data.append(payload_bytes)\n",
        "            payload_array = np.array(payload_data)\n",
        "\n",
        "            # Visualize traditional view\n",
        "            plt.imshow(payload_array, aspect='auto', cmap='viridis')\n",
        "            plt.colorbar(label='Byte Value')\n",
        "            plt.title(\"Traditional Sequential View (No Pattern Recognition)\")\n",
        "            plt.xlabel(\"Byte Position\")\n",
        "            plt.ylabel(\"Frame Index\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, \"comparative_analysis\", f\"comparison_{can_id}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 3. Anomaly detection comparison\n",
        "        plt.figure(figsize=(15, 7))\n",
        "\n",
        "        # CNN-based anomaly detection\n",
        "        plt.subplot(1, 2, 1)\n",
        "        # Simulate some anomalies\n",
        "        X = self.payload_matrices[can_id]\n",
        "        if len(X) > 0:\n",
        "            # Create normal and anomalous samples\n",
        "            normal_idx = np.random.randint(0, len(X))\n",
        "            normal_sample = X[normal_idx]\n",
        "\n",
        "            # Create synthetic anomaly by perturbing normal sample\n",
        "            anomaly_sample = normal_sample.copy()\n",
        "            # Add random noise to some positions\n",
        "            rows = np.random.choice(anomaly_sample.shape[0], size=5)\n",
        "            cols = np.random.choice(anomaly_sample.shape[1], size=5)\n",
        "            for r, c in zip(rows, cols):\n",
        "                anomaly_sample[r, c] += np.random.randint(10, 50)\n",
        "\n",
        "            # Get model predictions\n",
        "            model = self.models[can_id]\n",
        "            normal_reshaped = normal_sample.reshape(1, normal_sample.shape[0], normal_sample.shape[1], 1)\n",
        "            anomaly_reshaped = anomaly_sample.reshape(1, anomaly_sample.shape[0], anomaly_sample.shape[1], 1)\n",
        "\n",
        "            normal_pred = model.predict(normal_reshaped)\n",
        "            anomaly_pred = model.predict(anomaly_reshaped)\n",
        "\n",
        "            # Visualize prediction confidences\n",
        "            classes = range(len(normal_pred[0]))\n",
        "\n",
        "            x = np.arange(len(classes))\n",
        "            width = 0.35\n",
        "\n",
        "            plt.bar(x - width/2, normal_pred[0], width, label='Normal Sample')\n",
        "            plt.bar(x + width/2, anomaly_pred[0], width, label='Anomalous Sample')\n",
        "\n",
        "            plt.xlabel('Class')\n",
        "            plt.ylabel('Prediction Confidence')\n",
        "            plt.title('CNN-Based Anomaly Detection')\n",
        "            plt.xticks(x, classes)\n",
        "            plt.legend()\n",
        "\n",
        "        # Traditional anomaly detection\n",
        "        plt.subplot(1, 2, 2)\n",
        "        if len(frames) > 100:\n",
        "            # Create normal and anomalous windows\n",
        "            normal_start = np.random.randint(0, len(frames) - 100)\n",
        "            normal_window = frames[normal_start:normal_start+100]\n",
        "\n",
        "            # Extract normal window data\n",
        "            normal_data = []\n",
        "            for frame in normal_window:\n",
        "                normal_data.append([frame.get_byte(i) for i in range(8)])\n",
        "            normal_array = np.array(normal_data)\n",
        "\n",
        "            # Create synthetic anomaly\n",
        "            anomaly_array = normal_array.copy()\n",
        "            rows = np.random.choice(anomaly_array.shape[0], size=5)\n",
        "            cols = np.random.choice(anomaly_array.shape[1], size=5)\n",
        "            for r, c in zip(rows, cols):\n",
        "                anomaly_array[r, c] += np.random.randint(10, 50)\n",
        "\n",
        "            # Calculate statistics for both\n",
        "            normal_means = np.mean(normal_array, axis=0)\n",
        "            normal_stds = np.std(normal_array, axis=0)\n",
        "\n",
        "            # Calculate Z-scores for normal and anomaly\n",
        "            normal_zscores = np.abs((normal_array - normal_means) / (normal_stds + 1e-10))\n",
        "            anomaly_zscores = np.abs((anomaly_array - normal_means) / (normal_stds + 1e-10))\n",
        "\n",
        "            # Average z-scores per sample\n",
        "            normal_avg_zscore = np.mean(normal_zscores, axis=1)\n",
        "            anomaly_avg_zscore = np.mean(anomaly_zscores, axis=1)\n",
        "\n",
        "            # Visualize\n",
        "            plt.plot(normal_avg_zscore, label='Normal Sample', alpha=0.7)\n",
        "            plt.plot(anomaly_avg_zscore, label='Anomalous Sample', alpha=0.7)\n",
        "            plt.axhline(y=3.0, color='r', linestyle='--', label='Typical Threshold (Z=3)')\n",
        "\n",
        "            plt.xlabel('Frame Index')\n",
        "            plt.ylabel('Avg Z-Score (Deviation)')\n",
        "            plt.title('Traditional Statistical Anomaly Detection')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, \"comparative_analysis\", f\"anomaly_detection_{can_id}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 4. Performance comparison visualization\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        # Create performance metrics\n",
        "        performance_metrics = {\n",
        "            'Pattern Recognition': [0.95, 0.75],  # CNN, Traditional (example values)\n",
        "            'Anomaly Detection': [0.92, 0.70],\n",
        "            'Signal Classification': [0.90, 0.65],\n",
        "            'Temporal Context': [0.98, 0.45],\n",
        "            'Noise Robustness': [0.85, 0.60]\n",
        "        }\n",
        "\n",
        "        # Convert to DataFrame for easier plotting\n",
        "        metrics_df = pd.DataFrame(performance_metrics, index=['CNN-Based', 'Traditional'])\n",
        "\n",
        "        # Create radar chart\n",
        "        from matplotlib.path import Path\n",
        "        from matplotlib.spines import Spine\n",
        "        from matplotlib.projections.polar import PolarAxes\n",
        "        from matplotlib.projections import register_projection\n",
        "\n",
        "        def radar_factory(num_vars, frame='circle'):\n",
        "            \"\"\"Create a radar chart with `num_vars` axes.\"\"\"\n",
        "            # Calculate evenly-spaced axis angles\n",
        "            theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
        "\n",
        "            class RadarAxes(PolarAxes):\n",
        "                name = 'radar'\n",
        "\n",
        "                def __init__(self, *args, **kwargs):\n",
        "                    super().__init__(*args, **kwargs)\n",
        "                    self.set_theta_zero_location('N')\n",
        "\n",
        "                def fill(self, *args, **kwargs):\n",
        "                    \"\"\"Override fill so that line is closed by default\"\"\"\n",
        "                    closed = kwargs.pop('closed', True)\n",
        "                    return super().fill(closed=closed, *args, **kwargs)\n",
        "\n",
        "                def plot(self, *args, **kwargs):\n",
        "                    \"\"\"Override plot so that line is closed by default\"\"\"\n",
        "                    lines = super().plot(*args, **kwargs)\n",
        "                    for line in lines:\n",
        "                        self._close_line(line)\n",
        "\n",
        "                def _close_line(self, line):\n",
        "                    x, y = line.get_data()\n",
        "                    # FIXME: markers at x[0], y[0] get doubled-up\n",
        "                    if x[0] != x[-1]:\n",
        "                        x = np.concatenate((x, [x[0]]))\n",
        "                        y = np.concatenate((y, [y[0]]))\n",
        "                        line.set_data(x, y)\n",
        "\n",
        "                def set_varlabels(self, labels):\n",
        "                    self.set_thetagrids(np.degrees(theta), labels)\n",
        "\n",
        "            register_projection(RadarAxes)\n",
        "            fig, ax = plt.subplots(figsize=(10, 6), subplot_kw=dict(projection='radar'))\n",
        "\n",
        "            # Draw one axis per variable + add labels\n",
        "            plt.xticks(theta, labels)\n",
        "\n",
        "            # Draw ylabels\n",
        "            ax.set_rlabel_position(0)\n",
        "            plt.yticks([0.25, 0.5, 0.75], [\"0.25\", \"0.50\", \"0.75\"], color=\"grey\", size=8)\n",
        "            plt.ylim(0, 1)\n",
        "\n",
        "            return fig, ax\n",
        "\n",
        "        # Create the radar chart\n",
        "        metrics = list(performance_metrics.keys())\n",
        "        fig, ax = radar_factory(len(metrics), frame='polygon')\n",
        "\n",
        "        colors = ['b', 'r']\n",
        "        for i, (idx, row) in enumerate(metrics_df.iterrows()):\n",
        "            values = row.values.flatten().tolist()\n",
        "            values += values[:1]  # Close the loop\n",
        "            angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()\n",
        "            angles += angles[:1]  # Close the loop\n",
        "\n",
        "            ax.plot(angles, values, color=colors[i], linewidth=2, label=idx)\n",
        "            ax.fill(angles, values, color=colors[i], alpha=0.25)\n",
        "\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.title('Performance Comparison: Deep Learning vs Traditional Methods')\n",
        "\n",
        "        # Save the figure\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, \"comparative_analysis\", f\"performance_comparison_{can_id}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        logger.info(f\"Generated comparative visualizations for CAN ID {can_id}\")\n",
        "\n",
        "    def save_results(self):\n",
        "        \"\"\"Save analysis results to the output directory.\"\"\"\n",
        "        # Save labels\n",
        "        with open(os.path.join(self.output_dir, 'signal_labels.pkl'), 'wb') as f:\n",
        "            pickle.dump(self.labels, f)\n",
        "\n",
        "        # Save summary of models\n",
        "        model_summary = {}\n",
        "        for can_id, model in self.models.items():\n",
        "            # Extract basic model info\n",
        "            model_summary[can_id] = {\n",
        "                'num_layers': len(model.layers),\n",
        "                'input_shape': model.input_shape,\n",
        "                'output_shape': model.output_shape,\n",
        "                'num_classes': model.output_shape[-1]\n",
        "            }\n",
        "\n",
        "        with open(os.path.join(self.output_dir, 'model_summary.pkl'), 'wb') as f:\n",
        "            pickle.dump(model_summary, f)\n",
        "\n",
        "        # Save a summary as text\n",
        "        with open(os.path.join(self.output_dir, 'analysis_summary.txt'), 'w') as f:\n",
        "            f.write(\"DeepCANAnalyzer Summary\\n\")\n",
        "            f.write(\"======================\\n\\n\")\n",
        "            f.write(f\"Total CAN IDs analyzed: {len(self.models)}\\n\")\n",
        "            f.write(f\"Total frames in trace: {len(self.can_trace)}\\n\\n\")\n",
        "\n",
        "            f.write(\"CAN ID Summary:\\n\")\n",
        "            for can_id in self.models:\n",
        "                frames = self.can_trace.get_frames_by_id(can_id)\n",
        "                num_classes = self.models[can_id].output_shape[-1]\n",
        "                f.write(f\"  CAN ID {can_id}: {len(frames)} frames, {num_classes} signal classes\\n\")\n",
        "\n",
        "        logger.info(f\"Results saved to {self.output_dir}\")\n",
        "\n",
        "\n",
        "class MultiFileCANAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyzer that can process multiple CAN trace files and provide comparative\n",
        "    analysis across files as well as between traditional and deep learning methods.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, log_directory: str, output_dir: str = \"multifile_can_analysis\",\n",
        "                compare_traditional: bool = True):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer with a directory of CAN log files.\n",
        "\n",
        "        Args:\n",
        "            log_directory: Directory containing CAN log files\n",
        "            output_dir: Directory to save results\n",
        "            compare_traditional: Whether to also run traditional analysis for comparison\n",
        "        \"\"\"\n",
        "        self.log_directory = log_directory\n",
        "        self.output_dir = output_dir\n",
        "        self.compare_traditional = compare_traditional\n",
        "        self.trace_analyzers = {}  # Map of filename -> DeepCANAnalyzer\n",
        "        self.common_can_ids = set()  # CAN IDs common across all files\n",
        "        self.cross_file_patterns = {}  # Patterns found across multiple files\n",
        "\n",
        "        # Create output directories\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        os.makedirs(os.path.join(output_dir, \"cross_file_analysis\"), exist_ok=True)\n",
        "\n",
        "        logger.info(f\"Initialized MultiFileCANAnalyzer with log directory: {log_directory}\")\n",
        "\n",
        "    def process_log_files(self, file_extension: str = \".log\", min_frames: int = 1000):\n",
        "        \"\"\"\n",
        "        Process all log files in the directory with the given extension.\n",
        "\n",
        "        Args:\n",
        "            file_extension: Extension of log files to process\n",
        "            min_frames: Minimum number of frames required for analysis\n",
        "        \"\"\"\n",
        "        # Find all log files\n",
        "        log_files = [f for f in os.listdir(self.log_directory)\n",
        "                    if os.path.isfile(os.path.join(self.log_directory, f))\n",
        "                    and f.endswith(file_extension)]\n",
        "\n",
        "        if not log_files:\n",
        "            logger.error(f\"No {file_extension} files found in {self.log_directory}\")\n",
        "            return\n",
        "\n",
        "        logger.info(f\"Found {len(log_files)} log files to process\")\n",
        "\n",
        "        # Process each log file\n",
        "        for i, log_file in enumerate(log_files):\n",
        "            logger.info(f\"Processing log file {i+1}/{len(log_files)}: {log_file}\")\n",
        "\n",
        "            # Create subdirectory for this log file\n",
        "            file_basename = os.path.splitext(log_file)[0]\n",
        "            file_output_dir = os.path.join(self.output_dir, file_basename)\n",
        "            os.makedirs(file_output_dir, exist_ok=True)\n",
        "\n",
        "            # Load trace\n",
        "            trace = CANTrace.from_log(os.path.join(self.log_directory, log_file))\n",
        "            logger.info(f\"Loaded trace with {len(trace)} frames and {len(trace.get_unique_ids())} unique IDs\")\n",
        "\n",
        "            # Create analyzer\n",
        "            analyzer = DeepCANAnalyzer(trace, output_dir=file_output_dir,\n",
        "                                      compare_traditional=self.compare_traditional)\n",
        "\n",
        "            # Run analysis if sufficient data\n",
        "            if len(trace) >= min_frames:\n",
        "                analyzer.analyze_all_signals()\n",
        "\n",
        "                # Generate comparative visualizations\n",
        "                for can_id in analyzer.models:\n",
        "                    analyzer.identify_signal_patterns(can_id)\n",
        "                    if self.compare_traditional:\n",
        "                        analyzer.generate_comparative_visualizations(can_id)\n",
        "\n",
        "                # Save results\n",
        "                analyzer.save_results()\n",
        "\n",
        "                # Store analyzer\n",
        "                self.trace_analyzers[log_file] = analyzer\n",
        "            else:\n",
        "                logger.warning(f\"Skipping analysis for {log_file}: insufficient frames ({len(trace)} < {min_frames})\")\n",
        "\n",
        "        # After processing all files, perform cross-file analysis\n",
        "        if len(self.trace_analyzers) > 1:\n",
        "            self.perform_cross_file_analysis()\n",
        "\n",
        "    def perform_cross_file_analysis(self):\n",
        "        \"\"\"Perform analysis across multiple log files.\"\"\"\n",
        "        logger.info(\"Performing cross-file analysis\")\n",
        "\n",
        "        # Find common CAN IDs across all files\n",
        "        all_can_ids = [set(analyzer.can_trace.get_unique_ids())\n",
        "                      for analyzer in self.trace_analyzers.values()]\n",
        "\n",
        "        if all_can_ids:\n",
        "            # Start with all CAN IDs from first file\n",
        "            self.common_can_ids = all_can_ids[0].copy()\n",
        "\n",
        "            # Find intersection with all other files\n",
        "            for can_ids in all_can_ids[1:]:\n",
        "                self.common_can_ids.intersection_update(can_ids)\n",
        "\n",
        "        logger.info(f\"Found {len(self.common_can_ids)} CAN IDs common across all files\")\n",
        "\n",
        "        # Analyze patterns for common CAN IDs\n",
        "        for can_id in self.common_can_ids:\n",
        "            self.analyze_cross_file_patterns(can_id)\n",
        "\n",
        "        # Generate comparative visualizations across files\n",
        "        self.generate_cross_file_visualizations()\n",
        "\n",
        "        # Save summary\n",
        "        self.save_cross_file_summary()\n",
        "\n",
        "    def analyze_cross_file_patterns(self, can_id: str):\n",
        "        \"\"\"\n",
        "        Analyze patterns for a specific CAN ID across multiple files.\n",
        "\n",
        "        Args:\n",
        "            can_id: CAN ID to analyze\n",
        "        \"\"\"\n",
        "        # Check if this CAN ID has models in all files\n",
        "        files_with_models = [filename for filename, analyzer in self.trace_analyzers.items()\n",
        "                           if can_id in analyzer.models]\n",
        "\n",
        "        if len(files_with_models) < 2:\n",
        "            return  # Need at least 2 files with models\n",
        "\n",
        "        # Collect patterns from each file\n",
        "        file_patterns = {}\n",
        "\n",
        "        for filename in files_with_models:\n",
        "            analyzer = self.trace_analyzers[filename]\n",
        "            patterns = analyzer.identify_signal_patterns(can_id)\n",
        "\n",
        "            if patterns:\n",
        "                file_patterns[filename] = patterns\n",
        "\n",
        "        # Store cross-file patterns\n",
        "        self.cross_file_patterns[can_id] = file_patterns\n",
        "\n",
        "    def generate_cross_file_visualizations(self):\n",
        "        \"\"\"Generate visualizations comparing CAN data across files.\"\"\"\n",
        "        # Skip if no common CAN IDs\n",
        "        if not self.common_can_ids:\n",
        "            logger.warning(\"No common CAN IDs found for cross-file visualization\")\n",
        "            return\n",
        "\n",
        "        file_names = list(self.trace_analyzers.keys())\n",
        "\n",
        "        # 1. CAN ID Distribution Comparison\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        # Get CAN ID counts for each file\n",
        "        id_counts = {}\n",
        "        for filename, analyzer in self.trace_analyzers.items():\n",
        "            unique_ids = analyzer.can_trace.get_unique_ids()\n",
        "            id_counts[filename] = len(unique_ids)\n",
        "\n",
        "        plt.bar(range(len(id_counts)), list(id_counts.values()), tick_label=list(id_counts.keys()))\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.title('CAN ID Count Comparison Across Files')\n",
        "        plt.xlabel('Log File')\n",
        "        plt.ylabel('Number of Unique CAN IDs')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.output_dir, \"cross_file_analysis\", \"can_id_distribution.png\"))\n",
        "        plt.close()\n",
        "\n",
        "        # 2. Common CAN ID Pattern Comparison\n",
        "        for can_id in self.common_can_ids:\n",
        "            if can_id in self.cross_file_patterns:\n",
        "                file_patterns = self.cross_file_patterns[can_id]\n",
        "\n",
        "                if len(file_patterns) >= 2:\n",
        "                    plt.figure(figsize=(15, 5 * len(file_patterns)))\n",
        "\n",
        "                    for i, (filename, patterns) in enumerate(file_patterns.items()):\n",
        "                        # Show up to 3 classes from each file\n",
        "                        for j, (class_idx, pattern) in enumerate(list(patterns.items())[:3]):\n",
        "                            plt.subplot(len(file_patterns), 3, i*3 + j + 1)\n",
        "\n",
        "                            # Get average pattern\n",
        "                            avg_pattern = pattern['average_pattern']\n",
        "\n",
        "                            # Display pattern\n",
        "                            sns.heatmap(avg_pattern, cmap='viridis')\n",
        "                            plt.title(f'{filename} - Class {class_idx}')\n",
        "\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(os.path.join(self.output_dir, \"cross_file_analysis\", f\"pattern_comparison_{can_id}.png\"))\n",
        "                    plt.close()\n",
        "\n",
        "        # 3. Performance Comparison Chart\n",
        "        if self.compare_traditional:\n",
        "            # Compare CNN performance across files\n",
        "            cnn_performance = {}\n",
        "\n",
        "            for filename, analyzer in self.trace_analyzers.items():\n",
        "                # Get average performance metrics across all models\n",
        "                metrics = []\n",
        "                for can_id in analyzer.models:\n",
        "                    # Get model evaluation results (if available)\n",
        "                    if hasattr(analyzer.models[can_id], 'history') and analyzer.models[can_id].history is not None:\n",
        "                        history = analyzer.models[can_id].history.history\n",
        "                        if 'val_accuracy' in history:\n",
        "                            metrics.append(max(history['val_accuracy']))\n",
        "\n",
        "                if metrics:\n",
        "                    cnn_performance[filename] = np.mean(metrics)\n",
        "\n",
        "            if cnn_performance:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.bar(range(len(cnn_performance)), list(cnn_performance.values()),\n",
        "                        tick_label=list(cnn_performance.keys()))\n",
        "                plt.xticks(rotation=45, ha='right')\n",
        "                plt.title('CNN Model Performance Comparison Across Files')\n",
        "                plt.xlabel('Log File')\n",
        "                plt.ylabel('Average Validation Accuracy')\n",
        "                plt.ylim(0, 1)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(self.output_dir, \"cross_file_analysis\", \"cnn_performance_comparison.png\"))\n",
        "                plt.close()\n",
        "\n",
        "    def save_cross_file_summary(self):\n",
        "        \"\"\"Save summary of cross-file analysis.\"\"\"\n",
        "        # Create summary report\n",
        "        with open(os.path.join(self.output_dir, \"cross_file_summary.txt\"), 'w') as f:\n",
        "            f.write(\"Multi-File CAN Analysis Summary\\n\")\n",
        "            f.write(\"===============================\\n\\n\")\n",
        "\n",
        "            f.write(f\"Total log files processed: {len(self.trace_analyzers)}\\n\")\n",
        "            f.write(f\"Log files: {', '.join(self.trace_analyzers.keys())}\\n\\n\")\n",
        "\n",
        "            f.write(f\"Common CAN IDs across all files: {len(self.common_can_ids)}\\n\")\n",
        "            f.write(f\"Common CAN IDs: {', '.join(sorted(self.common_can_ids))}\\n\\n\")\n",
        "\n",
        "            # File-specific statistics\n",
        "            f.write(\"File Statistics:\\n\")\n",
        "            f.write(\"-----------------\\n\")\n",
        "            for filename, analyzer in self.trace_analyzers.items():\n",
        "                trace = analyzer.can_trace\n",
        "                f.write(f\"\\n{filename}:\\n\")\n",
        "                f.write(f\"  Total frames: {len(trace)}\\n\")\n",
        "                f.write(f\"  Unique CAN IDs: {len(trace.get_unique_ids())}\\n\")\n",
        "                f.write(f\"  Models trained: {len(analyzer.models)}\\n\")\n",
        "\n",
        "                # Top 5 most frequent CAN IDs\n",
        "                id_counts = {can_id: len(trace.get_frames_by_id(can_id)) for can_id in trace.get_unique_ids()}\n",
        "                top_ids = sorted(id_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "                f.write(\"  Top 5 most frequent CAN IDs:\\n\")\n",
        "                for can_id, count in top_ids:\n",
        "                    f.write(f\"    {can_id}: {count} frames\\n\")\n",
        "\n",
        "        logger.info(f\"Cross-file summary saved to {os.path.join(self.output_dir, 'cross_file_summary.txt')}\")\n",
        "\n",
        "        # Save common CAN IDs for future reference\n",
        "        with open(os.path.join(self.output_dir, \"common_can_ids.txt\"), 'w') as f:\n",
        "            for can_id in sorted(self.common_can_ids):\n",
        "                f.write(f\"{can_id}\\n\")\n",
        "\n",
        "\n",
        "# Demo usage\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Process all CAN log files and visualize the comparison between\n",
        "    traditional analysis and deep learning approaches.\n",
        "    \"\"\"\n",
        "    # Set the directory containing CAN log files\n",
        "    log_directory = \"can_logs\"  # Replace with your log directory\n",
        "\n",
        "    if not os.path.exists(log_directory):\n",
        "        # Create directory for demonstration\n",
        "        os.makedirs(log_directory, exist_ok=True)\n",
        "        logger.warning(f\"Created empty '{log_directory}' directory.\")\n",
        "        logger.info(\"Please add your CAN log files to this directory and run again.\")\n",
        "        logger.info(\"Example usage: place .log files containing CAN frames in the can_logs directory.\")\n",
        "        return\n",
        "\n",
        "    # Check if there are any log files\n",
        "    log_files = [f for f in os.listdir(log_directory)\n",
        "                if os.path.isfile(os.path.join(log_directory, f))\n",
        "                and f.endswith('.log')]\n",
        "\n",
        "    if not log_files:\n",
        "        logger.warning(f\"No .log files found in {log_directory}\")\n",
        "        logger.info(\"Please add your CAN log files to this directory and run again.\")\n",
        "        logger.info(\"Example usage: place .log files containing CAN frames in the can_logs directory.\")\n",
        "        return\n",
        "\n",
        "    # Create multi-file analyzer\n",
        "    analyzer = MultiFileCANAnalyzer(log_directory, compare_traditional=True)\n",
        "\n",
        "    # Process all log files (limit to max_files if you want to process only a subset)\n",
        "    # Set parallel=True if you have multiple processors and want faster processing\n",
        "    analyzer.process_log_files(file_extension=\".log\", min_frames=100)\n",
        "\n",
        "    print(\"\\nAnalysis complete! The following results have been generated:\")\n",
        "    print(f\"1. Individual file analyses: {os.path.join(analyzer.output_dir, '<filename>', '*.png')}\")\n",
        "    print(f\"2. Cross-file visualizations: {os.path.join(analyzer.output_dir, 'cross_file_analysis', '*.png')}\")\n",
        "    print(f\"3. Summary report: {os.path.join(analyzer.output_dir, 'cross_file_summary.txt')}\")\n",
        "\n",
        "    # Provide instructions for interpreting the results\n",
        "    print(\"\\nHow to interpret the results:\")\n",
        "    print(\"1. Pattern Comparison: Shows how CNN detects complex signal patterns vs. traditional methods\")\n",
        "    print(\"2. Anomaly Detection: Compares effectiveness of CNN vs. statistical methods for detecting anomalies\")\n",
        "    print(\"3. Performance Charts: Radar charts showing overall performance comparison across methods\")\n",
        "    print(\"4. Cross-file Analysis: Shows patterns that appear consistently across different log files\")\n",
        "\n",
        "    logger.info(\"Multi-file analysis complete!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWt9XlxCyB+EnEFlynS2Z5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}